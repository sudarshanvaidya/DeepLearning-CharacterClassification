{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Character Classification on Google Street View Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. Extraction of dataset from HDF and building Image Classification pipeline with train/val/test splits\n",
    "\n",
    "2. Determining optimal k-Nearest Neighbor value and implement KNN classifier \n",
    "\n",
    "3. Building deep neural network classifier (Fully Connected Layer with one hidden layer) from scratch in Python \n",
    "\n",
    "4. Implementation of feedforward neural network, RELU activations, vectorized backpropagation, cost stochastic gradient descent, cross entropy loss, cost functions and batch normalization\n",
    "\n",
    "5. Running Neural Network for permutations and combinations of hyper parameters and choose best parameters \n",
    "\n",
    "6. Implementing neural network using Keras and identifying optimal hyper parameters\n",
    "\n",
    "7. Applying ensemble of neural network classifiers \n",
    "\n",
    "8. Comparison of traditional method (KNN) and Neural Networks using Python and Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract datasets from HDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'SVHN_single_grey1.h5'\n",
    "f = h5py.File(filename, 'r')\n",
    "\n",
    "# List all groups\n",
    "a_group_key = list(f.keys())\n",
    "a_group_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape datasets for model compatibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 1024)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = f.get('X_train')\n",
    "Xtrain = np.asarray(X_train)\n",
    "Xtrain = Xtrain.reshape(42000, 1024)\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = f.get('X_val')\n",
    "Xval = np.asarray(X_val)\n",
    "Xval = Xval.reshape(60000, 1024)\n",
    "Xval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = f.get('X_test')\n",
    "Xtest = np.asarray(X_test)\n",
    "Xtest = Xtest.reshape(18000, 1024)\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = f.get('y_train')\n",
    "y_train = np.asarray(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = f.get('y_val')\n",
    "y_val = np.asarray(y_val)\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = f.get('y_test')\n",
    "y_test = np.asarray(y_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize as per image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "\n",
    "Xtrain = Xtrain / 255.0\n",
    "Xval = Xval / 255.0\n",
    "Xtest = Xtest / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
     ]
    }
   ],
   "source": [
    "# visualizing the first 10 images in the dataset and their labels\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(Xtrain[i].reshape(32, 32), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label for each of the above image: %s' % (y_train[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use K-Nearest Neighbor Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore optimal K for KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=83.78%\n",
      "k=3, accuracy=62.83%\n",
      "k=5, accuracy=62.42%\n",
      "k=1 achieved highest accuracy of 83.78% on validation data\n"
     ]
    }
   ],
   "source": [
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "\n",
    "kVals = range(1, 6, 2)\n",
    "accuracies = []\n",
    "\n",
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "\n",
    "for k in range(1, 6, 2):\n",
    "          # train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "          model = KNeighborsClassifier(n_neighbors=k)\n",
    "          model.fit(Xtrain, y_train)\n",
    "          # evaluate the model and update the accuracies list\n",
    "          score = model.score(Xval, y_val)\n",
    "          print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "          accuracies.append(score)\n",
    "          \n",
    "# find the value of k that has the largest accuracy\n",
    "i = np.argmax(accuracies)\n",
    "print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i],\n",
    "accuracies[i] * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use KNN on training set using best value of K (=1) and get performance details of KNN on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy=45.92%\n",
      "Evaluation on Test data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.52      0.49      1814\n",
      "           1       0.50      0.57      0.53      1828\n",
      "           2       0.54      0.48      0.51      1803\n",
      "           3       0.37      0.35      0.36      1719\n",
      "           4       0.60      0.57      0.59      1812\n",
      "           5       0.38      0.33      0.36      1768\n",
      "           6       0.38      0.39      0.39      1832\n",
      "           7       0.63      0.59      0.61      1808\n",
      "           8       0.34      0.37      0.36      1812\n",
      "           9       0.39      0.40      0.40      1804\n",
      "\n",
      "    accuracy                           0.46     18000\n",
      "   macro avg       0.46      0.46      0.46     18000\n",
      "weighted avg       0.46      0.46      0.46     18000\n",
      "\n",
      "Confusion matrix\n",
      "[[ 935   43   51   64   71   65  185   40  170  190]\n",
      " [  51 1034  109  122  123   69   68  119   67   66]\n",
      " [  63  135  872  131   82   74   65  168   79  134]\n",
      " [  67  166  127  610   88  229   82   80  149  121]\n",
      " [  85  195   51   71 1039   55  111   35   98   72]\n",
      " [  94  103   71  220   54  588  203   48  205  182]\n",
      " [ 225   74   51   72  102  154  711   33  295  115]\n",
      " [  65  173  146  100   34   38   48 1071   53   80]\n",
      " [ 150   62   59  130   86  153  267   44  675  186]\n",
      " [ 238   87   89  115   60  113  115   65  192  730]]\n"
     ]
    }
   ],
   "source": [
    "# re-train our classifier using the best k value (=1) and predict the labels of the\n",
    "# test data\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(Xtrain, y_train)\n",
    "predictions = model.predict(Xtest)\n",
    "#print(predictions[1])\n",
    "score = model.score(Xtest, y_test)\n",
    "print(\"Test Accuracy=%.2f%%\" % (score * 100))\n",
    "\n",
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "# for each of the digits\n",
    "\n",
    "print(\"Evaluation on Test data\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print (\"Confusion matrix\")\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Neural Network Using Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement Fully Connected Layer with one hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Layer (Linear Layer)\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "        self.gradInput = None        \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = np.dot(X, self.W) + self.b\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradW = np.dot(self.X.T, nextgrad)\n",
    "        self.gradB = np.sum(nextgrad, axis=0)\n",
    "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
    "        return self.gradInput, [self.gradW, self.gradB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified Linear Activation Layer (ReLU)\n",
    "\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.output = np.maximum(X, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradInput = nextgrad.copy()\n",
    "        self.gradInput[self.output <=0] = 0\n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the softmax function\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Cross Entropy Loss\n",
    "\n",
    "class CrossEntropy:\n",
    "    def forward(self, X, y):\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
    "        loss = np.sum(cross_entropy) / self.m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        y_idx = y.argmax()        \n",
    "        grad = softmax(X)\n",
    "        grad[range(self.m), y] -= 1\n",
    "        grad /= self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we define the container NN class that enables the forward prop and backward propagation of the entire network. \n",
    "# This enables us to add layers of different types and also pass gradients using the chain rule.\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, lossfunc=CrossEntropy(), mode='train'):\n",
    "        self.params = []\n",
    "        self.layers = []\n",
    "        self.loss_func = lossfunc\n",
    "        self.grads = []\n",
    "        self.mode = mode\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        self.clear_grad_param()\n",
    "        for layer in reversed(self.layers):\n",
    "            nextgrad, grad = layer.backward(nextgrad)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss = self.loss_func.forward(out,y)\n",
    "        nextgrad = self.loss_func.backward(out,y)\n",
    "        grads = self.backward(nextgrad)\n",
    "        return loss, grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    def predict_scores(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return p\n",
    "    \n",
    "    def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the update function (SGD with momentum)\n",
    "def update_params(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = mu * v[i] - learning_rate * g[i]\n",
    "            p[i] += v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which gives us the minibatches (both the datapoint and the corresponding label)\n",
    "# get minibatches\n",
    "def minibatch(X, y, minibatch_size):\n",
    "    n = X.shape[0]\n",
    "    minibatches = []\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    for i in range(0, n , minibatch_size):\n",
    "        X_batch = X[i:i + minibatch_size, :]\n",
    "        y_batch = y[i:i + minibatch_size, ]\n",
    "        minibatches.append((X_batch, y_batch))\n",
    "        \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The traning loop\n",
    "\n",
    "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu=0.9, X_val=None, y_val=None):\n",
    "    val_loss_epoch = []\n",
    "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
    "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
    "\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "            \n",
    "        # iterate over mini batches\n",
    "        for X_mini, y_mini in minibatches:\n",
    "            loss, grads = net.train_step(X_mini, y_mini)\n",
    "            loss_batch.append(loss)\n",
    "            update_params(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
    "\n",
    "        for X_mini_val, y_mini_val in minibatches_val:\n",
    "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
    "            val_loss_batch.append(val_loss)\n",
    "        \n",
    "        # accuracy of model at end of epoch after all mini batch updates\n",
    "        m_train = X_train.shape[0]\n",
    "        m_val = X_val.shape[0]\n",
    "        y_train_pred = np.array([], dtype=\"int64\")\n",
    "        y_val_pred = np.array([], dtype=\"int64\")\n",
    "        y_train1 = []\n",
    "        y_vall = []\n",
    "        for i in range(0, m_train, minibatch_size):\n",
    "            X_tr = X_train[i:i + minibatch_size, : ]\n",
    "            y_tr = y_train[i:i + minibatch_size,]\n",
    "            y_train1 = np.append(y_train1, y_tr)\n",
    "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
    "\n",
    "        for i in range(0, m_val, minibatch_size):\n",
    "            X_va = X_val[i:i + minibatch_size, : ]\n",
    "            y_va = y_val[i:i + minibatch_size,]\n",
    "            y_vall = np.append(y_vall, y_va)\n",
    "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
    "            \n",
    "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
    "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
    "\n",
    "        mean_train_loss = sum(loss_batch) / float(len(loss_batch))\n",
    "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
    "        \n",
    "        val_loss_epoch.append(mean_val_loss)\n",
    "        print(\"Loss = {0} | Training Accuracy = {1} | Val Loss = {2} | Val Accuracy = {3}\".format(mean_train_loss, train_acc, mean_val_loss, val_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy of the model\n",
    "def check_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NN for various permutations and combinations of hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.299570466517278 | Training Accuracy = 0.15338095238095237 | Val Loss = 2.293753332689139 | Val Accuracy = 0.1536\n",
      "Loss = 2.2681186168103156 | Training Accuracy = 0.3345238095238095 | Val Loss = 2.2232434176991878 | Val Accuracy = 0.33425\n",
      "Loss = 2.117856876814809 | Training Accuracy = 0.3905 | Val Loss = 1.9607768342891632 | Val Accuracy = 0.39086666666666664\n",
      "Loss = 1.822654272923574 | Training Accuracy = 0.5100714285714286 | Val Loss = 1.6610501631808994 | Val Accuracy = 0.5107333333333334\n",
      "Loss = 1.5625097365056502 | Training Accuracy = 0.584952380952381 | Val Loss = 1.4592783500155522 | Val Accuracy = 0.58485\n",
      "Loss = 1.380325118817089 | Training Accuracy = 0.6294285714285714 | Val Loss = 1.3155252817010556 | Val Accuracy = 0.6287333333333334\n",
      "Loss = 1.2580282171354935 | Training Accuracy = 0.652952380952381 | Val Loss = 1.2144456348955306 | Val Accuracy = 0.6526\n",
      "Loss = 1.1741537973876472 | Training Accuracy = 0.6699761904761905 | Val Loss = 1.142096368483676 | Val Accuracy = 0.6692833333333333\n",
      "Loss = 1.1105289514764745 | Training Accuracy = 0.685 | Val Loss = 1.085747359998271 | Val Accuracy = 0.6844166666666667\n",
      "Loss = 1.0604169450621215 | Training Accuracy = 0.6973571428571429 | Val Loss = 1.0398148514846026 | Val Accuracy = 0.6962666666666667\n",
      "Loss = 1.0183604516286162 | Training Accuracy = 0.7083333333333334 | Val Loss = 1.0036199118358184 | Val Accuracy = 0.7067333333333333\n",
      "Loss = 0.9808645058858355 | Training Accuracy = 0.7140714285714286 | Val Loss = 0.971942061074281 | Val Accuracy = 0.71275\n",
      "Loss = 0.950296831336691 | Training Accuracy = 0.7262857142857143 | Val Loss = 0.9413751674362576 | Val Accuracy = 0.7241\n",
      "Loss = 0.9201918991461722 | Training Accuracy = 0.7335238095238096 | Val Loss = 0.903166652421185 | Val Accuracy = 0.7311166666666666\n",
      "Loss = 0.892825703949471 | Training Accuracy = 0.739 | Val Loss = 0.8826975872227582 | Val Accuracy = 0.7368666666666667\n",
      "Loss = 0.8688029695100747 | Training Accuracy = 0.7436904761904762 | Val Loss = 0.8663575365750306 | Val Accuracy = 0.74165\n",
      "Loss = 0.8460675123185709 | Training Accuracy = 0.7463333333333333 | Val Loss = 0.8505561876669104 | Val Accuracy = 0.74375\n",
      "Loss = 0.8245755415942637 | Training Accuracy = 0.7550714285714286 | Val Loss = 0.8256532354298314 | Val Accuracy = 0.7522666666666666\n",
      "Loss = 0.8047666145984215 | Training Accuracy = 0.7598571428571429 | Val Loss = 0.8184040237494303 | Val Accuracy = 0.7562166666666666\n",
      "Loss = 0.784373613004377 | Training Accuracy = 0.7640476190476191 | Val Loss = 0.8001593215867256 | Val Accuracy = 0.7603833333333333\n",
      "Loss = 0.7650203153745622 | Training Accuracy = 0.7671904761904762 | Val Loss = 0.7733825688043854 | Val Accuracy = 0.7639333333333334\n",
      "Loss = 0.7496198032039707 | Training Accuracy = 0.7726190476190476 | Val Loss = 0.7701179513425621 | Val Accuracy = 0.7682333333333333\n",
      "Loss = 0.7332906280347181 | Training Accuracy = 0.777452380952381 | Val Loss = 0.7501007324021314 | Val Accuracy = 0.7730333333333334\n",
      "Loss = 0.7167014896502248 | Training Accuracy = 0.7779761904761905 | Val Loss = 0.7408415787497217 | Val Accuracy = 0.7731833333333333\n",
      "Loss = 0.7037440880709244 | Training Accuracy = 0.7830714285714285 | Val Loss = 0.7284138863346135 | Val Accuracy = 0.7783\n",
      "Loss = 0.6907422432827843 | Training Accuracy = 0.7878571428571428 | Val Loss = 0.724105111863756 | Val Accuracy = 0.78295\n",
      "Loss = 0.6785767792986143 | Training Accuracy = 0.7879285714285714 | Val Loss = 0.7179929131307878 | Val Accuracy = 0.78275\n",
      "Loss = 0.6654267341589187 | Training Accuracy = 0.7927380952380952 | Val Loss = 0.7064452470789818 | Val Accuracy = 0.7878666666666667\n",
      "Loss = 0.6551951176649872 | Training Accuracy = 0.7936428571428571 | Val Loss = 0.7110266607473953 | Val Accuracy = 0.7881833333333333\n",
      "Loss = 0.6439965327631302 | Training Accuracy = 0.7986428571428571 | Val Loss = 0.685829630735754 | Val Accuracy = 0.7931333333333334\n",
      "Loss = 0.6325809866482349 | Training Accuracy = 0.8007619047619048 | Val Loss = 0.68945497657554 | Val Accuracy = 0.7954166666666667\n",
      "Loss = 0.6243614615234998 | Training Accuracy = 0.8040238095238095 | Val Loss = 0.6779045266204096 | Val Accuracy = 0.7985\n",
      "Loss = 0.6158069061806056 | Training Accuracy = 0.8118571428571428 | Val Loss = 0.6490189921679034 | Val Accuracy = 0.80585\n",
      "Loss = 0.6061346745799835 | Training Accuracy = 0.8098571428571428 | Val Loss = 0.6561072684586141 | Val Accuracy = 0.8040166666666667\n",
      "Loss = 0.598432943173346 | Training Accuracy = 0.8107857142857143 | Val Loss = 0.6525937294789532 | Val Accuracy = 0.805\n",
      "Loss = 0.5887061113767499 | Training Accuracy = 0.8125238095238095 | Val Loss = 0.6372510168721158 | Val Accuracy = 0.8061666666666667\n",
      "Loss = 0.5827785717234683 | Training Accuracy = 0.8104761904761905 | Val Loss = 0.6447434058229953 | Val Accuracy = 0.8037833333333333\n",
      "Loss = 0.5749835137641085 | Training Accuracy = 0.8191904761904761 | Val Loss = 0.6221909968489 | Val Accuracy = 0.8123\n",
      "Loss = 0.5682027125877738 | Training Accuracy = 0.8162380952380952 | Val Loss = 0.6282965040206608 | Val Accuracy = 0.8092166666666667\n",
      "Loss = 0.5603392910453968 | Training Accuracy = 0.8181428571428572 | Val Loss = 0.6183053942015801 | Val Accuracy = 0.8110333333333334\n",
      "Loss = 0.5525629414145729 | Training Accuracy = 0.8223809523809524 | Val Loss = 0.6031948046517353 | Val Accuracy = 0.8155\n",
      "Loss = 0.5462065080316612 | Training Accuracy = 0.8239285714285715 | Val Loss = 0.6085451674777 | Val Accuracy = 0.8161\n",
      "Loss = 0.5415140469032966 | Training Accuracy = 0.8251666666666667 | Val Loss = 0.6014777663683717 | Val Accuracy = 0.8178166666666666\n",
      "Loss = 0.5354212707594617 | Training Accuracy = 0.8292380952380952 | Val Loss = 0.5903550249557006 | Val Accuracy = 0.8213666666666667\n",
      "Loss = 0.5291567228468432 | Training Accuracy = 0.8251904761904761 | Val Loss = 0.5993839377198047 | Val Accuracy = 0.8177333333333333\n",
      "Loss = 0.5238096363231723 | Training Accuracy = 0.8289285714285715 | Val Loss = 0.5854027684389368 | Val Accuracy = 0.8213\n",
      "Loss = 0.5202202099688611 | Training Accuracy = 0.8298095238095238 | Val Loss = 0.5941861350945417 | Val Accuracy = 0.82215\n",
      "Loss = 0.5148918766959035 | Training Accuracy = 0.8297857142857142 | Val Loss = 0.5887312985583442 | Val Accuracy = 0.8219666666666666\n",
      "Loss = 0.5105077836545503 | Training Accuracy = 0.8318571428571429 | Val Loss = 0.5652904022122723 | Val Accuracy = 0.8239166666666666\n",
      "Loss = 0.5045698585430916 | Training Accuracy = 0.8304047619047619 | Val Loss = 0.5775464914233728 | Val Accuracy = 0.8227833333333333\n",
      "Loss = 0.49904912382065775 | Training Accuracy = 0.8343571428571429 | Val Loss = 0.5747182567289247 | Val Accuracy = 0.8261333333333334\n",
      "Loss = 0.4953910324923811 | Training Accuracy = 0.8322380952380952 | Val Loss = 0.5742421488351762 | Val Accuracy = 0.8237833333333333\n",
      "Loss = 0.4905239868814713 | Training Accuracy = 0.8335714285714285 | Val Loss = 0.5670688609439679 | Val Accuracy = 0.82545\n",
      "Loss = 0.4855162703928072 | Training Accuracy = 0.8348333333333333 | Val Loss = 0.5686940882646268 | Val Accuracy = 0.8267333333333333\n",
      "Loss = 0.4830208420656147 | Training Accuracy = 0.8370714285714286 | Val Loss = 0.5620960393840712 | Val Accuracy = 0.8285833333333333\n",
      "Loss = 0.4783742345047436 | Training Accuracy = 0.8369761904761904 | Val Loss = 0.562532508938876 | Val Accuracy = 0.8285\n",
      "Loss = 0.47574655706169405 | Training Accuracy = 0.8350238095238095 | Val Loss = 0.5600912681835865 | Val Accuracy = 0.8268333333333333\n",
      "Loss = 0.47034972909164474 | Training Accuracy = 0.8383095238095238 | Val Loss = 0.5596791325858705 | Val Accuracy = 0.8292166666666667\n",
      "Loss = 0.46571678434482155 | Training Accuracy = 0.8409047619047619 | Val Loss = 0.5414018850549007 | Val Accuracy = 0.8322166666666667\n",
      "Loss = 0.46143969545309 | Training Accuracy = 0.8469047619047619 | Val Loss = 0.5439102402936381 | Val Accuracy = 0.83765\n",
      "Loss = 0.45828217961629203 | Training Accuracy = 0.8408095238095238 | Val Loss = 0.5376204722359823 | Val Accuracy = 0.83205\n",
      "Loss = 0.4555999948169822 | Training Accuracy = 0.8447619047619047 | Val Loss = 0.5462491992295387 | Val Accuracy = 0.8356666666666667\n",
      "Loss = 0.4524343329400509 | Training Accuracy = 0.8429523809523809 | Val Loss = 0.550043591257756 | Val Accuracy = 0.8337166666666667\n",
      "Loss = 0.4491672488622462 | Training Accuracy = 0.8454761904761905 | Val Loss = 0.5505803185106708 | Val Accuracy = 0.8361333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.441956716206664 | Training Accuracy = 0.8452142857142857 | Val Loss = 0.5391609237239371 | Val Accuracy = 0.8358833333333333\n",
      "Loss = 0.4414125748722879 | Training Accuracy = 0.8465952380952381 | Val Loss = 0.5513668911297323 | Val Accuracy = 0.8367666666666667\n",
      "Loss = 0.4372113466569696 | Training Accuracy = 0.8461666666666666 | Val Loss = 0.5396458578931822 | Val Accuracy = 0.8368333333333333\n",
      "Loss = 0.4340163635004303 | Training Accuracy = 0.8448333333333333 | Val Loss = 0.5563522545623023 | Val Accuracy = 0.83525\n",
      "Loss = 0.43251194556875705 | Training Accuracy = 0.8469285714285715 | Val Loss = 0.5528971091117398 | Val Accuracy = 0.83735\n",
      "Loss = 0.42928350233482576 | Training Accuracy = 0.8513571428571428 | Val Loss = 0.5355152372561919 | Val Accuracy = 0.8415333333333334\n",
      "Loss = 0.4250828151495085 | Training Accuracy = 0.8525 | Val Loss = 0.5395657536597838 | Val Accuracy = 0.84235\n",
      "Loss = 0.42076502606269944 | Training Accuracy = 0.8483333333333334 | Val Loss = 0.539153244488545 | Val Accuracy = 0.8388166666666667\n",
      "Loss = 0.41870911810222233 | Training Accuracy = 0.851047619047619 | Val Loss = 0.5278922776016379 | Val Accuracy = 0.8404666666666667\n",
      "Loss = 0.41731804634065317 | Training Accuracy = 0.8491666666666666 | Val Loss = 0.5388650734178947 | Val Accuracy = 0.8392\n",
      "Loss = 0.4133182128329268 | Training Accuracy = 0.8537380952380952 | Val Loss = 0.5396117147125784 | Val Accuracy = 0.8431333333333333\n",
      "Loss = 0.4090788654795175 | Training Accuracy = 0.8528333333333333 | Val Loss = 0.534028599495372 | Val Accuracy = 0.8423666666666667\n",
      "Loss = 0.40812811174054986 | Training Accuracy = 0.855547619047619 | Val Loss = 0.5225173198410764 | Val Accuracy = 0.8448\n",
      "Loss = 0.40445312736586 | Training Accuracy = 0.8527380952380952 | Val Loss = 0.5423915625947848 | Val Accuracy = 0.84165\n",
      "Loss = 0.40059869786093727 | Training Accuracy = 0.8553333333333333 | Val Loss = 0.5269423730017679 | Val Accuracy = 0.8443833333333334\n",
      "Loss = 0.39928738830337707 | Training Accuracy = 0.8578333333333333 | Val Loss = 0.5165670816901419 | Val Accuracy = 0.84655\n",
      "Loss = 0.39635276452124296 | Training Accuracy = 0.8572380952380952 | Val Loss = 0.521412300969285 | Val Accuracy = 0.8457\n",
      "Loss = 0.39373605253919286 | Training Accuracy = 0.8548333333333333 | Val Loss = 0.5318809503745876 | Val Accuracy = 0.84355\n",
      "Loss = 0.3920796756217902 | Training Accuracy = 0.855452380952381 | Val Loss = 0.5289455569631969 | Val Accuracy = 0.844\n",
      "Loss = 0.38747564372624077 | Training Accuracy = 0.8611666666666666 | Val Loss = 0.5142389109342506 | Val Accuracy = 0.84915\n",
      "Loss = 0.38596098763122605 | Training Accuracy = 0.8589523809523809 | Val Loss = 0.5434463772433935 | Val Accuracy = 0.8470666666666666\n",
      "Loss = 0.38247505519861347 | Training Accuracy = 0.8575238095238096 | Val Loss = 0.5257928876898433 | Val Accuracy = 0.8458333333333333\n",
      "Loss = 0.3830991999718866 | Training Accuracy = 0.8618809523809524 | Val Loss = 0.5279708541513515 | Val Accuracy = 0.8497333333333333\n",
      "Loss = 0.3814783657451168 | Training Accuracy = 0.8600952380952381 | Val Loss = 0.543947741738586 | Val Accuracy = 0.8477833333333333\n",
      "Loss = 0.3778934901307696 | Training Accuracy = 0.8603095238095239 | Val Loss = 0.5342174998449709 | Val Accuracy = 0.8480333333333333\n",
      "Loss = 0.3770942191916078 | Training Accuracy = 0.8657380952380952 | Val Loss = 0.5238283660152064 | Val Accuracy = 0.8531833333333333\n",
      "Loss = 0.3742480444050274 | Training Accuracy = 0.8658571428571429 | Val Loss = 0.518328402375374 | Val Accuracy = 0.8535833333333334\n",
      "Loss = 0.3715817692029148 | Training Accuracy = 0.8665238095238095 | Val Loss = 0.5225276647173696 | Val Accuracy = 0.8540166666666666\n",
      "Loss = 0.37145532789513896 | Training Accuracy = 0.8704285714285714 | Val Loss = 0.5119304586342515 | Val Accuracy = 0.8575\n",
      "Loss = 0.36770758013345584 | Training Accuracy = 0.8673571428571428 | Val Loss = 0.5133317780599191 | Val Accuracy = 0.85465\n",
      "Loss = 0.3670127687469642 | Training Accuracy = 0.8653571428571428 | Val Loss = 0.5331729491633131 | Val Accuracy = 0.8526666666666667\n",
      "Loss = 0.3672557688106343 | Training Accuracy = 0.8714285714285714 | Val Loss = 0.5044682732410875 | Val Accuracy = 0.8580166666666666\n",
      "Loss = 0.3645945436174175 | Training Accuracy = 0.8660238095238095 | Val Loss = 0.511737882510306 | Val Accuracy = 0.8538\n",
      "Loss = 0.36076209284151123 | Training Accuracy = 0.8697619047619047 | Val Loss = 0.5321926723908803 | Val Accuracy = 0.8566166666666667\n",
      "Loss = 0.3603105653674857 | Training Accuracy = 0.867904761904762 | Val Loss = 0.5138233908076557 | Val Accuracy = 0.8548666666666667\n",
      "Loss = 0.35686286707500314 | Training Accuracy = 0.8648809523809524 | Val Loss = 0.5193903616892604 | Val Accuracy = 0.8522833333333333\n",
      "Loss = 0.35577678288251396 | Training Accuracy = 0.873547619047619 | Val Loss = 0.49870907224693395 | Val Accuracy = 0.86085\n",
      "Loss = 0.35786990207659564 | Training Accuracy = 0.8702857142857143 | Val Loss = 0.5184317452349373 | Val Accuracy = 0.8574\n",
      "Loss = 0.35133328600987024 | Training Accuracy = 0.8668571428571429 | Val Loss = 0.5394143052266054 | Val Accuracy = 0.8537833333333333\n",
      "Loss = 0.3499782197958862 | Training Accuracy = 0.8731190476190476 | Val Loss = 0.49865553518974365 | Val Accuracy = 0.86005\n",
      "Loss = 0.3498409424190004 | Training Accuracy = 0.8779047619047619 | Val Loss = 0.4978000352398079 | Val Accuracy = 0.8641166666666666\n",
      "Loss = 0.3470027557963135 | Training Accuracy = 0.8727142857142857 | Val Loss = 0.5177206400352106 | Val Accuracy = 0.8592\n",
      "Loss = 0.349618207351335 | Training Accuracy = 0.8593333333333333 | Val Loss = 0.5420956011310722 | Val Accuracy = 0.8465166666666667\n",
      "Loss = 0.3478487691004674 | Training Accuracy = 0.8794047619047619 | Val Loss = 0.4869942901855235 | Val Accuracy = 0.8657166666666667\n",
      "Loss = 0.34531187622925835 | Training Accuracy = 0.8684761904761905 | Val Loss = 0.5357060226017716 | Val Accuracy = 0.8549333333333333\n",
      "Loss = 0.3413808649512059 | Training Accuracy = 0.8751428571428571 | Val Loss = 0.5127460879207777 | Val Accuracy = 0.8613166666666666\n",
      "Loss = 0.33860855275294227 | Training Accuracy = 0.8722142857142857 | Val Loss = 0.5218856131880337 | Val Accuracy = 0.8585666666666667\n",
      "Loss = 0.3386794700166502 | Training Accuracy = 0.8747857142857143 | Val Loss = 0.5312047535511119 | Val Accuracy = 0.8607833333333333\n",
      "Loss = 0.33521793674647954 | Training Accuracy = 0.8748095238095238 | Val Loss = 0.5169265019820528 | Val Accuracy = 0.8612333333333333\n",
      "Loss = 0.3353148499894472 | Training Accuracy = 0.875452380952381 | Val Loss = 0.5256738637112897 | Val Accuracy = 0.8616\n",
      "Loss = 0.33365906750305957 | Training Accuracy = 0.8756666666666667 | Val Loss = 0.5228836387534469 | Val Accuracy = 0.86175\n",
      "Loss = 0.3342667192317329 | Training Accuracy = 0.8751904761904762 | Val Loss = 0.5240956674161165 | Val Accuracy = 0.8608666666666667\n",
      "Loss = 0.33084537846692114 | Training Accuracy = 0.8720238095238095 | Val Loss = 0.5366627680211792 | Val Accuracy = 0.8577833333333333\n",
      "Loss = 0.33082247920252605 | Training Accuracy = 0.8760238095238095 | Val Loss = 0.5329437069530165 | Val Accuracy = 0.86185\n",
      "Loss = 0.330311503372869 | Training Accuracy = 0.8739761904761905 | Val Loss = 0.5404641072157598 | Val Accuracy = 0.85965\n",
      "Loss = 0.32665649176455525 | Training Accuracy = 0.8816666666666667 | Val Loss = 0.5217806762861152 | Val Accuracy = 0.8667833333333334\n",
      "Loss = 0.32484911490988233 | Training Accuracy = 0.8779285714285714 | Val Loss = 0.5246941318530672 | Val Accuracy = 0.8631666666666666\n",
      "Loss = 0.3253637983708033 | Training Accuracy = 0.8766666666666667 | Val Loss = 0.5243419305954291 | Val Accuracy = 0.8624333333333334\n",
      "Loss = 0.3285714523159252 | Training Accuracy = 0.8773095238095238 | Val Loss = 0.5275356258100884 | Val Accuracy = 0.8629333333333333\n",
      "Loss = 0.324527845972771 | Training Accuracy = 0.8798809523809524 | Val Loss = 0.5102207677510641 | Val Accuracy = 0.8647833333333333\n",
      "Loss = 0.32659297369589596 | Training Accuracy = 0.8764047619047619 | Val Loss = 0.5157998753809725 | Val Accuracy = 0.86155\n",
      "Loss = 0.3227116404472745 | Training Accuracy = 0.8779047619047619 | Val Loss = 0.5189501956320333 | Val Accuracy = 0.8628333333333333\n",
      "Loss = 0.32226295033080116 | Training Accuracy = 0.8767380952380952 | Val Loss = 0.5211485863832471 | Val Accuracy = 0.8623333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.32333331437754065 | Training Accuracy = 0.8774761904761905 | Val Loss = 0.5251748763759309 | Val Accuracy = 0.86245\n",
      "Loss = 0.31854043054580383 | Training Accuracy = 0.8866428571428572 | Val Loss = 0.5013037370994736 | Val Accuracy = 0.8708\n",
      "Loss = 0.3178976454747998 | Training Accuracy = 0.8819047619047619 | Val Loss = 0.5317803326212519 | Val Accuracy = 0.8662666666666666\n",
      "Loss = 0.31850068381850105 | Training Accuracy = 0.8795 | Val Loss = 0.5354634706535222 | Val Accuracy = 0.8647166666666667\n",
      "Loss = 0.315814353442455 | Training Accuracy = 0.8805714285714286 | Val Loss = 0.5246619348401702 | Val Accuracy = 0.8653333333333333\n",
      "Loss = 0.31266495416083007 | Training Accuracy = 0.8872619047619048 | Val Loss = 0.51864861427178 | Val Accuracy = 0.8706\n",
      "Loss = 0.31285185412283495 | Training Accuracy = 0.885095238095238 | Val Loss = 0.5132050744404519 | Val Accuracy = 0.8683166666666666\n",
      "Loss = 0.31240086615604007 | Training Accuracy = 0.8899761904761905 | Val Loss = 0.5180002449131371 | Val Accuracy = 0.8739666666666667\n",
      "Loss = 0.3087437433181066 | Training Accuracy = 0.8865714285714286 | Val Loss = 0.5314523603052231 | Val Accuracy = 0.8699666666666667\n",
      "Loss = 0.3080898091686891 | Training Accuracy = 0.8867857142857143 | Val Loss = 0.5119610221670995 | Val Accuracy = 0.87015\n",
      "Loss = 0.30608520148410157 | Training Accuracy = 0.8902857142857142 | Val Loss = 0.5000047229452498 | Val Accuracy = 0.8734333333333333\n",
      "Loss = 0.3034915362358026 | Training Accuracy = 0.8911428571428571 | Val Loss = 0.49783148073529093 | Val Accuracy = 0.8742\n",
      "Loss = 0.3004012578156965 | Training Accuracy = 0.8905 | Val Loss = 0.5059094622739599 | Val Accuracy = 0.8738833333333333\n",
      "Loss = 0.3027212140598457 | Training Accuracy = 0.8896428571428572 | Val Loss = 0.5152249250664901 | Val Accuracy = 0.8726833333333334\n",
      "Loss = 0.29946425672045096 | Training Accuracy = 0.8986666666666666 | Val Loss = 0.5073018996226705 | Val Accuracy = 0.8811666666666667\n",
      "Loss = 0.2974022181304824 | Training Accuracy = 0.8858095238095238 | Val Loss = 0.5124356010887428 | Val Accuracy = 0.8692\n",
      "Loss = 0.29586314941632086 | Training Accuracy = 0.888095238095238 | Val Loss = 0.5348713253099416 | Val Accuracy = 0.8710666666666667\n",
      "Loss = 0.29729194099140677 | Training Accuracy = 0.8931190476190476 | Val Loss = 0.5219323963789098 | Val Accuracy = 0.8752833333333333\n",
      "Loss = 0.2925500824353767 | Training Accuracy = 0.8930476190476191 | Val Loss = 0.5055348224707472 | Val Accuracy = 0.8754666666666666\n",
      "Loss = 0.29451980320835003 | Training Accuracy = 0.8942380952380953 | Val Loss = 0.5199506291211288 | Val Accuracy = 0.8767333333333334\n",
      "Loss = 0.2947775834376634 | Training Accuracy = 0.8918095238095238 | Val Loss = 0.5395285449384436 | Val Accuracy = 0.87505\n",
      "Loss = 0.2916541058291194 | Training Accuracy = 0.8942380952380953 | Val Loss = 0.5275056531494305 | Val Accuracy = 0.8767833333333334\n",
      "Loss = 0.29225534927030694 | Training Accuracy = 0.8900714285714286 | Val Loss = 0.5327094467110677 | Val Accuracy = 0.8724333333333333\n",
      "Loss = 0.2925654843162648 | Training Accuracy = 0.8937142857142857 | Val Loss = 0.533851336394681 | Val Accuracy = 0.87615\n",
      "Loss = 0.2877281074144407 | Training Accuracy = 0.8959761904761905 | Val Loss = 0.5371052139087276 | Val Accuracy = 0.8783833333333333\n",
      "Loss = 0.2888124157452111 | Training Accuracy = 0.8970714285714285 | Val Loss = 0.5415605647583646 | Val Accuracy = 0.87895\n",
      "Loss = 0.2872722311965275 | Training Accuracy = 0.8982619047619048 | Val Loss = 0.5286543468169668 | Val Accuracy = 0.8805833333333334\n",
      "Loss = 0.2832566376396891 | Training Accuracy = 0.8936428571428572 | Val Loss = 0.5521503297920453 | Val Accuracy = 0.87585\n",
      "Loss = 0.28481183301815083 | Training Accuracy = 0.8961428571428571 | Val Loss = 0.5464329478197671 | Val Accuracy = 0.8782\n",
      "Loss = 0.28365938373549876 | Training Accuracy = 0.8966904761904761 | Val Loss = 0.554308638438158 | Val Accuracy = 0.8789833333333333\n",
      "Loss = 0.28410088823439233 | Training Accuracy = 0.8953333333333333 | Val Loss = 0.5459588628150387 | Val Accuracy = 0.8771833333333333\n",
      "Loss = 0.2829193245500469 | Training Accuracy = 0.8992857142857142 | Val Loss = 0.553874093210396 | Val Accuracy = 0.8805666666666667\n",
      "Loss = 0.28059450740708297 | Training Accuracy = 0.8976428571428572 | Val Loss = 0.560316292088146 | Val Accuracy = 0.8794333333333333\n",
      "Loss = 0.28072413082064185 | Training Accuracy = 0.8946666666666667 | Val Loss = 0.5549030834417842 | Val Accuracy = 0.8762\n",
      "Loss = 0.2793312014063775 | Training Accuracy = 0.8962142857142857 | Val Loss = 0.5733884967804082 | Val Accuracy = 0.8778\n",
      "Loss = 0.27921614569121395 | Training Accuracy = 0.8971904761904762 | Val Loss = 0.557451569331743 | Val Accuracy = 0.8785166666666666\n",
      "Loss = 0.28043323801847686 | Training Accuracy = 0.8961904761904762 | Val Loss = 0.5639193865898992 | Val Accuracy = 0.8781666666666667\n",
      "Loss = 0.27720253393414235 | Training Accuracy = 0.8978809523809523 | Val Loss = 0.5493802572983097 | Val Accuracy = 0.8791833333333333\n",
      "Loss = 0.27489352636841263 | Training Accuracy = 0.8987857142857143 | Val Loss = 0.5461879388115819 | Val Accuracy = 0.8800333333333333\n",
      "Loss = 0.2737693188277623 | Training Accuracy = 0.9040238095238096 | Val Loss = 0.5279497217923137 | Val Accuracy = 0.8847833333333334\n",
      "Loss = 0.2716819633051955 | Training Accuracy = 0.9001428571428571 | Val Loss = 0.543748548884432 | Val Accuracy = 0.88125\n",
      "Loss = 0.27413036444848016 | Training Accuracy = 0.8987142857142857 | Val Loss = 0.5706909117654385 | Val Accuracy = 0.88075\n",
      "Loss = 0.2698830991307352 | Training Accuracy = 0.8992142857142857 | Val Loss = 0.5438473282081762 | Val Accuracy = 0.8804333333333333\n",
      "Loss = 0.2714258264644094 | Training Accuracy = 0.8923571428571428 | Val Loss = 0.5757876537080315 | Val Accuracy = 0.8741\n",
      "Loss = 0.2680114439654191 | Training Accuracy = 0.8973809523809524 | Val Loss = 0.5511574470096996 | Val Accuracy = 0.8787166666666667\n",
      "Loss = 0.26652172900982224 | Training Accuracy = 0.9003809523809524 | Val Loss = 0.5708174192419442 | Val Accuracy = 0.8812166666666666\n",
      "Loss = 0.26686662413360956 | Training Accuracy = 0.8978809523809523 | Val Loss = 0.5748593254333434 | Val Accuracy = 0.8787333333333334\n",
      "Loss = 0.26412430977569346 | Training Accuracy = 0.8961666666666667 | Val Loss = 0.5637585752659869 | Val Accuracy = 0.8774666666666666\n",
      "Loss = 0.265177148649371 | Training Accuracy = 0.896 | Val Loss = 0.5878460714994864 | Val Accuracy = 0.8767333333333334\n",
      "Loss = 0.2626571549255457 | Training Accuracy = 0.8964761904761904 | Val Loss = 0.5741215602644166 | Val Accuracy = 0.87705\n",
      "Loss = 0.2627790162772909 | Training Accuracy = 0.8876428571428572 | Val Loss = 0.6206234636903865 | Val Accuracy = 0.8689\n",
      "Loss = 0.2635526922256545 | Training Accuracy = 0.8905238095238095 | Val Loss = 0.6007073171603091 | Val Accuracy = 0.8721333333333333\n",
      "Loss = 0.2629341031230054 | Training Accuracy = 0.8893333333333333 | Val Loss = 0.6017185684186681 | Val Accuracy = 0.8706166666666667\n",
      "Loss = 0.25889131378116986 | Training Accuracy = 0.8815714285714286 | Val Loss = 0.6311687131569648 | Val Accuracy = 0.86395\n",
      "Loss = 0.2585812672264806 | Training Accuracy = 0.8890476190476191 | Val Loss = 0.6040663695946002 | Val Accuracy = 0.8701666666666666\n",
      "Loss = 0.25671035949352894 | Training Accuracy = 0.8963095238095238 | Val Loss = 0.5939491964633622 | Val Accuracy = 0.8767333333333334\n",
      "Loss = 0.25498607672901424 | Training Accuracy = 0.8962380952380953 | Val Loss = 0.5873427737900194 | Val Accuracy = 0.8769333333333333\n",
      "Loss = 0.2530994113122292 | Training Accuracy = 0.8932142857142857 | Val Loss = 0.5825900011937711 | Val Accuracy = 0.87435\n",
      "Loss = 0.2536060086131461 | Training Accuracy = 0.8895238095238095 | Val Loss = 0.588472651529597 | Val Accuracy = 0.8712833333333333\n",
      "Loss = 0.2538485964931835 | Training Accuracy = 0.8983571428571429 | Val Loss = 0.5486558199205255 | Val Accuracy = 0.8787166666666667\n",
      "Loss = 0.2506462328269519 | Training Accuracy = 0.8985952380952381 | Val Loss = 0.5445709322744001 | Val Accuracy = 0.8790833333333333\n",
      "Loss = 0.24902943012652212 | Training Accuracy = 0.8950714285714285 | Val Loss = 0.5550398275813854 | Val Accuracy = 0.87615\n",
      "Loss = 0.24667870118794488 | Training Accuracy = 0.8965238095238095 | Val Loss = 0.5849092865468059 | Val Accuracy = 0.8774666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.2492004308475857 | Training Accuracy = 0.9071666666666667 | Val Loss = 0.5320110265164998 | Val Accuracy = 0.8872333333333333\n",
      "Loss = 0.2456478805310002 | Training Accuracy = 0.8946904761904761 | Val Loss = 0.5860318927673724 | Val Accuracy = 0.8756833333333334\n",
      "Loss = 0.24461126649327983 | Training Accuracy = 0.9003333333333333 | Val Loss = 0.5758822057583495 | Val Accuracy = 0.8809166666666667\n",
      "Loss = 0.24379855601717332 | Training Accuracy = 0.9025952380952381 | Val Loss = 0.5645045108772528 | Val Accuracy = 0.88225\n",
      "Loss = 0.2448006901429751 | Training Accuracy = 0.8989285714285714 | Val Loss = 0.5648507042984449 | Val Accuracy = 0.8790333333333333\n",
      "Loss = 0.2408633349124435 | Training Accuracy = 0.8993809523809524 | Val Loss = 0.6169320696416516 | Val Accuracy = 0.8796666666666667\n",
      "Loss = 0.24233405574754852 | Training Accuracy = 0.9012619047619047 | Val Loss = 0.5976616229108143 | Val Accuracy = 0.8810833333333333\n",
      "Loss = 0.2411223585212576 | Training Accuracy = 0.9067380952380952 | Val Loss = 0.5743606092104401 | Val Accuracy = 0.8865166666666666\n",
      "Loss = 0.2386562933268823 | Training Accuracy = 0.903452380952381 | Val Loss = 0.5919784911929206 | Val Accuracy = 0.8831666666666667\n",
      "Loss = 0.237656089470422 | Training Accuracy = 0.9023333333333333 | Val Loss = 0.589680384935188 | Val Accuracy = 0.8819666666666667\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## input size\n",
    "input_dim = Xtrain.shape[1]\n",
    "\n",
    "## hyperparameters\n",
    "iterations = 200\n",
    "learning_rate = 0.01\n",
    "hidden_nodes = 1000\n",
    "output_nodes = 10\n",
    "\n",
    "## define neural net\n",
    "nn = NN()\n",
    "nn.add_layer(Linear(input_dim, hidden_nodes))\n",
    "nn.add_layer(ReLU())\n",
    "nn.add_layer(Linear(hidden_nodes, output_nodes))\n",
    "\n",
    "nn = train(nn, Xtrain , y_train, minibatch_size=200, epoch=iterations, \\\n",
    "           learning_rate=learning_rate, X_val=Xval, y_val=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify the best hyper parameters - More than 20 different hyper parameter combinations were tried "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run\tHyper Parameters\tLoss\tTraining Accuracy\tValidation Loss\tValidation Accuracy\n",
    "1\t\"iterations = 50\n",
    "learning_rate = 0.01\n",
    "hidden_nodes = 500\"\t0.5253\t82.93%\t0.6802\t82.14%\n",
    "2\t\"iterations = 50\n",
    "learning_rate = 0.005\n",
    "hidden_nodes = 500\"\t0.6677\t81.05%\t0.8422\t80.40%\n",
    "3\t\"iterations = 50\n",
    "learning_rate = 0.0075\n",
    "hidden_nodes = 500\"\t0.5785\t81.92%\t0.6093\t81.19%\n",
    "4\t\"iterations = 50\n",
    "learning_rate = 0.0125\n",
    "hidden_nodes = 500\"\t0.5136\t83.93%\t0.6134\t82.96%\n",
    "5\t\"iterations = 50\n",
    "learning_rate = 0.015\n",
    "hidden_nodes = 500\"\t0.4994\t83.80%\t0.6643\t82.97%\n",
    "6\t\"iterations = 50\n",
    "learning_rate = 0.02\n",
    "hidden_nodes = 500\"\t0.5138\t84.10%\t0.5777\t83.24%\n",
    "7\t\"iterations = 50\n",
    "learning_rate = 0.075\n",
    "hidden_nodes = 1000\"\t0.8736\t73.52%\t0.9275\t72.83%\n",
    "8\t\"iterations = 50\n",
    "learning_rate = 0.0075\n",
    "hidden_nodes = 1000\"\t0.5518\t83.70%\t0.6945\t82.88%\n",
    "9\t\"iterations = 50\n",
    "learning_rate = 0.02\n",
    "hidden_nodes = 1000\"\t0.4999\t83.51%\t0.5220\t82.60%\n",
    "10\t\"iterations = 100\n",
    "learning_rate = 0.02\n",
    "hidden_nodes = 1000\"\t0.3745\t87.46%\t0.6511\t85.84%\n",
    "11\t\"iterations = 200\n",
    "learning_rate = 0.01\n",
    "hidden_nodes = 1000\"\t0.2450\t91.45%\t0.6135\t89.30%\n",
    "12\t\"iterations = 200\n",
    "learning_rate = 0.0075\n",
    "hidden_nodes = 1000\"\t0.2392\t89.75%\t0.4534\t87.87%\n",
    "13\t\"iterations = 200\n",
    "learning_rate = 0.02\n",
    "hidden_nodes = 1000\"\t0.2487\t87.96%\t0.4816\t85.73%\n",
    "14\t\"iterations = 200\n",
    "learning_rate = 0.012\n",
    "hidden_nodes = 1000\"\t0.2245\t90.18%\t0.4944\t87.92%\n",
    "15\t\"iterations = 200\n",
    "learning_rate = 0.009\n",
    "hidden_nodes = 1000\"\t0.2413\t90.18%\t0.6271\t88.21%\n",
    "16\t\"iterations = 200\n",
    "learning_rate = 0.009\n",
    "hidden_nodes = 10\"\t1.1631\t63.57%\t1.2745\t63.31%\n",
    "17\t\"iterations = 20\n",
    "learning_rate = 0.009\n",
    "hidden_nodes = 100\"\t0.9328\t72.48%\t0.9936\t72.14%\n",
    "18\t\"iterations = 20\n",
    "learning_rate = 0.009\n",
    "hidden_nodes = 1000\"\t0.8056\t75.67%\t0.7072\t75.24%\n",
    "19\t\"iterations = 20\n",
    "learning_rate = 0.009\n",
    "hidden_nodes = 2500\"\t0.7745\t78.38%\t0.7355\t77.88%\n",
    "20\t\"iterations = 200\n",
    "learning_rate = 0.011\n",
    "hidden_nodes = 500\"\t0.2668\t88.76%\t0.5717\t86.91%\n",
    "21\t\"iterations = 200\n",
    "learning_rate = 0.01\n",
    "hidden_nodes = 2000\"\t0.2305\t91.12%\t0.5005\t88.91%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply model (with best hyper parameters) on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.2991101837027825 | Training Accuracy = 0.14526190476190476 | Val Loss = 2.295347168399871 | Val Accuracy = 0.14322222222222222\n",
      "Loss = 2.2647512019702725 | Training Accuracy = 0.3348809523809524 | Val Loss = 2.2284456494436515 | Val Accuracy = 0.32944444444444443\n",
      "Loss = 2.108943585725267 | Training Accuracy = 0.41595238095238096 | Val Loss = 1.9828527980279775 | Val Accuracy = 0.4122777777777778\n",
      "Loss = 1.8113843207257911 | Training Accuracy = 0.510452380952381 | Val Loss = 1.7101220760929041 | Val Accuracy = 0.5095555555555555\n",
      "Loss = 1.5528157803462495 | Training Accuracy = 0.5598809523809524 | Val Loss = 1.5224548294631572 | Val Accuracy = 0.5571111111111111\n",
      "Loss = 1.3775569020444187 | Training Accuracy = 0.5986190476190476 | Val Loss = 1.379118940870355 | Val Accuracy = 0.5965\n",
      "Loss = 1.2577407635071962 | Training Accuracy = 0.6315714285714286 | Val Loss = 1.2843954385042904 | Val Accuracy = 0.6286111111111111\n",
      "Loss = 1.1752689339985654 | Training Accuracy = 0.6494047619047619 | Val Loss = 1.2142038001663198 | Val Accuracy = 0.6465\n",
      "Loss = 1.1139460906490726 | Training Accuracy = 0.6650714285714285 | Val Loss = 1.1619535805654204 | Val Accuracy = 0.664\n",
      "Loss = 1.065887871327364 | Training Accuracy = 0.6770238095238095 | Val Loss = 1.1187091241566793 | Val Accuracy = 0.6756666666666666\n",
      "Loss = 1.026863432971846 | Training Accuracy = 0.6882857142857143 | Val Loss = 1.0865895502295153 | Val Accuracy = 0.6874444444444444\n",
      "Loss = 0.9923599332782496 | Training Accuracy = 0.6984523809523809 | Val Loss = 1.0462987738901823 | Val Accuracy = 0.6965\n",
      "Loss = 0.9612262686660242 | Training Accuracy = 0.7048571428571428 | Val Loss = 1.0190266696841517 | Val Accuracy = 0.7001111111111111\n",
      "Loss = 0.9323370002231836 | Training Accuracy = 0.717452380952381 | Val Loss = 0.9905246146154989 | Val Accuracy = 0.7098888888888889\n",
      "Loss = 0.906370400634978 | Training Accuracy = 0.7243809523809523 | Val Loss = 0.96297810884173 | Val Accuracy = 0.7154444444444444\n",
      "Loss = 0.8816193779284706 | Training Accuracy = 0.7325714285714285 | Val Loss = 0.9389116680292388 | Val Accuracy = 0.7215\n",
      "Loss = 0.8581121276717765 | Training Accuracy = 0.7397619047619047 | Val Loss = 0.9211373082608388 | Val Accuracy = 0.7277777777777777\n",
      "Loss = 0.8346648322125793 | Training Accuracy = 0.7487380952380952 | Val Loss = 0.8933298965010383 | Val Accuracy = 0.736\n",
      "Loss = 0.8136029471458984 | Training Accuracy = 0.7526190476190476 | Val Loss = 0.8777845429940172 | Val Accuracy = 0.7385\n",
      "Loss = 0.7930890607013987 | Training Accuracy = 0.7601428571428571 | Val Loss = 0.8581289764591004 | Val Accuracy = 0.7463333333333333\n",
      "Loss = 0.7750514069537476 | Training Accuracy = 0.7702619047619048 | Val Loss = 0.8349184103064984 | Val Accuracy = 0.7560555555555556\n",
      "Loss = 0.7560609946113721 | Training Accuracy = 0.7733333333333333 | Val Loss = 0.824359315817844 | Val Accuracy = 0.7583333333333333\n",
      "Loss = 0.7413167542225314 | Training Accuracy = 0.7790476190476191 | Val Loss = 0.807861852761874 | Val Accuracy = 0.7635555555555555\n",
      "Loss = 0.7241286933258553 | Training Accuracy = 0.7845952380952381 | Val Loss = 0.7920388066307136 | Val Accuracy = 0.7685555555555555\n",
      "Loss = 0.7094132337184289 | Training Accuracy = 0.787547619047619 | Val Loss = 0.7844594165973949 | Val Accuracy = 0.7701111111111111\n",
      "Loss = 0.6975882173086327 | Training Accuracy = 0.7923571428571429 | Val Loss = 0.7733465958620651 | Val Accuracy = 0.7737222222222222\n",
      "Loss = 0.6859073073770584 | Training Accuracy = 0.7942857142857143 | Val Loss = 0.7644438930033606 | Val Accuracy = 0.7754444444444445\n",
      "Loss = 0.6730657339781525 | Training Accuracy = 0.8001190476190476 | Val Loss = 0.7562344003664951 | Val Accuracy = 0.7806666666666666\n",
      "Loss = 0.6633321250756591 | Training Accuracy = 0.8012619047619047 | Val Loss = 0.7476227786131706 | Val Accuracy = 0.7808333333333334\n",
      "Loss = 0.6553377480863156 | Training Accuracy = 0.8045476190476191 | Val Loss = 0.7346724850931138 | Val Accuracy = 0.7836666666666666\n",
      "Loss = 0.6450139319836016 | Training Accuracy = 0.8065238095238095 | Val Loss = 0.7340013153520657 | Val Accuracy = 0.7878888888888889\n",
      "Loss = 0.6336749546058094 | Training Accuracy = 0.810047619047619 | Val Loss = 0.7224338584303478 | Val Accuracy = 0.788\n",
      "Loss = 0.6239417710836402 | Training Accuracy = 0.8139047619047619 | Val Loss = 0.7066324800936794 | Val Accuracy = 0.7943333333333333\n",
      "Loss = 0.616178027122645 | Training Accuracy = 0.8161428571428572 | Val Loss = 0.7097098509496287 | Val Accuracy = 0.7944444444444444\n",
      "Loss = 0.6076438421717679 | Training Accuracy = 0.8134047619047619 | Val Loss = 0.7187563697455134 | Val Accuracy = 0.7908888888888889\n",
      "Loss = 0.6005215950185998 | Training Accuracy = 0.8161666666666667 | Val Loss = 0.7023235559894926 | Val Accuracy = 0.7931666666666667\n",
      "Loss = 0.592269015620119 | Training Accuracy = 0.8188095238095238 | Val Loss = 0.6845687638459519 | Val Accuracy = 0.7966111111111112\n",
      "Loss = 0.5867402696114746 | Training Accuracy = 0.8183571428571429 | Val Loss = 0.6818383995690239 | Val Accuracy = 0.7987222222222222\n",
      "Loss = 0.5786476468094569 | Training Accuracy = 0.8186428571428571 | Val Loss = 0.6843127542750533 | Val Accuracy = 0.7960555555555555\n",
      "Loss = 0.5749828197166419 | Training Accuracy = 0.8248809523809524 | Val Loss = 0.6758298711207421 | Val Accuracy = 0.8010555555555555\n",
      "Loss = 0.5662158415622621 | Training Accuracy = 0.8217142857142857 | Val Loss = 0.671174317919707 | Val Accuracy = 0.7997777777777778\n",
      "Loss = 0.5615522159984819 | Training Accuracy = 0.821 | Val Loss = 0.6847550492748182 | Val Accuracy = 0.7985555555555556\n",
      "Loss = 0.5567456709999121 | Training Accuracy = 0.8247857142857142 | Val Loss = 0.6669577314108467 | Val Accuracy = 0.8008333333333333\n",
      "Loss = 0.5491531024744288 | Training Accuracy = 0.8234047619047619 | Val Loss = 0.6746297000346626 | Val Accuracy = 0.7988333333333333\n",
      "Loss = 0.5457785861086689 | Training Accuracy = 0.8284285714285714 | Val Loss = 0.6443470176249677 | Val Accuracy = 0.8041111111111111\n",
      "Loss = 0.5396392691596806 | Training Accuracy = 0.8272142857142857 | Val Loss = 0.6552420745702245 | Val Accuracy = 0.8026111111111112\n",
      "Loss = 0.5329339206180354 | Training Accuracy = 0.8283809523809523 | Val Loss = 0.6594872823340556 | Val Accuracy = 0.8009444444444445\n",
      "Loss = 0.5286713392355187 | Training Accuracy = 0.8271666666666667 | Val Loss = 0.6471725365289349 | Val Accuracy = 0.8025555555555556\n",
      "Loss = 0.5225641140631923 | Training Accuracy = 0.8299761904761904 | Val Loss = 0.6428470052866895 | Val Accuracy = 0.8038888888888889\n",
      "Loss = 0.5178615926147808 | Training Accuracy = 0.8313571428571429 | Val Loss = 0.6497815082571635 | Val Accuracy = 0.8012777777777778\n",
      "Loss = 0.5116745717483466 | Training Accuracy = 0.8333095238095238 | Val Loss = 0.6445449656556268 | Val Accuracy = 0.8036111111111112\n",
      "Loss = 0.5058980279393985 | Training Accuracy = 0.834452380952381 | Val Loss = 0.6477301567855831 | Val Accuracy = 0.8065\n",
      "Loss = 0.5034509666674766 | Training Accuracy = 0.8380714285714286 | Val Loss = 0.6298153839593467 | Val Accuracy = 0.8090555555555555\n",
      "Loss = 0.49791379967160543 | Training Accuracy = 0.8382142857142857 | Val Loss = 0.6228132458454213 | Val Accuracy = 0.8066666666666666\n",
      "Loss = 0.4933700688748924 | Training Accuracy = 0.8407619047619047 | Val Loss = 0.625065011294664 | Val Accuracy = 0.8086666666666666\n",
      "Loss = 0.4891395264477772 | Training Accuracy = 0.8381428571428572 | Val Loss = 0.6202054868341377 | Val Accuracy = 0.8056666666666666\n",
      "Loss = 0.4847688948022565 | Training Accuracy = 0.8408095238095238 | Val Loss = 0.6218250619651609 | Val Accuracy = 0.8095555555555556\n",
      "Loss = 0.48051923824955645 | Training Accuracy = 0.8436904761904762 | Val Loss = 0.6178434677332784 | Val Accuracy = 0.8104444444444444\n",
      "Loss = 0.47726289653305903 | Training Accuracy = 0.8438095238095238 | Val Loss = 0.6126766210648076 | Val Accuracy = 0.8107222222222222\n",
      "Loss = 0.4714054705857616 | Training Accuracy = 0.8458333333333333 | Val Loss = 0.6120729298980885 | Val Accuracy = 0.8122222222222222\n",
      "Loss = 0.4669429686748232 | Training Accuracy = 0.8478095238095238 | Val Loss = 0.6185887667217357 | Val Accuracy = 0.8135\n",
      "Loss = 0.46470521656305463 | Training Accuracy = 0.8445952380952381 | Val Loss = 0.6104416511413413 | Val Accuracy = 0.8097222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.46080178551897677 | Training Accuracy = 0.8447142857142858 | Val Loss = 0.6207033850577868 | Val Accuracy = 0.812\n",
      "Loss = 0.45644303362267746 | Training Accuracy = 0.8465714285714285 | Val Loss = 0.6105208543508012 | Val Accuracy = 0.8113888888888889\n",
      "Loss = 0.45438606100424234 | Training Accuracy = 0.8448809523809524 | Val Loss = 0.6111289011574494 | Val Accuracy = 0.8105\n",
      "Loss = 0.44979582473023605 | Training Accuracy = 0.8498571428571429 | Val Loss = 0.610051945664951 | Val Accuracy = 0.8137777777777778\n",
      "Loss = 0.4473125246914303 | Training Accuracy = 0.8508333333333333 | Val Loss = 0.6045595877324529 | Val Accuracy = 0.8144444444444444\n",
      "Loss = 0.44303929496156724 | Training Accuracy = 0.8528333333333333 | Val Loss = 0.6056668439535586 | Val Accuracy = 0.8161666666666667\n",
      "Loss = 0.44052749698582905 | Training Accuracy = 0.8548333333333333 | Val Loss = 0.6118140917290306 | Val Accuracy = 0.8177777777777778\n",
      "Loss = 0.4337260302579069 | Training Accuracy = 0.8533571428571428 | Val Loss = 0.6064674445961695 | Val Accuracy = 0.8170555555555555\n",
      "Loss = 0.4338853762518801 | Training Accuracy = 0.8524285714285714 | Val Loss = 0.6091747852590449 | Val Accuracy = 0.815\n",
      "Loss = 0.4311749873628666 | Training Accuracy = 0.8515952380952381 | Val Loss = 0.5989940497722016 | Val Accuracy = 0.8147777777777778\n",
      "Loss = 0.4273801435184182 | Training Accuracy = 0.8553809523809524 | Val Loss = 0.6096377699892537 | Val Accuracy = 0.8163888888888889\n",
      "Loss = 0.42228980660066323 | Training Accuracy = 0.8538809523809524 | Val Loss = 0.6047379956232822 | Val Accuracy = 0.815\n",
      "Loss = 0.4204120719209925 | Training Accuracy = 0.8604285714285714 | Val Loss = 0.5895905391765125 | Val Accuracy = 0.8217222222222222\n",
      "Loss = 0.41890047246900997 | Training Accuracy = 0.8548095238095238 | Val Loss = 0.6087898795573085 | Val Accuracy = 0.8141111111111111\n",
      "Loss = 0.41638602357353915 | Training Accuracy = 0.8599523809523809 | Val Loss = 0.5848710763477862 | Val Accuracy = 0.8211666666666667\n",
      "Loss = 0.4135434894049763 | Training Accuracy = 0.8560476190476191 | Val Loss = 0.5970123584635373 | Val Accuracy = 0.8178333333333333\n",
      "Loss = 0.40940310816930914 | Training Accuracy = 0.859 | Val Loss = 0.5867431985344423 | Val Accuracy = 0.8176666666666667\n",
      "Loss = 0.4075069162857394 | Training Accuracy = 0.8567142857142858 | Val Loss = 0.5987524606756389 | Val Accuracy = 0.8162222222222222\n",
      "Loss = 0.4058484235998539 | Training Accuracy = 0.860547619047619 | Val Loss = 0.5836033896392933 | Val Accuracy = 0.8166666666666667\n",
      "Loss = 0.4030399851538563 | Training Accuracy = 0.8602857142857143 | Val Loss = 0.5859781829020961 | Val Accuracy = 0.8182222222222222\n",
      "Loss = 0.4001662245850413 | Training Accuracy = 0.8600714285714286 | Val Loss = 0.5913549845746309 | Val Accuracy = 0.8184444444444444\n",
      "Loss = 0.3958035592515346 | Training Accuracy = 0.8674285714285714 | Val Loss = 0.5713960180283926 | Val Accuracy = 0.8225555555555556\n",
      "Loss = 0.39321012716156645 | Training Accuracy = 0.8615714285714285 | Val Loss = 0.594469567890812 | Val Accuracy = 0.8203333333333334\n",
      "Loss = 0.3941249618800458 | Training Accuracy = 0.8626428571428572 | Val Loss = 0.5956574451567888 | Val Accuracy = 0.8215\n",
      "Loss = 0.3922829907735014 | Training Accuracy = 0.8631904761904762 | Val Loss = 0.571753794688556 | Val Accuracy = 0.8203333333333334\n",
      "Loss = 0.3873860125660804 | Training Accuracy = 0.8625952380952381 | Val Loss = 0.5768047587851514 | Val Accuracy = 0.8193333333333334\n",
      "Loss = 0.38458181184576656 | Training Accuracy = 0.8670238095238095 | Val Loss = 0.5660970537580726 | Val Accuracy = 0.8241111111111111\n",
      "Loss = 0.3843776700695901 | Training Accuracy = 0.863 | Val Loss = 0.5798849595651844 | Val Accuracy = 0.8178333333333333\n",
      "Loss = 0.38048998887697083 | Training Accuracy = 0.8703333333333333 | Val Loss = 0.5604463165728711 | Val Accuracy = 0.8273888888888888\n",
      "Loss = 0.38128881890391253 | Training Accuracy = 0.8678333333333333 | Val Loss = 0.571344470521907 | Val Accuracy = 0.8233333333333334\n",
      "Loss = 0.376743434537334 | Training Accuracy = 0.8682142857142857 | Val Loss = 0.5808161319513178 | Val Accuracy = 0.8230555555555555\n",
      "Loss = 0.37522783824334577 | Training Accuracy = 0.8708571428571429 | Val Loss = 0.5657498186383041 | Val Accuracy = 0.8252777777777778\n",
      "Loss = 0.3742000685814891 | Training Accuracy = 0.8675238095238095 | Val Loss = 0.5722822432550976 | Val Accuracy = 0.8225\n",
      "Loss = 0.3718849108221754 | Training Accuracy = 0.8693571428571428 | Val Loss = 0.5803359093005173 | Val Accuracy = 0.8219444444444445\n",
      "Loss = 0.3683830360123318 | Training Accuracy = 0.8710714285714286 | Val Loss = 0.5630234024576686 | Val Accuracy = 0.8216111111111111\n",
      "Loss = 0.36516667787669777 | Training Accuracy = 0.8707380952380952 | Val Loss = 0.5623618583777409 | Val Accuracy = 0.8230555555555555\n",
      "Loss = 0.36385547851565453 | Training Accuracy = 0.8749047619047619 | Val Loss = 0.5421541631995693 | Val Accuracy = 0.8279444444444445\n",
      "Loss = 0.3623654083355952 | Training Accuracy = 0.8737619047619047 | Val Loss = 0.5595647970613502 | Val Accuracy = 0.8266666666666667\n",
      "Loss = 0.3622513279407259 | Training Accuracy = 0.8718571428571429 | Val Loss = 0.5728387597566501 | Val Accuracy = 0.8212222222222222\n",
      "Loss = 0.3600981155177707 | Training Accuracy = 0.8753095238095238 | Val Loss = 0.5631548557686084 | Val Accuracy = 0.8263888888888888\n",
      "Loss = 0.3598142490104403 | Training Accuracy = 0.8720238095238095 | Val Loss = 0.5636554796433356 | Val Accuracy = 0.8253888888888888\n",
      "Loss = 0.35732218345185246 | Training Accuracy = 0.8741428571428571 | Val Loss = 0.5460781741370457 | Val Accuracy = 0.8277222222222222\n",
      "Loss = 0.3542046265141534 | Training Accuracy = 0.8736190476190476 | Val Loss = 0.5567350126723749 | Val Accuracy = 0.8256111111111111\n",
      "Loss = 0.3565191113135449 | Training Accuracy = 0.8738333333333334 | Val Loss = 0.5571454750114224 | Val Accuracy = 0.8248888888888889\n",
      "Loss = 0.35197903280067283 | Training Accuracy = 0.8771428571428571 | Val Loss = 0.5501176659094096 | Val Accuracy = 0.8282777777777778\n",
      "Loss = 0.35056798965191244 | Training Accuracy = 0.8726666666666667 | Val Loss = 0.5596170365800005 | Val Accuracy = 0.8231111111111111\n",
      "Loss = 0.3465734848367557 | Training Accuracy = 0.8771904761904762 | Val Loss = 0.5578711203766109 | Val Accuracy = 0.8280555555555555\n",
      "Loss = 0.3470062766666914 | Training Accuracy = 0.8750714285714286 | Val Loss = 0.5495182326999947 | Val Accuracy = 0.8269444444444445\n",
      "Loss = 0.3447555494300009 | Training Accuracy = 0.8730238095238095 | Val Loss = 0.5627692431609996 | Val Accuracy = 0.8232222222222222\n",
      "Loss = 0.3450059556394984 | Training Accuracy = 0.8719761904761905 | Val Loss = 0.5605290487564099 | Val Accuracy = 0.8238888888888889\n",
      "Loss = 0.34394030939761955 | Training Accuracy = 0.8793809523809524 | Val Loss = 0.5575267136663005 | Val Accuracy = 0.8294444444444444\n",
      "Loss = 0.34293542855540105 | Training Accuracy = 0.8769523809523809 | Val Loss = 0.5486570815379825 | Val Accuracy = 0.8264444444444444\n",
      "Loss = 0.34117980223876937 | Training Accuracy = 0.8822380952380953 | Val Loss = 0.5450991822386508 | Val Accuracy = 0.8310555555555555\n",
      "Loss = 0.34010874510772526 | Training Accuracy = 0.8757142857142857 | Val Loss = 0.5766013881228608 | Val Accuracy = 0.8252777777777778\n",
      "Loss = 0.34058027175236727 | Training Accuracy = 0.8776190476190476 | Val Loss = 0.5574084879302893 | Val Accuracy = 0.8261666666666667\n",
      "Loss = 0.3392004496464464 | Training Accuracy = 0.8800238095238095 | Val Loss = 0.5492160109949255 | Val Accuracy = 0.8281666666666667\n",
      "Loss = 0.3364399791500767 | Training Accuracy = 0.880452380952381 | Val Loss = 0.5425804538704432 | Val Accuracy = 0.8292222222222222\n",
      "Loss = 0.3354072561804747 | Training Accuracy = 0.8798809523809524 | Val Loss = 0.5486447759136088 | Val Accuracy = 0.8297777777777777\n",
      "Loss = 0.33889521262874717 | Training Accuracy = 0.8792619047619048 | Val Loss = 0.5458603650315036 | Val Accuracy = 0.8286666666666667\n",
      "Loss = 0.3335498474467705 | Training Accuracy = 0.8775952380952381 | Val Loss = 0.5536064886434229 | Val Accuracy = 0.8268888888888889\n",
      "Loss = 0.3351061608203458 | Training Accuracy = 0.8832380952380953 | Val Loss = 0.5388667858594895 | Val Accuracy = 0.8321111111111111\n",
      "Loss = 0.3317786527359889 | Training Accuracy = 0.8838571428571429 | Val Loss = 0.5385518346517563 | Val Accuracy = 0.8306111111111111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.3293670715141569 | Training Accuracy = 0.8837380952380952 | Val Loss = 0.5439331481301507 | Val Accuracy = 0.8297222222222222\n",
      "Loss = 0.3290849943922977 | Training Accuracy = 0.8852380952380953 | Val Loss = 0.5216503662772276 | Val Accuracy = 0.8302777777777778\n",
      "Loss = 0.3270022030822722 | Training Accuracy = 0.8839285714285714 | Val Loss = 0.5445295295495716 | Val Accuracy = 0.8307222222222223\n",
      "Loss = 0.3247597393672097 | Training Accuracy = 0.8894761904761905 | Val Loss = 0.5339378002716213 | Val Accuracy = 0.8339444444444445\n",
      "Loss = 0.32534089975207015 | Training Accuracy = 0.8876666666666667 | Val Loss = 0.5357512863932659 | Val Accuracy = 0.8307222222222223\n",
      "Loss = 0.3244193346793292 | Training Accuracy = 0.8840714285714286 | Val Loss = 0.5446734136499334 | Val Accuracy = 0.8296666666666667\n",
      "Loss = 0.3190272218890719 | Training Accuracy = 0.8886666666666667 | Val Loss = 0.5347454439536086 | Val Accuracy = 0.8306666666666667\n",
      "Loss = 0.3160756759387311 | Training Accuracy = 0.8896904761904761 | Val Loss = 0.5234043759345701 | Val Accuracy = 0.8317222222222223\n",
      "Loss = 0.31802680128326677 | Training Accuracy = 0.8917380952380952 | Val Loss = 0.5312651442903208 | Val Accuracy = 0.8347222222222223\n",
      "Loss = 0.31457931056730387 | Training Accuracy = 0.8911190476190476 | Val Loss = 0.5250305488967372 | Val Accuracy = 0.8332222222222222\n",
      "Loss = 0.31338789492232333 | Training Accuracy = 0.8879761904761905 | Val Loss = 0.5194739938231673 | Val Accuracy = 0.8313888888888888\n",
      "Loss = 0.3105607712333033 | Training Accuracy = 0.8914047619047619 | Val Loss = 0.5273874351959189 | Val Accuracy = 0.8330555555555555\n",
      "Loss = 0.3110842530887576 | Training Accuracy = 0.894095238095238 | Val Loss = 0.5346244397138283 | Val Accuracy = 0.8352777777777778\n",
      "Loss = 0.3058465936525558 | Training Accuracy = 0.8902380952380953 | Val Loss = 0.5315746522292675 | Val Accuracy = 0.8313333333333334\n",
      "Loss = 0.3022871769522808 | Training Accuracy = 0.895095238095238 | Val Loss = 0.5236343418940071 | Val Accuracy = 0.8360555555555556\n",
      "Loss = 0.3040063895889325 | Training Accuracy = 0.891095238095238 | Val Loss = 0.5246940230680774 | Val Accuracy = 0.8305555555555556\n",
      "Loss = 0.3006341034152007 | Training Accuracy = 0.8907142857142857 | Val Loss = 0.5239423799596642 | Val Accuracy = 0.8317222222222223\n",
      "Loss = 0.30072184195370144 | Training Accuracy = 0.8887619047619048 | Val Loss = 0.5202150763377796 | Val Accuracy = 0.8296666666666667\n",
      "Loss = 0.2987100072329056 | Training Accuracy = 0.8886904761904761 | Val Loss = 0.5308955370439724 | Val Accuracy = 0.8298888888888889\n",
      "Loss = 0.29753331361072183 | Training Accuracy = 0.8895714285714286 | Val Loss = 0.5309110341174025 | Val Accuracy = 0.8303888888888888\n",
      "Loss = 0.2950690921150694 | Training Accuracy = 0.892547619047619 | Val Loss = 0.5071993142756311 | Val Accuracy = 0.8334444444444444\n",
      "Loss = 0.2944878136720034 | Training Accuracy = 0.8935 | Val Loss = 0.5266445387685389 | Val Accuracy = 0.8323333333333334\n",
      "Loss = 0.293881919629922 | Training Accuracy = 0.8937380952380952 | Val Loss = 0.5110850852634806 | Val Accuracy = 0.8346666666666667\n",
      "Loss = 0.29274758731381634 | Training Accuracy = 0.893095238095238 | Val Loss = 0.5089556702649698 | Val Accuracy = 0.8312777777777778\n",
      "Loss = 0.29060878681165675 | Training Accuracy = 0.8923333333333333 | Val Loss = 0.5181600403143326 | Val Accuracy = 0.8312777777777778\n",
      "Loss = 0.2897291867084077 | Training Accuracy = 0.8913333333333333 | Val Loss = 0.5118667083318178 | Val Accuracy = 0.8313333333333334\n",
      "Loss = 0.2884176763152103 | Training Accuracy = 0.8900714285714286 | Val Loss = 0.5301044488115216 | Val Accuracy = 0.8304444444444444\n",
      "Loss = 0.28710438877136024 | Training Accuracy = 0.8943571428571429 | Val Loss = 0.5230690566111129 | Val Accuracy = 0.8313333333333334\n",
      "Loss = 0.2877207571234693 | Training Accuracy = 0.890095238095238 | Val Loss = 0.5149392573977498 | Val Accuracy = 0.8292222222222222\n",
      "Loss = 0.2828752660130954 | Training Accuracy = 0.891 | Val Loss = 0.5325916392390228 | Val Accuracy = 0.8295\n",
      "Loss = 0.2843975962117079 | Training Accuracy = 0.8914047619047619 | Val Loss = 0.520494683134416 | Val Accuracy = 0.8304444444444444\n",
      "Loss = 0.28411693240827635 | Training Accuracy = 0.8926190476190476 | Val Loss = 0.5185253189922256 | Val Accuracy = 0.8321666666666667\n",
      "Loss = 0.2826179576152298 | Training Accuracy = 0.8952857142857142 | Val Loss = 0.5111052946547167 | Val Accuracy = 0.8341666666666666\n",
      "Loss = 0.28111218300066115 | Training Accuracy = 0.8956428571428572 | Val Loss = 0.5153556916600356 | Val Accuracy = 0.8346111111111111\n",
      "Loss = 0.28124496802827653 | Training Accuracy = 0.8928095238095238 | Val Loss = 0.5202317952695475 | Val Accuracy = 0.8312222222222222\n",
      "Loss = 0.2780746043375013 | Training Accuracy = 0.8968809523809523 | Val Loss = 0.5105261170958788 | Val Accuracy = 0.8342222222222222\n",
      "Loss = 0.27778603991320705 | Training Accuracy = 0.8940714285714285 | Val Loss = 0.5232272957542385 | Val Accuracy = 0.8318888888888889\n",
      "Loss = 0.2773389062674604 | Training Accuracy = 0.8962857142857142 | Val Loss = 0.5301314035601911 | Val Accuracy = 0.8322222222222222\n",
      "Loss = 0.27458663281460755 | Training Accuracy = 0.8943333333333333 | Val Loss = 0.5296739628253584 | Val Accuracy = 0.8321666666666667\n",
      "Loss = 0.2760604577807202 | Training Accuracy = 0.896952380952381 | Val Loss = 0.5211938424376251 | Val Accuracy = 0.8342222222222222\n",
      "Loss = 0.2715692399665885 | Training Accuracy = 0.8912619047619048 | Val Loss = 0.5191370670423722 | Val Accuracy = 0.8285555555555556\n",
      "Loss = 0.2725794980007988 | Training Accuracy = 0.8939047619047619 | Val Loss = 0.5225351002158101 | Val Accuracy = 0.8327222222222223\n",
      "Loss = 0.2715854758409337 | Training Accuracy = 0.8926190476190476 | Val Loss = 0.5130187878731511 | Val Accuracy = 0.8325\n",
      "Loss = 0.26957635424986776 | Training Accuracy = 0.8930476190476191 | Val Loss = 0.5366959843701555 | Val Accuracy = 0.8307222222222223\n",
      "Loss = 0.2691140047522296 | Training Accuracy = 0.8978095238095238 | Val Loss = 0.5191991726055888 | Val Accuracy = 0.8339444444444445\n",
      "Loss = 0.26517085100664334 | Training Accuracy = 0.8990714285714285 | Val Loss = 0.5038042859258526 | Val Accuracy = 0.8335\n",
      "Loss = 0.26599255925023824 | Training Accuracy = 0.8943571428571429 | Val Loss = 0.5108409359371116 | Val Accuracy = 0.8304444444444444\n",
      "Loss = 0.2638848549043668 | Training Accuracy = 0.8985714285714286 | Val Loss = 0.49698439600471284 | Val Accuracy = 0.8343888888888888\n",
      "Loss = 0.2649420447389005 | Training Accuracy = 0.900452380952381 | Val Loss = 0.5026520127501347 | Val Accuracy = 0.8338333333333333\n",
      "Loss = 0.26177408621667725 | Training Accuracy = 0.9003333333333333 | Val Loss = 0.5161709620934593 | Val Accuracy = 0.8347777777777777\n",
      "Loss = 0.2587868235785118 | Training Accuracy = 0.8958571428571429 | Val Loss = 0.5070030313614946 | Val Accuracy = 0.8315\n",
      "Loss = 0.25846708564379745 | Training Accuracy = 0.9013333333333333 | Val Loss = 0.5066532996147235 | Val Accuracy = 0.8339444444444445\n",
      "Loss = 0.2563571027150951 | Training Accuracy = 0.9001666666666667 | Val Loss = 0.4933852101828791 | Val Accuracy = 0.8339444444444445\n",
      "Loss = 0.25819172035631777 | Training Accuracy = 0.8999761904761905 | Val Loss = 0.5217072240581349 | Val Accuracy = 0.8336111111111111\n",
      "Loss = 0.2572993322190264 | Training Accuracy = 0.8987619047619048 | Val Loss = 0.5149166202904756 | Val Accuracy = 0.8312222222222222\n",
      "Loss = 0.2559398324803953 | Training Accuracy = 0.902 | Val Loss = 0.4982374813312893 | Val Accuracy = 0.8351111111111111\n",
      "Loss = 0.25179190456339234 | Training Accuracy = 0.8966190476190476 | Val Loss = 0.5040446196029008 | Val Accuracy = 0.8288333333333333\n",
      "Loss = 0.2529674335560969 | Training Accuracy = 0.8930238095238096 | Val Loss = 0.5091705644316206 | Val Accuracy = 0.8280555555555555\n",
      "Loss = 0.2519496458164394 | Training Accuracy = 0.9056904761904762 | Val Loss = 0.4799223776428933 | Val Accuracy = 0.8355555555555556\n",
      "Loss = 0.2524968456760687 | Training Accuracy = 0.9002857142857142 | Val Loss = 0.49682499554699416 | Val Accuracy = 0.8315\n",
      "Loss = 0.25161097220879 | Training Accuracy = 0.8999285714285714 | Val Loss = 0.5081335885694481 | Val Accuracy = 0.8315\n",
      "Loss = 0.251129400471993 | Training Accuracy = 0.9055238095238095 | Val Loss = 0.5037516379836255 | Val Accuracy = 0.8345555555555556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.2519462912642164 | Training Accuracy = 0.8978095238095238 | Val Loss = 0.5000282992111788 | Val Accuracy = 0.8285555555555556\n",
      "Loss = 0.24965477884546253 | Training Accuracy = 0.8987142857142857 | Val Loss = 0.49709169271677667 | Val Accuracy = 0.8296666666666667\n",
      "Loss = 0.24758549752914594 | Training Accuracy = 0.9011666666666667 | Val Loss = 0.489526846846339 | Val Accuracy = 0.8346111111111111\n",
      "Loss = 0.24743078404763408 | Training Accuracy = 0.904 | Val Loss = 0.48069053897099623 | Val Accuracy = 0.8328888888888889\n",
      "Loss = 0.24652886445490912 | Training Accuracy = 0.9028333333333334 | Val Loss = 0.48886690547027967 | Val Accuracy = 0.835\n",
      "Loss = 0.24773147434742115 | Training Accuracy = 0.8985 | Val Loss = 0.4836099940151779 | Val Accuracy = 0.8318888888888889\n",
      "Loss = 0.2494260628829373 | Training Accuracy = 0.9012142857142857 | Val Loss = 0.49932220346783157 | Val Accuracy = 0.8320555555555555\n",
      "Loss = 0.24891352724073307 | Training Accuracy = 0.9027142857142857 | Val Loss = 0.4857760507544591 | Val Accuracy = 0.8332222222222222\n",
      "Loss = 0.246380848761795 | Training Accuracy = 0.9002142857142857 | Val Loss = 0.48878843932759236 | Val Accuracy = 0.8312222222222222\n",
      "Loss = 0.2436040099520411 | Training Accuracy = 0.8996190476190477 | Val Loss = 0.5162113294675257 | Val Accuracy = 0.8314444444444444\n",
      "Loss = 0.24607862611387946 | Training Accuracy = 0.9020238095238096 | Val Loss = 0.5054048897041573 | Val Accuracy = 0.8321666666666667\n",
      "Loss = 0.2421467583155441 | Training Accuracy = 0.9020476190476191 | Val Loss = 0.5180896064598836 | Val Accuracy = 0.8331111111111111\n",
      "Loss = 0.24046457633858184 | Training Accuracy = 0.901 | Val Loss = 0.5071428233396754 | Val Accuracy = 0.8327222222222223\n",
      "Loss = 0.24248644983570564 | Training Accuracy = 0.8993333333333333 | Val Loss = 0.48176291443014707 | Val Accuracy = 0.8301111111111111\n"
     ]
    }
   ],
   "source": [
    "## input size\n",
    "input_dim = Xtrain.shape[1]\n",
    "\n",
    "## hyperparameters\n",
    "iterations = 200\n",
    "learning_rate = 0.01\n",
    "hidden_nodes = 1000\n",
    "output_nodes = 10\n",
    "\n",
    "## define neural net\n",
    "nn = NN()\n",
    "nn.add_layer(Linear(input_dim, hidden_nodes))\n",
    "nn.add_layer(ReLU())\n",
    "nn.add_layer(Linear(hidden_nodes, output_nodes))\n",
    "\n",
    "nn = train(nn, Xtrain, y_train, minibatch_size=200, epoch=iterations, \\\n",
    "           learning_rate=learning_rate, X_val=Xtest, y_val=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network Using Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify best hyper parameters, create ensemble and apply model on Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 1: 50 nodes X 4 layers X 5 ensemble X 100 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 models to ensemble\n",
    "model1 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model2 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model3 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model4 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model5 = KerasClassifier(build_fn = mlp_model, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_clf = VotingClassifier(estimators = [('model1', model1), ('model2', model2), ('model3', model3), ('model4', model4), ('model5', model5)], voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 14s 329us/step - loss: 2.0933 - acc: 0.2178\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.7744 - acc: 0.3649\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.6267 - acc: 0.4370\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.5533 - acc: 0.4666\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.5147 - acc: 0.4867\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.4579 - acc: 0.5146\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.4171 - acc: 0.5382\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.3859 - acc: 0.5507\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.3503 - acc: 0.5663\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.3246 - acc: 0.5774\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.3043 - acc: 0.5844\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.2912 - acc: 0.5923\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.2813 - acc: 0.5977\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.2608 - acc: 0.6045\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.2527 - acc: 0.6092\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.2491 - acc: 0.6060\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.2442 - acc: 0.6102\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.2385 - acc: 0.6135\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.2247 - acc: 0.6193 0s - loss: 1.2245 - acc: 0.619\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.2269 - acc: 0.6187\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.2164 - acc: 0.6214\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.2141 - acc: 0.6225\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.2092 - acc: 0.6224\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.2053 - acc: 0.6274\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 8s 199us/step - loss: 1.2069 - acc: 0.6246\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.2008 - acc: 0.6292\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1918 - acc: 0.6290\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 1.1948 - acc: 0.6306\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1783 - acc: 0.6337\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1865 - acc: 0.6301\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.1758 - acc: 0.6364\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1769 - acc: 0.6354\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1700 - acc: 0.6364\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1709 - acc: 0.6369\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1666 - acc: 0.6377\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.1614 - acc: 0.6409\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1573 - acc: 0.6412\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1579 - acc: 0.6425\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1601 - acc: 0.6417\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1519 - acc: 0.6425\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1542 - acc: 0.6415\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.1489 - acc: 0.6449\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1479 - acc: 0.6440\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1523 - acc: 0.6435\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1430 - acc: 0.6450\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1456 - acc: 0.6451\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1453 - acc: 0.6462\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1383 - acc: 0.6483\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1366 - acc: 0.6491\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1414 - acc: 0.6465\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 1.1355 - acc: 0.649 - 8s 181us/step - loss: 1.1355 - acc: 0.6497\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.1241 - acc: 0.6526\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1393 - acc: 0.6492\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.1351 - acc: 0.6491\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.1309 - acc: 0.6530\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.1310 - acc: 0.6491\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.1364 - acc: 0.6494\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1313 - acc: 0.6468\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.1340 - acc: 0.6499\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1274 - acc: 0.6517\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1279 - acc: 0.6495\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1245 - acc: 0.6519\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1239 - acc: 0.6495\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1190 - acc: 0.6537\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1210 - acc: 0.6545\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1196 - acc: 0.6531\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1263 - acc: 0.6523\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1122 - acc: 0.6561\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1121 - acc: 0.6580\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1132 - acc: 0.6565\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1072 - acc: 0.6582\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1069 - acc: 0.6560\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.1132 - acc: 0.6590\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.1055 - acc: 0.6600\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1161 - acc: 0.6562\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.1128 - acc: 0.6588\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1059 - acc: 0.6610\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1067 - acc: 0.6570\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1004 - acc: 0.6607\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.1022 - acc: 0.6587\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0988 - acc: 0.6604\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0993 - acc: 0.6602\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.1045 - acc: 0.6586\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.0947 - acc: 0.6632\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0929 - acc: 0.6620\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0856 - acc: 0.6659\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0953 - acc: 0.6620\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0973 - acc: 0.6616\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.0920 - acc: 0.6646\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1003 - acc: 0.6610\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0881 - acc: 0.6610 1s - los\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0965 - acc: 0.6596\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0963 - acc: 0.6639\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0886 - acc: 0.6653 1s - lo\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0923 - acc: 0.6635 1s - loss: 1.\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0975 - acc: 0.6633\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0874 - acc: 0.6633\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0883 - acc: 0.6634 1s\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0861 - acc: 0.6639\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0883 - acc: 0.6643\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 2.0432 - acc: 0.240 - 9s 225us/step - loss: 2.0434 - acc: 0.2404\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.6846 - acc: 0.4055\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.5734 - acc: 0.4575\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.5078 - acc: 0.4879\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.4594 - acc: 0.5040\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.4276 - acc: 0.5261\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.3879 - acc: 0.5484\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.3515 - acc: 0.5706\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.3237 - acc: 0.5809\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.3124 - acc: 0.5884\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.2795 - acc: 0.6005\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.2703 - acc: 0.6023\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.2614 - acc: 0.6087\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.2447 - acc: 0.6140\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.2399 - acc: 0.6140\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.2224 - acc: 0.6223\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.2321 - acc: 0.6178\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.2210 - acc: 0.6182\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.2075 - acc: 0.6271\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.2013 - acc: 0.6271\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1942 - acc: 0.6308\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.1921 - acc: 0.6309\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1875 - acc: 0.6332\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1875 - acc: 0.6325\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 7s 179us/step - loss: 1.1832 - acc: 0.6348\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1805 - acc: 0.6350\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1669 - acc: 0.6409\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.1732 - acc: 0.6376\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1567 - acc: 0.6439\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1668 - acc: 0.6405\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1640 - acc: 0.6439\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1579 - acc: 0.6422\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.1518 - acc: 0.6433\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1587 - acc: 0.6419\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.1399 - acc: 0.6485\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1477 - acc: 0.6463\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.1385 - acc: 0.6489\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1481 - acc: 0.6439\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1305 - acc: 0.6520\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1480 - acc: 0.6433\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.1331 - acc: 0.6510\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.1321 - acc: 0.6507\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1349 - acc: 0.6516\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1251 - acc: 0.6536\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1275 - acc: 0.6508\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.1235 - acc: 0.6532\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1241 - acc: 0.6556\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1213 - acc: 0.6545\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.1214 - acc: 0.6540\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1204 - acc: 0.6573\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 1.1263 - acc: 0.654 - 7s 178us/step - loss: 1.1265 - acc: 0.6548\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.1149 - acc: 0.6567\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.1218 - acc: 0.6550\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.1148 - acc: 0.6580\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.1153 - acc: 0.6596\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1132 - acc: 0.6570\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.1143 - acc: 0.6578\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.1032 - acc: 0.6593\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.1104 - acc: 0.6571\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.1018 - acc: 0.6581\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1026 - acc: 0.6622\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.1070 - acc: 0.6594\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.1049 - acc: 0.6603\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.0904 - acc: 0.6641 1s - loss: 1.0914 - acc: 0.66 - ETA: 1s - \n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 9s 204us/step - loss: 1.1002 - acc: 0.6628\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 1.0981 - acc: 0.6614\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 10s 237us/step - loss: 1.1028 - acc: 0.6575\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 1.1038 - acc: 0.6612\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.0950 - acc: 0.6642\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 9s 204us/step - loss: 1.0908 - acc: 0.6647\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.0888 - acc: 0.6665\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0963 - acc: 0.6623\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0893 - acc: 0.6638\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.0876 - acc: 0.6623\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 1.0877 - acc: 0.6667\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.0873 - acc: 0.6695\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0901 - acc: 0.6655\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0892 - acc: 0.6638\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0812 - acc: 0.6648\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0828 - acc: 0.6683\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0851 - acc: 0.6657\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0914 - acc: 0.6653\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 1.0866 - acc: 0.668 - 7s 178us/step - loss: 1.0866 - acc: 0.6680\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0840 - acc: 0.6650\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0742 - acc: 0.6706\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0819 - acc: 0.6661\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0776 - acc: 0.6678\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0825 - acc: 0.6686\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0751 - acc: 0.6690\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0710 - acc: 0.6702\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0735 - acc: 0.6710\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0793 - acc: 0.6683\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0816 - acc: 0.6698\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0764 - acc: 0.6702\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0654 - acc: 0.6719\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0787 - acc: 0.6671\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0762 - acc: 0.6683\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0783 - acc: 0.6677\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0666 - acc: 0.6715\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0730 - acc: 0.6699\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 10s 233us/step - loss: 2.0560 - acc: 0.2378\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.7036 - acc: 0.4008\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.5815 - acc: 0.4556\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.5249 - acc: 0.4839\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.4743 - acc: 0.5153\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.4316 - acc: 0.5320\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.3924 - acc: 0.5532\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.3659 - acc: 0.5661\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 1.3406 - acc: 0.579 - 8s 182us/step - loss: 1.3406 - acc: 0.5797\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.3169 - acc: 0.5875\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.3002 - acc: 0.5898\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.2817 - acc: 0.5983\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.2589 - acc: 0.6069\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.2550 - acc: 0.6099\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.2405 - acc: 0.6120\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.2308 - acc: 0.6158\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.2286 - acc: 0.6178\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.2231 - acc: 0.6222\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.2130 - acc: 0.6255\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 1.2020 - acc: 0.626 - 8s 187us/step - loss: 1.2014 - acc: 0.6269\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.2024 - acc: 0.6274\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 1.1982 - acc: 0.6296\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.1952 - acc: 0.6335\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.1846 - acc: 0.6349\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 1.1821 - acc: 0.6370\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1764 - acc: 0.6367\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 1.1759 - acc: 0.6359\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 1.1777 - acc: 0.6354\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.1720 - acc: 0.6386\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.1606 - acc: 0.6426\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.1560 - acc: 0.6448\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.1504 - acc: 0.6445\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1567 - acc: 0.6432\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 7s 179us/step - loss: 1.1540 - acc: 0.6446\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1540 - acc: 0.6457\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.1473 - acc: 0.6435\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1399 - acc: 0.6484\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.1393 - acc: 0.6484\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.1401 - acc: 0.6487\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1349 - acc: 0.6527\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1298 - acc: 0.6508\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.1374 - acc: 0.6512\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.1278 - acc: 0.6541\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 1.1365 - acc: 0.6510\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1397 - acc: 0.6492\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.1226 - acc: 0.6531\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.1203 - acc: 0.6554\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1156 - acc: 0.6554\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.1251 - acc: 0.6547\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.1233 - acc: 0.6550\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1119 - acc: 0.6586\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 8s 200us/step - loss: 1.1155 - acc: 0.6588\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 1.1075 - acc: 0.6576\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.1188 - acc: 0.6533\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 1.1044 - acc: 0.6608\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 1.1035 - acc: 0.6606\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 1.1088 - acc: 0.6595\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 8s 201us/step - loss: 1.1002 - acc: 0.6610\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 10s 236us/step - loss: 1.1048 - acc: 0.6600\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 1.1123 - acc: 0.6582\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 9s 205us/step - loss: 1.1008 - acc: 0.6602\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.1068 - acc: 0.6571\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 8s 202us/step - loss: 1.1055 - acc: 0.6617\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 1.1018 - acc: 0.6618\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 1.0987 - acc: 0.6612\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 9s 204us/step - loss: 1.0958 - acc: 0.6640\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 1.0963 - acc: 0.6632\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 1.0977 - acc: 0.6613\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 9s 204us/step - loss: 1.0947 - acc: 0.6615\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.0945 - acc: 0.6615\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 1.0871 - acc: 0.6655\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.0905 - acc: 0.6635\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0914 - acc: 0.6651\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.0867 - acc: 0.6670\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0825 - acc: 0.6692\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0964 - acc: 0.6624\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0868 - acc: 0.6650\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0895 - acc: 0.6676\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0892 - acc: 0.6645\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0848 - acc: 0.6660\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0788 - acc: 0.6676\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0773 - acc: 0.6699\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0860 - acc: 0.6655\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0791 - acc: 0.6671\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0790 - acc: 0.6699\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0877 - acc: 0.6629\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0825 - acc: 0.6671\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0841 - acc: 0.6645\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0809 - acc: 0.6680\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0715 - acc: 0.6708\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0736 - acc: 0.6693\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0718 - acc: 0.6699\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0724 - acc: 0.6700\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0686 - acc: 0.6720\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0660 - acc: 0.6717\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0682 - acc: 0.6731\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 7s 179us/step - loss: 1.0692 - acc: 0.6691\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0646 - acc: 0.6711\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0757 - acc: 0.6705\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0700 - acc: 0.6715\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 2.0458 - acc: 0.2376\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.7072 - acc: 0.3971\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.5952 - acc: 0.4514\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.5204 - acc: 0.4871\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.4664 - acc: 0.5100\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.4295 - acc: 0.5287\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.3837 - acc: 0.5498\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.3479 - acc: 0.5682\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.3215 - acc: 0.5779\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.2972 - acc: 0.5859\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.2865 - acc: 0.5950\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.2652 - acc: 0.6029\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.2418 - acc: 0.6087\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.2441 - acc: 0.6101\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.2342 - acc: 0.6099\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.2255 - acc: 0.6162\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.2120 - acc: 0.6213\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.2129 - acc: 0.6207\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1852 - acc: 0.6297\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1881 - acc: 0.6297\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1840 - acc: 0.6310\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1836 - acc: 0.6299\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1768 - acc: 0.6331\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1659 - acc: 0.6352\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 1.1610 - acc: 0.6375\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 1.1531 - acc: 0.6432\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1561 - acc: 0.6428\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1489 - acc: 0.6423\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1407 - acc: 0.6456\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1410 - acc: 0.6458\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1453 - acc: 0.6445\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1317 - acc: 0.6471\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1305 - acc: 0.6476\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.1281 - acc: 0.6505\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1232 - acc: 0.6516\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.1252 - acc: 0.6480\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1235 - acc: 0.6507\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1221 - acc: 0.6511\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1223 - acc: 0.6538\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1154 - acc: 0.6542\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1049 - acc: 0.6563\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1178 - acc: 0.6515\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1059 - acc: 0.6565\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1107 - acc: 0.6554\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.1114 - acc: 0.6563\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0965 - acc: 0.6592\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.0990 - acc: 0.6592\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0995 - acc: 0.6580\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.1007 - acc: 0.6573\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0972 - acc: 0.6602\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0942 - acc: 0.6602\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0952 - acc: 0.6589\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0856 - acc: 0.6632\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0861 - acc: 0.6634\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.0835 - acc: 0.6624\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0904 - acc: 0.6624\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0825 - acc: 0.6630\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0945 - acc: 0.6623\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0813 - acc: 0.6654\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.0861 - acc: 0.6627\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.0794 - acc: 0.6665\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.0793 - acc: 0.6636\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0790 - acc: 0.6645\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0724 - acc: 0.6663\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0736 - acc: 0.6679\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0710 - acc: 0.6688\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0717 - acc: 0.6667\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0679 - acc: 0.6670\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.0722 - acc: 0.6664\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.0678 - acc: 0.6712\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0652 - acc: 0.6672\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0681 - acc: 0.6678\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0687 - acc: 0.6668\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0609 - acc: 0.6692\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0700 - acc: 0.6681\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0683 - acc: 0.6692\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0579 - acc: 0.6707\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0632 - acc: 0.6697\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.0628 - acc: 0.6701\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0597 - acc: 0.6722\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0528 - acc: 0.6729\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0600 - acc: 0.6681\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0578 - acc: 0.6721\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0568 - acc: 0.6704\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.0582 - acc: 0.6705\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.0542 - acc: 0.6729\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.0534 - acc: 0.6740\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0587 - acc: 0.6700\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0467 - acc: 0.6736\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.0546 - acc: 0.6728\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.0543 - acc: 0.6741\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.0563 - acc: 0.6725\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 1.0437 - acc: 0.6784\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.0600 - acc: 0.6709\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.0485 - acc: 0.6745\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 8s 200us/step - loss: 1.0442 - acc: 0.6782\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 1.0438 - acc: 0.6754\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.0509 - acc: 0.6740\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 1.0557 - acc: 0.6715\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.0464 - acc: 0.6761\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 11s 271us/step - loss: 2.1115 - acc: 0.2075\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.7436 - acc: 0.3879\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 8s 199us/step - loss: 1.5838 - acc: 0.4632\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.5101 - acc: 0.4962\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.4612 - acc: 0.5198\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.4186 - acc: 0.5386\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.3761 - acc: 0.5584\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.3507 - acc: 0.5662\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.3250 - acc: 0.5775\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.2970 - acc: 0.5890 1s -\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.2840 - acc: 0.5978\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.2730 - acc: 0.6016\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 1.2558 - acc: 0.6068\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.2366 - acc: 0.6124\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.2326 - acc: 0.6138\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.2254 - acc: 0.6179\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.2220 - acc: 0.6211\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.2115 - acc: 0.6221\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.2015 - acc: 0.6247\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 1.2013 - acc: 0.6275\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.1832 - acc: 0.6311\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 1.1845 - acc: 0.6328\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 1.1880 - acc: 0.6317\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 8s 199us/step - loss: 1.1789 - acc: 0.6349\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 1.1740 - acc: 0.6360\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.1708 - acc: 0.6362\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1659 - acc: 0.6394\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.1613 - acc: 0.6399\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1538 - acc: 0.6454\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.1503 - acc: 0.6421\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1496 - acc: 0.6434\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 1.1528 - acc: 0.6450\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1432 - acc: 0.6463\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 1.1430 - acc: 0.6476\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.1440 - acc: 0.6468\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 1.1409 - acc: 0.6479\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1372 - acc: 0.6491\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1282 - acc: 0.6492\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.1330 - acc: 0.6502\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.1263 - acc: 0.6524\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 1.1182 - acc: 0.654 - 8s 185us/step - loss: 1.1182 - acc: 0.6544\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1306 - acc: 0.6498\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 1.1281 - acc: 0.649 - 8s 192us/step - loss: 1.1277 - acc: 0.6500\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1252 - acc: 0.6523\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1129 - acc: 0.6559\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1142 - acc: 0.6536\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 1.1116 - acc: 0.6568\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.1142 - acc: 0.6529\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1060 - acc: 0.6572\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.1163 - acc: 0.6558 0s - loss: 1.1172 \n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.1073 - acc: 0.6543\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 1.1050 - acc: 0.6589\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 1.1107 - acc: 0.6567\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 1.1045 - acc: 0.6572 0s - loss: 1.1025\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 1.0983 - acc: 0.6620\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.1033 - acc: 0.6584\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 1.1024 - acc: 0.6556\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 1.1020 - acc: 0.6575\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 1.1009 - acc: 0.6587\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 1.1060 - acc: 0.6604\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 1.0967 - acc: 0.661 - 9s 209us/step - loss: 1.0967 - acc: 0.6612\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 1.0954 - acc: 0.6605\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 9s 216us/step - loss: 1.0885 - acc: 0.6619\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.0854 - acc: 0.6637\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 1.0952 - acc: 0.6605\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 1.0932 - acc: 0.6613\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 9s 226us/step - loss: 1.0986 - acc: 0.6603\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 1.0890 - acc: 0.6634\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 1.0907 - acc: 0.6666\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 1.0933 - acc: 0.6619\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 1.0751 - acc: 0.6653\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 1.0923 - acc: 0.6606\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 1.0877 - acc: 0.6645\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 1.0819 - acc: 0.6658\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 1.0919 - acc: 0.6615\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 1.0833 - acc: 0.6663\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 1.0911 - acc: 0.6629\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 1.0809 - acc: 0.6647\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.0837 - acc: 0.6650\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 1.0728 - acc: 0.6675\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 1.0824 - acc: 0.6677\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 1.0890 - acc: 0.6629\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 1.0819 - acc: 0.6637\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 1.0835 - acc: 0.6637\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 1.0808 - acc: 0.6686\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 1.0727 - acc: 0.6680\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.0783 - acc: 0.6678\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 1.0810 - acc: 0.6685\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 1.0701 - acc: 0.6702\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.0795 - acc: 0.6663\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 1.0729 - acc: 0.6668\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 1.0755 - acc: 0.6688 0s - loss: 1.0764 - \n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 1.0661 - acc: 0.6723\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 1.0720 - acc: 0.6689\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 1.0723 - acc: 0.6681\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 1.0701 - acc: 0.6681\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 1.0677 - acc: 0.6699\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 1.0690 - acc: 0.6710\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.0743 - acc: 0.6692\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 1.0651 - acc: 0.6720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('model1',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000000513CC88>),\n",
       "                             ('model2',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000000513CA20>),\n",
       "                             ('model3',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000000B91D4A8>),\n",
       "                             ('model4',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000000B91D278>),\n",
       "                             ('model5',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000000B91DBA8>)],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_clf.fit(Xtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Validation:  0.8183333333333334\n"
     ]
    }
   ],
   "source": [
    "y_pred_val1 = ensemble_clf.predict(Xval)\n",
    "print('Accuracy on Validation: ', accuracy_score(y_pred_val1, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing:  0.8039444444444445\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = ensemble_clf.predict(Xtest)\n",
    "print('Accuracy on Testing: ', accuracy_score(y_pred1, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 2: 200 nodes X 4 layers X 5 ensemble X 100 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model2():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(200, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 models to ensemble\n",
    "model21 = KerasClassifier(build_fn = mlp_model2, epochs = 100)\n",
    "model22 = KerasClassifier(build_fn = mlp_model2, epochs = 100)\n",
    "model23 = KerasClassifier(build_fn = mlp_model2, epochs = 100)\n",
    "model24 = KerasClassifier(build_fn = mlp_model2, epochs = 100)\n",
    "model25 = KerasClassifier(build_fn = mlp_model2, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_clf2 = VotingClassifier(estimators = [('model21', model21), ('model22', model22), ('model23', model23), ('model24', model24), ('model25', model25)], voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 23s 547us/step - loss: 1.8897 - acc: 0.3216\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 1.4349 - acc: 0.5287\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 21s 489us/step - loss: 1.2652 - acc: 0.59890s - loss: 1.2659 - acc: 0.5\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 1.1766 - acc: 0.6298\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 1.1181 - acc: 0.6497\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 1.0718 - acc: 0.6634\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 1.0377 - acc: 0.6761\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.9967 - acc: 0.6895\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.9734 - acc: 0.6940\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.9527 - acc: 0.7025\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.9204 - acc: 0.7141\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.9119 - acc: 0.7174\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 17s 410us/step - loss: 0.8884 - acc: 0.7255\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.8755 - acc: 0.7258\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.8664 - acc: 0.7299\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.8524 - acc: 0.73501s - l\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.8426 - acc: 0.7395\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.8356 - acc: 0.7401\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.8243 - acc: 0.7419\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.8143 - acc: 0.74520s - loss: 0.8149 - acc: \n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 0.8080 - acc: 0.74961s - loss: \n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.7964 - acc: 0.7516\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.7858 - acc: 0.7556\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.7894 - acc: 0.75440s - loss: 0.7894 - acc: 0.754\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.7711 - acc: 0.7581\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.7688 - acc: 0.7613\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.7599 - acc: 0.7614\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.7532 - acc: 0.7673\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 18s 419us/step - loss: 0.7527 - acc: 0.76580s - loss: 0.7\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.7497 - acc: 0.7680\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.7401 - acc: 0.7702\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.7355 - acc: 0.7714\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.7315 - acc: 0.7730\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.7287 - acc: 0.7737\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.7180 - acc: 0.7756\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.7152 - acc: 0.7763\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.7169 - acc: 0.7779\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.7106 - acc: 0.77920s - loss: 0.7\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.7120 - acc: 0.7796\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.7000 - acc: 0.7827\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6957 - acc: 0.7837\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6943 - acc: 0.7830\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.7005 - acc: 0.7818\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6960 - acc: 0.78371s\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.6848 - acc: 0.7895\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.6783 - acc: 0.7881\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.6800 - acc: 0.78990s - loss: 0.6\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.6897 - acc: 0.7864\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 18s 419us/step - loss: 0.6716 - acc: 0.7902\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6734 - acc: 0.7906\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 18s 419us/step - loss: 0.6727 - acc: 0.78903s - loss: 0.6693 - acc: 0.790 - ETA: 3s - l\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6768 - acc: 0.7892\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.6654 - acc: 0.7923\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6672 - acc: 0.7927\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.6651 - acc: 0.7921\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6549 - acc: 0.7969\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.6654 - acc: 0.7929\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.6564 - acc: 0.7950\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 18s 419us/step - loss: 0.6541 - acc: 0.7951\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6539 - acc: 0.7963\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 18s 421us/step - loss: 0.6502 - acc: 0.7978\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.6465 - acc: 0.79921s - \n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.6464 - acc: 0.79770s - loss: 0.6463 - \n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.6441 - acc: 0.7994\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6381 - acc: 0.8017\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6477 - acc: 0.7977\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 18s 419us/step - loss: 0.6394 - acc: 0.80102s  - ETA: 0s - loss: 0\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.6369 - acc: 0.8012\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.6294 - acc: 0.80283s  - ET\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.6374 - acc: 0.7999\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.6323 - acc: 0.8035\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6308 - acc: 0.8019\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 17s 417us/step - loss: 0.6314 - acc: 0.8009\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6294 - acc: 0.8037\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 17s 417us/step - loss: 0.6245 - acc: 0.8054\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.6292 - acc: 0.8033\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.6266 - acc: 0.8057\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6253 - acc: 0.8034\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6216 - acc: 0.80540s - loss: 0.6\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6205 - acc: 0.80510s - loss: 0.6199 - a\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6179 - acc: 0.8080\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6188 - acc: 0.8065\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6242 - acc: 0.8075\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.6211 - acc: 0.8058\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6088 - acc: 0.8091\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.6184 - acc: 0.8072\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6081 - acc: 0.8106\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6138 - acc: 0.80900s - loss: 0.6149 - acc: 0\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.6120 - acc: 0.8082\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 18s 421us/step - loss: 0.6112 - acc: 0.8093\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.6072 - acc: 0.8114\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.6042 - acc: 0.8105\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6092 - acc: 0.8096ETA: 2s - loss: 0.6107 - a \n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6037 - acc: 0.8113\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.6038 - acc: 0.8113\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.5980 - acc: 0.8124\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6036 - acc: 0.8100\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5939 - acc: 0.8150\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5991 - acc: 0.81341s \n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.6010 - acc: 0.8125\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 24s 580us/step - loss: 1.8152 - acc: 0.3635\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 1.3958 - acc: 0.54790s - loss: 1.3979\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 1.2610 - acc: 0.6000\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 1.1841 - acc: 0.6284\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 1.1226 - acc: 0.6477\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 1.0895 - acc: 0.6584\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 1.0428 - acc: 0.67416s\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 1.0086 - acc: 0.6855\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.9779 - acc: 0.6972\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.9559 - acc: 0.70260s - loss: 0\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.9277 - acc: 0.70960s - loss: 0.9\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.9148 - acc: 0.71661s\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.8892 - acc: 0.7235\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.8701 - acc: 0.7290\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.8626 - acc: 0.73120s - loss: 0.8617 - ac\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.8561 - acc: 0.7343\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.8381 - acc: 0.7385\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.8316 - acc: 0.7414\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.8204 - acc: 0.7464\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.8044 - acc: 0.74950s - loss: 0.\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.7975 - acc: 0.7520\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.7911 - acc: 0.7537\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.7821 - acc: 0.7577\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.7753 - acc: 0.76003s - loss: 0.7726 - acc: - ETA: 3s - - ETA:\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.7704 - acc: 0.7601\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.7617 - acc: 0.7621\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.7550 - acc: 0.7652\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.7588 - acc: 0.7628\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.7313 - acc: 0.7719\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.7345 - acc: 0.7725\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.7360 - acc: 0.77020s - loss: 0.7353 - acc: 0.7\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.7324 - acc: 0.7735\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.7222 - acc: 0.7757\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.7171 - acc: 0.7780\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.7164 - acc: 0.77640s - loss: 0.7157 - acc: 0\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.7047 - acc: 0.7804\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6973 - acc: 0.7810\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6989 - acc: 0.7856\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.6929 - acc: 0.7847\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.6957 - acc: 0.7820\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.6830 - acc: 0.7873\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.6868 - acc: 0.7862\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.6827 - acc: 0.7875\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6842 - acc: 0.7869\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.6793 - acc: 0.7878\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6789 - acc: 0.7893\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.6724 - acc: 0.78880s - loss: 0.6717 - ac\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6669 - acc: 0.7916\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6658 - acc: 0.7912\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6647 - acc: 0.7912\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6638 - acc: 0.7945\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6593 - acc: 0.79433s - l\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6551 - acc: 0.7957\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.6593 - acc: 0.7949\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6495 - acc: 0.7997\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6489 - acc: 0.7991\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6424 - acc: 0.7995\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 18s 419us/step - loss: 0.6479 - acc: 0.7983\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.6430 - acc: 0.7979\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.6431 - acc: 0.7997\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.6404 - acc: 0.7990\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.6352 - acc: 0.8025\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 17s 413us/step - loss: 0.6344 - acc: 0.8010\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6351 - acc: 0.8017\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 17s 412us/step - loss: 0.6355 - acc: 0.8018\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 17s 413us/step - loss: 0.6252 - acc: 0.80460s - loss: 0.6255 - acc: 0.8\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6293 - acc: 0.8040\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.6178 - acc: 0.8069\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 17s 417us/step - loss: 0.6208 - acc: 0.8057\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 17s 412us/step - loss: 0.6177 - acc: 0.8078\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 17s 409us/step - loss: 0.6218 - acc: 0.8056\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 17s 410us/step - loss: 0.6221 - acc: 0.80731s -  - ETA: 0s - loss: 0.6210 - acc: \n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6157 - acc: 0.8081\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.6141 - acc: 0.8081\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 17s 411us/step - loss: 0.6144 - acc: 0.8061\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.6090 - acc: 0.81083s -\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.6128 - acc: 0.80730s - loss: 0.61\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 17s 411us/step - loss: 0.6078 - acc: 0.8105\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 17s 410us/step - loss: 0.6031 - acc: 0.8106\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 17s 410us/step - loss: 0.6041 - acc: 0.8106\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 17s 412us/step - loss: 0.6093 - acc: 0.8107\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 17s 413us/step - loss: 0.6090 - acc: 0.8093\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 17s 412us/step - loss: 0.6057 - acc: 0.8105\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6040 - acc: 0.8119\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5933 - acc: 0.8135\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 17s 411us/step - loss: 0.6007 - acc: 0.8130\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 17s 409us/step - loss: 0.5925 - acc: 0.8137\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 17s 409us/step - loss: 0.5961 - acc: 0.8118\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 17s 409us/step - loss: 0.5968 - acc: 0.8133\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 17s 410us/step - loss: 0.5888 - acc: 0.8149\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 17s 410us/step - loss: 0.5896 - acc: 0.8141\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 17s 407us/step - loss: 0.5925 - acc: 0.8156\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.5876 - acc: 0.8156\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.5915 - acc: 0.8146\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.5777 - acc: 0.8182\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.5906 - acc: 0.8157\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.5909 - acc: 0.8127\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.5891 - acc: 0.8137\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5850 - acc: 0.8180\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.5886 - acc: 0.8140\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 22s 529us/step - loss: 1.8324 - acc: 0.3501\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 1.4092 - acc: 0.5360\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 1.2653 - acc: 0.59661s - loss\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 1.1881 - acc: 0.6241\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 1.1299 - acc: 0.6447\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 1.0812 - acc: 0.6612\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 1.0462 - acc: 0.6727\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 1.0062 - acc: 0.6859\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.9884 - acc: 0.6927\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.9595 - acc: 0.7018\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.9341 - acc: 0.70921s - loss\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.9174 - acc: 0.7131\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.8975 - acc: 0.7193\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.8803 - acc: 0.7256\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.8731 - acc: 0.7284\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.8654 - acc: 0.7305\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.8454 - acc: 0.73840s - loss: 0.844\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.8413 - acc: 0.7384\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.8247 - acc: 0.7433\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.8171 - acc: 0.7460\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.8104 - acc: 0.7506\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.7969 - acc: 0.7530\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.7909 - acc: 0.7530\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.7811 - acc: 0.7566\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.7710 - acc: 0.7589\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.7658 - acc: 0.7616\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 18s 421us/step - loss: 0.7571 - acc: 0.7642\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.7613 - acc: 0.7616\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.7504 - acc: 0.76690s - loss: 0.7511 - \n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.7460 - acc: 0.7680\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.7368 - acc: 0.7723\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.7328 - acc: 0.7702\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.7328 - acc: 0.7729\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.7226 - acc: 0.7759\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.7194 - acc: 0.7750\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.7104 - acc: 0.7766\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.7133 - acc: 0.7773\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.7092 - acc: 0.7792\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.7127 - acc: 0.7773\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6970 - acc: 0.78220s - loss: 0.6\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.6977 - acc: 0.7829\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.7001 - acc: 0.7820\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6925 - acc: 0.7856\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6894 - acc: 0.7846\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6827 - acc: 0.7866\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6833 - acc: 0.7855\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.6787 - acc: 0.7873\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6791 - acc: 0.78991s - l\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6753 - acc: 0.7894\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6742 - acc: 0.7899\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6695 - acc: 0.7913\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.6700 - acc: 0.7914\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.6548 - acc: 0.7950\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6693 - acc: 0.79181s - lo\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 18s 421us/step - loss: 0.6621 - acc: 0.7935\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.6682 - acc: 0.7916\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6574 - acc: 0.7954\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.6592 - acc: 0.7965\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6503 - acc: 0.7946\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6514 - acc: 0.7969\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.6450 - acc: 0.79850s - loss: 0.6450 - acc: 0.798\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6477 - acc: 0.7983\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.6463 - acc: 0.7974\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.6414 - acc: 0.7985\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6443 - acc: 0.7992\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6351 - acc: 0.8021\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.6383 - acc: 0.8015\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6332 - acc: 0.8016\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6287 - acc: 0.80364s - ETA: 3 - ETA: 1s - \n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.6290 - acc: 0.8044\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.6324 - acc: 0.8026\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 0.6232 - acc: 0.8038\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.6320 - acc: 0.8018\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.6218 - acc: 0.80470s - loss: 0.6214 - acc: 0.\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.6253 - acc: 0.8041\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6287 - acc: 0.8045\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.6194 - acc: 0.8064\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.6200 - acc: 0.8047\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6197 - acc: 0.8058\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.6093 - acc: 0.80741s\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.6162 - acc: 0.8065\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.6090 - acc: 0.8100\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 16s 391us/step - loss: 0.6158 - acc: 0.8085\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 16s 393us/step - loss: 0.6089 - acc: 0.8113\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 0.6080 - acc: 0.8106\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 17s 402us/step - loss: 0.6172 - acc: 0.8080\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 17s 405us/step - loss: 0.6116 - acc: 0.8079\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.6092 - acc: 0.8078\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 0.6061 - acc: 0.80981s - loss: \n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.6018 - acc: 0.8109\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 17s 402us/step - loss: 0.5988 - acc: 0.8130\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 0.5993 - acc: 0.8133\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.6004 - acc: 0.8121\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 17s 395us/step - loss: 0.5992 - acc: 0.8132\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 0.6038 - acc: 0.8099\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 17s 393us/step - loss: 0.5915 - acc: 0.81461s - loss: \n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 16s 391us/step - loss: 0.5909 - acc: 0.8156\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 17s 393us/step - loss: 0.5908 - acc: 0.8162\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 17s 393us/step - loss: 0.5963 - acc: 0.8143\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 17s 401us/step - loss: 0.5944 - acc: 0.8145\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 23s 537us/step - loss: 1.8615 - acc: 0.3351\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 1.4127 - acc: 0.5356\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 1.2533 - acc: 0.6046\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 1.1761 - acc: 0.6289\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 1.1205 - acc: 0.6485\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 18s 423us/step - loss: 1.0780 - acc: 0.6635\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 1.0352 - acc: 0.6771\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 1.0030 - acc: 0.6862\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.9698 - acc: 0.6983\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.9591 - acc: 0.7003\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.9266 - acc: 0.7114\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.9123 - acc: 0.7139\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.8913 - acc: 0.72201\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.8804 - acc: 0.7270\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.8582 - acc: 0.7332\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.8437 - acc: 0.7380\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.8387 - acc: 0.7411\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.8207 - acc: 0.7435\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.8073 - acc: 0.7496\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.8015 - acc: 0.7508\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.7962 - acc: 0.75451s - loss: 0\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.7843 - acc: 0.7570\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.7737 - acc: 0.7600\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.7747 - acc: 0.7573\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.7708 - acc: 0.7593\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.7576 - acc: 0.7661\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.7530 - acc: 0.76830s - loss: 0.7512 - acc: 0\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.7479 - acc: 0.7670\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.7429 - acc: 0.7682\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.7373 - acc: 0.7706\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.7276 - acc: 0.7729\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.7257 - acc: 0.7721\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.7213 - acc: 0.7748\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.7200 - acc: 0.7763\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.7088 - acc: 0.7801\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.7096 - acc: 0.7795\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.7098 - acc: 0.7820\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.7053 - acc: 0.7815\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6933 - acc: 0.7822\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.6929 - acc: 0.7839\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6960 - acc: 0.78330s - loss: 0.697\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.6850 - acc: 0.7864\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.6776 - acc: 0.7887\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6719 - acc: 0.7895\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.6730 - acc: 0.7915\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6698 - acc: 0.7915\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6692 - acc: 0.7897\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6662 - acc: 0.7920\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6615 - acc: 0.7948\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.6667 - acc: 0.79401s - loss: \n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6564 - acc: 0.79790s - loss: 0.655\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6490 - acc: 0.7984\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6534 - acc: 0.7970\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6538 - acc: 0.7982\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6548 - acc: 0.7952\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.6407 - acc: 0.8000\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.6350 - acc: 0.8017\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6477 - acc: 0.7973\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6465 - acc: 0.7992\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6408 - acc: 0.8007\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.6321 - acc: 0.8051\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6352 - acc: 0.80311s - loss: \n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6308 - acc: 0.8042\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6315 - acc: 0.8035\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6283 - acc: 0.8029\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6269 - acc: 0.8049\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6241 - acc: 0.8058\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6169 - acc: 0.8087\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.6262 - acc: 0.8075\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6158 - acc: 0.8062\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.6098 - acc: 0.8112\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.6134 - acc: 0.8100\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.6156 - acc: 0.8055\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 18s 421us/step - loss: 0.6178 - acc: 0.8076\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6097 - acc: 0.8105\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 21s 488us/step - loss: 0.6088 - acc: 0.8125\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6057 - acc: 0.8111\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6075 - acc: 0.8123\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6061 - acc: 0.8115\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.6063 - acc: 0.8098\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6020 - acc: 0.8107\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6069 - acc: 0.8111\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5991 - acc: 0.81542s - loss: 0.5979 - acc:  - E\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5988 - acc: 0.8137\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6019 - acc: 0.8116\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5964 - acc: 0.8145\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6003 - acc: 0.8126\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.6001 - acc: 0.8139\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5927 - acc: 0.8152\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5912 - acc: 0.8165\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5881 - acc: 0.8160\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5990 - acc: 0.8143\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5933 - acc: 0.8116\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5945 - acc: 0.8114\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.5855 - acc: 0.81620s - loss: 0.5854 - acc: 0.816\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.5767 - acc: 0.8198\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.5874 - acc: 0.8149\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.5832 - acc: 0.8192\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5839 - acc: 0.8180\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5840 - acc: 0.8152\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 1.8609 - acc: 0.3390\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 16s 391us/step - loss: 1.3999 - acc: 0.5445\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 16s 392us/step - loss: 1.2502 - acc: 0.6023\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 16s 390us/step - loss: 1.1639 - acc: 0.6337\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 16s 391us/step - loss: 1.1036 - acc: 0.6498\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 1.0672 - acc: 0.6631\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 16s 389us/step - loss: 1.0243 - acc: 0.6788\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 16s 390us/step - loss: 0.9894 - acc: 0.6939\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 16s 392us/step - loss: 0.9690 - acc: 0.6963\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 17s 393us/step - loss: 0.9480 - acc: 0.7034\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 16s 391us/step - loss: 0.9213 - acc: 0.7168\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 16s 389us/step - loss: 0.9074 - acc: 0.7183\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 16s 390us/step - loss: 0.8805 - acc: 0.7274\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 16s 389us/step - loss: 0.8666 - acc: 0.7300\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 16s 389us/step - loss: 0.8584 - acc: 0.7325\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 17s 395us/step - loss: 0.8519 - acc: 0.7327\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 16s 392us/step - loss: 0.8349 - acc: 0.74080s - loss: 0.8356\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 16s 390us/step - loss: 0.8221 - acc: 0.7459\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.8138 - acc: 0.7459\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 17s 404us/step - loss: 0.8103 - acc: 0.7498\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 17s 399us/step - loss: 0.8024 - acc: 0.7510\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.7888 - acc: 0.7573\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.7843 - acc: 0.7579\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 0.7765 - acc: 0.7560\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.7667 - acc: 0.7612\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 17s 400us/step - loss: 0.7679 - acc: 0.7619\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.7545 - acc: 0.7648\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 17s 395us/step - loss: 0.7483 - acc: 0.7674\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.7475 - acc: 0.7680\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.7396 - acc: 0.7709\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.7325 - acc: 0.7737\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 17s 404us/step - loss: 0.7292 - acc: 0.7740\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 17s 405us/step - loss: 0.7284 - acc: 0.7727\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 17s 406us/step - loss: 0.7162 - acc: 0.7756\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 17s 399us/step - loss: 0.7169 - acc: 0.7765\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 17s 404us/step - loss: 0.7169 - acc: 0.7786\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.7055 - acc: 0.7810\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.7041 - acc: 0.7783\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 0.6977 - acc: 0.7830\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 17s 399us/step - loss: 0.7015 - acc: 0.7812\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 17s 400us/step - loss: 0.6968 - acc: 0.7835\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 17s 399us/step - loss: 0.6888 - acc: 0.7836\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 17s 401us/step - loss: 0.6934 - acc: 0.7836\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.6856 - acc: 0.7863\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.6828 - acc: 0.7882\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 17s 400us/step - loss: 0.6733 - acc: 0.7892\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.6788 - acc: 0.7876\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 0.6729 - acc: 0.7909\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.6729 - acc: 0.7890\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 17s 399us/step - loss: 0.6701 - acc: 0.7919\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.6651 - acc: 0.7927\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.6562 - acc: 0.7973\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 17s 399us/step - loss: 0.6592 - acc: 0.7940\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.6585 - acc: 0.7950\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.6505 - acc: 0.7971\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 17s 405us/step - loss: 0.6554 - acc: 0.7954\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 0.6547 - acc: 0.7956\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 17s 399us/step - loss: 0.6525 - acc: 0.7956\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.6466 - acc: 0.7984\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 17s 409us/step - loss: 0.6468 - acc: 0.7975\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.6432 - acc: 0.7998\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 18s 421us/step - loss: 0.6382 - acc: 0.8022\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.6398 - acc: 0.8013\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.6328 - acc: 0.8037\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.6284 - acc: 0.8038\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 0.6311 - acc: 0.804 - 17s 414us/step - loss: 0.6313 - acc: 0.8041\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6311 - acc: 0.8017\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.6280 - acc: 0.8051\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 17s 413us/step - loss: 0.6274 - acc: 0.8047\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.6348 - acc: 0.8004\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 17s 404us/step - loss: 0.6205 - acc: 0.8063\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 17s 403us/step - loss: 0.6278 - acc: 0.8040\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 17s 411us/step - loss: 0.6244 - acc: 0.8044\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 0.6212 - acc: 0.8048\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 0.6208 - acc: 0.8082\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.6099 - acc: 0.8094\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 17s 407us/step - loss: 0.6169 - acc: 0.8066\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 17s 410us/step - loss: 0.6122 - acc: 0.8103\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 17s 412us/step - loss: 0.6094 - acc: 0.8096\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.6157 - acc: 0.8069\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 17s 400us/step - loss: 0.6211 - acc: 0.8067\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 17s 413us/step - loss: 0.6112 - acc: 0.8094\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 17s 408us/step - loss: 0.6090 - acc: 0.8083\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 17s 407us/step - loss: 0.6063 - acc: 0.8121\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 17s 407us/step - loss: 0.6111 - acc: 0.8103\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 17s 403us/step - loss: 0.5997 - acc: 0.8130\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.6084 - acc: 0.8108\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 17s 401us/step - loss: 0.6030 - acc: 0.8111\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 17s 400us/step - loss: 0.6045 - acc: 0.8106\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 17s 407us/step - loss: 0.6035 - acc: 0.8099\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 17s 403us/step - loss: 0.5963 - acc: 0.8145\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 17s 401us/step - loss: 0.5999 - acc: 0.8131\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.5933 - acc: 0.8145\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 17s 401us/step - loss: 0.5943 - acc: 0.8137\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.5912 - acc: 0.8137\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 17s 402us/step - loss: 0.5905 - acc: 0.8157\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 17s 401us/step - loss: 0.5870 - acc: 0.8142\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 0.5849 - acc: 0.8168\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 17s 402us/step - loss: 0.5901 - acc: 0.8159\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 17s 403us/step - loss: 0.5928 - acc: 0.8154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('model21',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000005859B358>),\n",
       "                             ('model22',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000005859B828>),\n",
       "                             ('model23',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000005859B630>),\n",
       "                             ('model24',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000005859BA20>),\n",
       "                             ('model25',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000005859BE10>)],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_clf2.fit(Xtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Validation:  0.90105\n"
     ]
    }
   ],
   "source": [
    "y_pred_val2 = ensemble_clf2.predict(Xval)\n",
    "print('Accuracy on Validation: ', accuracy_score(y_pred_val2, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing:  0.8681111111111111\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = ensemble_clf2.predict(Xtest)\n",
    "print('Accuracy on Testing: ', accuracy_score(y_pred2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 3: 200 nodes X 4 layers X 5 ensemble X 200 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model3():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(200, input_shape = (1024, ), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 models to ensemble\n",
    "model31 = KerasClassifier(build_fn = mlp_model3, epochs = 200)\n",
    "model32 = KerasClassifier(build_fn = mlp_model3, epochs = 200)\n",
    "model33 = KerasClassifier(build_fn = mlp_model3, epochs = 200)\n",
    "model34 = KerasClassifier(build_fn = mlp_model3, epochs = 200)\n",
    "model35 = KerasClassifier(build_fn = mlp_model3, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_clf3 = VotingClassifier(estimators = [('model31', model31), ('model32', model32), ('model33', model33), ('model34', model34), ('model35', model35)], voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42000/42000 [==============================] - 21s 495us/step - loss: 1.9070 - acc: 0.3141\n",
      "Epoch 2/200\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 1.4243 - acc: 0.5315\n",
      "Epoch 3/200\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 1.2630 - acc: 0.5975\n",
      "Epoch 4/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 1.1695 - acc: 0.6306\n",
      "Epoch 5/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 1.1118 - acc: 0.6526\n",
      "Epoch 6/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 1.0732 - acc: 0.6604\n",
      "Epoch 7/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 1.0263 - acc: 0.6782\n",
      "Epoch 8/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 1.0077 - acc: 0.6876\n",
      "Epoch 9/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.9673 - acc: 0.6970\n",
      "Epoch 10/200\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 0.9531 - acc: 0.7026\n",
      "Epoch 11/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.9338 - acc: 0.70721s - loss:\n",
      "Epoch 12/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.9133 - acc: 0.7142\n",
      "Epoch 13/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.8957 - acc: 0.7215\n",
      "Epoch 14/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.8742 - acc: 0.7283\n",
      "Epoch 15/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.8676 - acc: 0.7310\n",
      "Epoch 16/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.8519 - acc: 0.7339\n",
      "Epoch 17/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.8369 - acc: 0.7402\n",
      "Epoch 18/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.8261 - acc: 0.7438\n",
      "Epoch 19/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.8141 - acc: 0.7460\n",
      "Epoch 20/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.8136 - acc: 0.7472\n",
      "Epoch 21/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.7957 - acc: 0.7517\n",
      "Epoch 22/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.8005 - acc: 0.7497\n",
      "Epoch 23/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.7833 - acc: 0.7567\n",
      "Epoch 24/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.7703 - acc: 0.76210s - loss: 0.\n",
      "Epoch 25/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.7740 - acc: 0.7591\n",
      "Epoch 26/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.7617 - acc: 0.76231s -\n",
      "Epoch 27/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.7596 - acc: 0.76393s - loss: 0.7570 - ac - - ETA: 1s - loss\n",
      "Epoch 28/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.7466 - acc: 0.7654\n",
      "Epoch 29/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.7501 - acc: 0.7651\n",
      "Epoch 30/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.7318 - acc: 0.7735\n",
      "Epoch 31/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.7332 - acc: 0.7715\n",
      "Epoch 32/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.7333 - acc: 0.77480s - loss: 0.73\n",
      "Epoch 33/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.7205 - acc: 0.7744\n",
      "Epoch 34/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.7218 - acc: 0.77383s - loss: \n",
      "Epoch 35/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.7131 - acc: 0.77870s - loss: 0.7117 - a\n",
      "Epoch 36/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.7096 - acc: 0.7806\n",
      "Epoch 37/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.7090 - acc: 0.7791\n",
      "Epoch 38/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.7044 - acc: 0.7788\n",
      "Epoch 39/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6958 - acc: 0.7831\n",
      "Epoch 40/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6936 - acc: 0.78410s - loss: 0.694\n",
      "Epoch 41/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6898 - acc: 0.7835\n",
      "Epoch 42/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.6846 - acc: 0.78881\n",
      "Epoch 43/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6865 - acc: 0.7848\n",
      "Epoch 44/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.6787 - acc: 0.7875\n",
      "Epoch 45/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.6788 - acc: 0.7878\n",
      "Epoch 46/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6758 - acc: 0.78931s \n",
      "Epoch 47/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.6736 - acc: 0.7901\n",
      "Epoch 48/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.6677 - acc: 0.7913\n",
      "Epoch 49/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.6737 - acc: 0.79100s - loss: 0.6726\n",
      "Epoch 50/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6685 - acc: 0.7916\n",
      "Epoch 51/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6639 - acc: 0.7941\n",
      "Epoch 52/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6592 - acc: 0.7944\n",
      "Epoch 53/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6533 - acc: 0.7964\n",
      "Epoch 54/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6412 - acc: 0.7995\n",
      "Epoch 55/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6528 - acc: 0.7969\n",
      "Epoch 56/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6516 - acc: 0.79680s - loss: 0.6517 - acc: 0.\n",
      "Epoch 57/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6501 - acc: 0.7980\n",
      "Epoch 58/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6457 - acc: 0.7973\n",
      "Epoch 59/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6430 - acc: 0.8007\n",
      "Epoch 60/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.6395 - acc: 0.8010\n",
      "Epoch 61/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6404 - acc: 0.8005\n",
      "Epoch 62/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6417 - acc: 0.7998\n",
      "Epoch 63/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.6326 - acc: 0.8017\n",
      "Epoch 64/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.6331 - acc: 0.8034\n",
      "Epoch 65/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6326 - acc: 0.80290s - loss: 0.6335 - acc: 0.80 - ETA: 0s - loss: 0.6333 - acc\n",
      "Epoch 66/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6294 - acc: 0.8037\n",
      "Epoch 67/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.6275 - acc: 0.8042\n",
      "Epoch 68/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6239 - acc: 0.8055\n",
      "Epoch 69/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6200 - acc: 0.8071\n",
      "Epoch 70/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6235 - acc: 0.8041\n",
      "Epoch 71/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6250 - acc: 0.8072\n",
      "Epoch 72/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6169 - acc: 0.8058\n",
      "Epoch 73/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6259 - acc: 0.8054\n",
      "Epoch 74/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.6248 - acc: 0.8028\n",
      "Epoch 75/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6153 - acc: 0.8072\n",
      "Epoch 76/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6182 - acc: 0.8050\n",
      "Epoch 77/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.6221 - acc: 0.8044\n",
      "Epoch 78/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.6084 - acc: 0.8095\n",
      "Epoch 79/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.6075 - acc: 0.8093\n",
      "Epoch 80/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.6117 - acc: 0.8091\n",
      "Epoch 81/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.6154 - acc: 0.8072\n",
      "Epoch 82/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.6033 - acc: 0.8111\n",
      "Epoch 83/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.6066 - acc: 0.8095\n",
      "Epoch 84/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.6065 - acc: 0.8081\n",
      "Epoch 85/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5993 - acc: 0.8125\n",
      "Epoch 86/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.6118 - acc: 0.8094\n",
      "Epoch 87/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.6043 - acc: 0.8123\n",
      "Epoch 88/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5956 - acc: 0.8151\n",
      "Epoch 89/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5981 - acc: 0.8112\n",
      "Epoch 90/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.6019 - acc: 0.8138\n",
      "Epoch 91/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.6021 - acc: 0.8134\n",
      "Epoch 92/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5933 - acc: 0.8159\n",
      "Epoch 93/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5938 - acc: 0.8145\n",
      "Epoch 94/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.5890 - acc: 0.8154\n",
      "Epoch 95/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5921 - acc: 0.8143\n",
      "Epoch 96/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5915 - acc: 0.8161\n",
      "Epoch 97/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5940 - acc: 0.8131\n",
      "Epoch 98/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5865 - acc: 0.8171\n",
      "Epoch 99/200\n",
      "42000/42000 [==============================] - 20s 478us/step - loss: 0.5869 - acc: 0.8166\n",
      "Epoch 100/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.5929 - acc: 0.8154\n",
      "Epoch 101/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.5768 - acc: 0.8206\n",
      "Epoch 102/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.5890 - acc: 0.8172\n",
      "Epoch 103/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5805 - acc: 0.8183\n",
      "Epoch 104/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5816 - acc: 0.8168\n",
      "Epoch 105/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5782 - acc: 0.81921s - loss: \n",
      "Epoch 106/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5856 - acc: 0.8162\n",
      "Epoch 107/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5765 - acc: 0.8189\n",
      "Epoch 108/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5792 - acc: 0.8198\n",
      "Epoch 109/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5799 - acc: 0.8200\n",
      "Epoch 110/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.5774 - acc: 0.8183\n",
      "Epoch 111/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5705 - acc: 0.82121s - l\n",
      "Epoch 112/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5767 - acc: 0.82010s - loss: 0.5758 - acc: 0\n",
      "Epoch 113/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.5748 - acc: 0.8188\n",
      "Epoch 114/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5715 - acc: 0.8210\n",
      "Epoch 115/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5728 - acc: 0.82060s - loss: 0.5\n",
      "Epoch 116/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5684 - acc: 0.82180s - loss: 0.56\n",
      "Epoch 117/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5714 - acc: 0.8199\n",
      "Epoch 118/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5692 - acc: 0.8197\n",
      "Epoch 119/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5669 - acc: 0.82260s - loss: 0.5660 - acc: 0 - ETA: 0s - loss: 0.5656 -\n",
      "Epoch 120/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5682 - acc: 0.8220\n",
      "Epoch 121/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5694 - acc: 0.8203\n",
      "Epoch 122/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5652 - acc: 0.82370s - loss: 0.5653 - acc: 0.823\n",
      "Epoch 123/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5664 - acc: 0.8212\n",
      "Epoch 124/200\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 0.5630 - acc: 0.8246 - 19s 443us/step - loss: 0.5632 - acc: 0.8246\n",
      "Epoch 125/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5663 - acc: 0.8239\n",
      "Epoch 126/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5603 - acc: 0.8256\n",
      "Epoch 127/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5665 - acc: 0.8226\n",
      "Epoch 128/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5601 - acc: 0.8244\n",
      "Epoch 129/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5694 - acc: 0.82253s - l\n",
      "Epoch 130/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5603 - acc: 0.8255\n",
      "Epoch 131/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5604 - acc: 0.82380s - loss: 0.5604 - acc: 0.823\n",
      "Epoch 132/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5601 - acc: 0.8259\n",
      "Epoch 133/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.5612 - acc: 0.8237\n",
      "Epoch 134/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5590 - acc: 0.82350s - loss: 0.5583 - acc: 0.8 - ETA: 0s - loss: 0.5590 - acc: 0.8\n",
      "Epoch 135/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5592 - acc: 0.8252\n",
      "Epoch 136/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5568 - acc: 0.8263\n",
      "Epoch 137/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5513 - acc: 0.8275\n",
      "Epoch 138/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5558 - acc: 0.8254\n",
      "Epoch 139/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5577 - acc: 0.8257\n",
      "Epoch 140/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.5553 - acc: 0.8254\n",
      "Epoch 141/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5555 - acc: 0.8270\n",
      "Epoch 142/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5545 - acc: 0.8245\n",
      "Epoch 143/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5568 - acc: 0.8258\n",
      "Epoch 144/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5554 - acc: 0.8247\n",
      "Epoch 145/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5530 - acc: 0.8271\n",
      "Epoch 146/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5553 - acc: 0.8256\n",
      "Epoch 147/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5502 - acc: 0.8285\n",
      "Epoch 148/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5496 - acc: 0.8271\n",
      "Epoch 149/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.5480 - acc: 0.8261\n",
      "Epoch 150/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5477 - acc: 0.8306\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5500 - acc: 0.8273\n",
      "Epoch 152/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5537 - acc: 0.8249\n",
      "Epoch 153/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.5515 - acc: 0.8268\n",
      "Epoch 154/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.5522 - acc: 0.82661s - loss: 0.5\n",
      "Epoch 155/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5427 - acc: 0.8306\n",
      "Epoch 156/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.5442 - acc: 0.8291\n",
      "Epoch 157/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5435 - acc: 0.8272\n",
      "Epoch 158/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.5446 - acc: 0.8276\n",
      "Epoch 159/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.5423 - acc: 0.8305\n",
      "Epoch 160/200\n",
      "42000/42000 [==============================] - 20s 475us/step - loss: 0.5404 - acc: 0.82990s - loss: 0.5400 - acc: 0.\n",
      "Epoch 161/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5433 - acc: 0.828610s - loss: 0.5481\n",
      "Epoch 162/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5437 - acc: 0.8268\n",
      "Epoch 163/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5436 - acc: 0.8307\n",
      "Epoch 164/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.5382 - acc: 0.8305\n",
      "Epoch 165/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5372 - acc: 0.8303\n",
      "Epoch 166/200\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 0.5398 - acc: 0.82921s - loss\n",
      "Epoch 167/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.5387 - acc: 0.8325\n",
      "Epoch 168/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5391 - acc: 0.8316\n",
      "Epoch 169/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5385 - acc: 0.8320\n",
      "Epoch 170/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5394 - acc: 0.8299\n",
      "Epoch 171/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5436 - acc: 0.8296\n",
      "Epoch 172/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5350 - acc: 0.83011s - los\n",
      "Epoch 173/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5378 - acc: 0.8322\n",
      "Epoch 174/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5341 - acc: 0.8318\n",
      "Epoch 175/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5380 - acc: 0.8318\n",
      "Epoch 176/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5345 - acc: 0.8322\n",
      "Epoch 177/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5330 - acc: 0.8308\n",
      "Epoch 178/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5425 - acc: 0.8292\n",
      "Epoch 179/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5315 - acc: 0.8325\n",
      "Epoch 180/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5318 - acc: 0.8335\n",
      "Epoch 181/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5305 - acc: 0.8346\n",
      "Epoch 182/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5254 - acc: 0.83381s - loss\n",
      "Epoch 183/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5309 - acc: 0.8324\n",
      "Epoch 184/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.5228 - acc: 0.83606s - los - ETA: 5s - los\n",
      "Epoch 185/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5311 - acc: 0.8320\n",
      "Epoch 186/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5289 - acc: 0.8340\n",
      "Epoch 187/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5311 - acc: 0.8328\n",
      "Epoch 188/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5302 - acc: 0.8324\n",
      "Epoch 189/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5293 - acc: 0.8322\n",
      "Epoch 190/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5224 - acc: 0.8369\n",
      "Epoch 191/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5244 - acc: 0.83550s - loss: 0.5229 - acc:\n",
      "Epoch 192/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5314 - acc: 0.83471s - loss: \n",
      "Epoch 193/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.5269 - acc: 0.8336\n",
      "Epoch 194/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5256 - acc: 0.83624s - loss: 0.5238 - acc: 0.83 - ETA: 3\n",
      "Epoch 195/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5341 - acc: 0.83261s - l\n",
      "Epoch 196/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5205 - acc: 0.8368\n",
      "Epoch 197/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5239 - acc: 0.8365\n",
      "Epoch 198/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.5278 - acc: 0.8327\n",
      "Epoch 199/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5262 - acc: 0.8343\n",
      "Epoch 200/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5299 - acc: 0.8331\n",
      "Epoch 1/200\n",
      "42000/42000 [==============================] - 24s 562us/step - loss: 1.8067 - acc: 0.3674\n",
      "Epoch 2/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 1.3737 - acc: 0.5561\n",
      "Epoch 3/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 1.2346 - acc: 0.6083\n",
      "Epoch 4/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 1.1679 - acc: 0.6340\n",
      "Epoch 5/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 1.1127 - acc: 0.6513\n",
      "Epoch 6/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 1.0697 - acc: 0.66590s - loss: 1.0\n",
      "Epoch 7/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 1.0242 - acc: 0.6807\n",
      "Epoch 8/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.9966 - acc: 0.6888\n",
      "Epoch 9/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.9655 - acc: 0.6990\n",
      "Epoch 10/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.9374 - acc: 0.7078\n",
      "Epoch 11/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.9257 - acc: 0.7150\n",
      "Epoch 12/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.9011 - acc: 0.7196\n",
      "Epoch 13/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.8837 - acc: 0.7250\n",
      "Epoch 14/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.8663 - acc: 0.7332\n",
      "Epoch 15/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.8502 - acc: 0.7367\n",
      "Epoch 16/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.8441 - acc: 0.7413\n",
      "Epoch 17/200\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 0.8293 - acc: 0.7418\n",
      "Epoch 18/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.8171 - acc: 0.7463\n",
      "Epoch 19/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.8038 - acc: 0.7496\n",
      "Epoch 20/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.8003 - acc: 0.7490\n",
      "Epoch 21/200\n",
      "42000/42000 [==============================] - 20s 464us/step - loss: 0.7851 - acc: 0.75601\n",
      "Epoch 22/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.7787 - acc: 0.7571\n",
      "Epoch 23/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.7700 - acc: 0.7595\n",
      "Epoch 24/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.7747 - acc: 0.7621\n",
      "Epoch 25/200\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 0.7696 - acc: 0.7586\n",
      "Epoch 26/200\n",
      "42000/42000 [==============================] - 20s 480us/step - loss: 0.7561 - acc: 0.76390s - loss: 0.7564 - acc: 0.\n",
      "Epoch 27/200\n",
      "42000/42000 [==============================] - 20s 485us/step - loss: 0.7554 - acc: 0.7663\n",
      "Epoch 28/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.7428 - acc: 0.7686\n",
      "Epoch 29/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.7385 - acc: 0.7699\n",
      "Epoch 30/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.7419 - acc: 0.76901s - loss: \n",
      "Epoch 31/200\n",
      "42000/42000 [==============================] - 20s 478us/step - loss: 0.7295 - acc: 0.7719\n",
      "Epoch 32/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.7277 - acc: 0.7744\n",
      "Epoch 33/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.7153 - acc: 0.7767\n",
      "Epoch 34/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.7168 - acc: 0.7760\n",
      "Epoch 35/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.7115 - acc: 0.77751s\n",
      "Epoch 36/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.7065 - acc: 0.7811\n",
      "Epoch 37/200\n",
      "42000/42000 [==============================] - 20s 480us/step - loss: 0.7052 - acc: 0.7809\n",
      "Epoch 38/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.7043 - acc: 0.7810\n",
      "Epoch 39/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.6939 - acc: 0.7845\n",
      "Epoch 40/200\n",
      "42000/42000 [==============================] - 20s 475us/step - loss: 0.6931 - acc: 0.7836\n",
      "Epoch 41/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.6920 - acc: 0.7843TA: 1\n",
      "Epoch 42/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.6805 - acc: 0.7902\n",
      "Epoch 43/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.6908 - acc: 0.7876\n",
      "Epoch 44/200\n",
      "42000/42000 [==============================] - 20s 483us/step - loss: 0.6789 - acc: 0.7886\n",
      "Epoch 45/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.6857 - acc: 0.7862\n",
      "Epoch 46/200\n",
      "42000/42000 [==============================] - 20s 475us/step - loss: 0.6776 - acc: 0.7906\n",
      "Epoch 47/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.6660 - acc: 0.7927\n",
      "Epoch 48/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.6743 - acc: 0.7909\n",
      "Epoch 49/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.6683 - acc: 0.79210s - loss: 0.6681 - acc: 0\n",
      "Epoch 50/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.6612 - acc: 0.79500s - loss: 0.6613 - acc: 0\n",
      "Epoch 51/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.6681 - acc: 0.7917\n",
      "Epoch 52/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.6629 - acc: 0.7940\n",
      "Epoch 53/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.6584 - acc: 0.7937\n",
      "Epoch 54/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.6525 - acc: 0.79550s - loss: 0.6526 -\n",
      "Epoch 55/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.6485 - acc: 0.7975\n",
      "Epoch 56/200\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 0.6518 - acc: 0.7961- ETA: 3s - loss: 0.  - 19s 462us/step - loss: 0.6520 - acc: 0.7960\n",
      "Epoch 57/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.6509 - acc: 0.7985\n",
      "Epoch 58/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.6486 - acc: 0.7972\n",
      "Epoch 59/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.6460 - acc: 0.7988\n",
      "Epoch 60/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6462 - acc: 0.7995\n",
      "Epoch 61/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.6371 - acc: 0.8011\n",
      "Epoch 62/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6414 - acc: 0.7992\n",
      "Epoch 63/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6386 - acc: 0.7992\n",
      "Epoch 64/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6287 - acc: 0.8037\n",
      "Epoch 65/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.6277 - acc: 0.8056\n",
      "Epoch 66/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6336 - acc: 0.8044\n",
      "Epoch 67/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6333 - acc: 0.8011\n",
      "Epoch 68/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.6243 - acc: 0.8043\n",
      "Epoch 69/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.6221 - acc: 0.8042\n",
      "Epoch 70/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6223 - acc: 0.8064\n",
      "Epoch 71/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6248 - acc: 0.8034\n",
      "Epoch 72/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6232 - acc: 0.8060\n",
      "Epoch 73/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6223 - acc: 0.8066\n",
      "Epoch 74/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6233 - acc: 0.8052\n",
      "Epoch 75/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6186 - acc: 0.8056\n",
      "Epoch 76/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6186 - acc: 0.8083\n",
      "Epoch 77/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6136 - acc: 0.8096\n",
      "Epoch 78/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6146 - acc: 0.8071\n",
      "Epoch 79/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6138 - acc: 0.8091\n",
      "Epoch 80/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6081 - acc: 0.8114\n",
      "Epoch 81/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6100 - acc: 0.8094\n",
      "Epoch 82/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6077 - acc: 0.8088\n",
      "Epoch 83/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6038 - acc: 0.8120\n",
      "Epoch 84/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6032 - acc: 0.81321s - los\n",
      "Epoch 85/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.6048 - acc: 0.8125\n",
      "Epoch 86/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6033 - acc: 0.8104\n",
      "Epoch 87/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.6064 - acc: 0.8097\n",
      "Epoch 88/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6025 - acc: 0.8131\n",
      "Epoch 89/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.6066 - acc: 0.8120\n",
      "Epoch 90/200\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.5998 - acc: 0.8118\n",
      "Epoch 91/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5934 - acc: 0.8150\n",
      "Epoch 92/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5998 - acc: 0.81340s - loss: 0.5995 - acc: \n",
      "Epoch 93/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5935 - acc: 0.8159\n",
      "Epoch 94/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5879 - acc: 0.8155\n",
      "Epoch 95/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5900 - acc: 0.8145\n",
      "Epoch 96/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.5943 - acc: 0.8151\n",
      "Epoch 97/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5840 - acc: 0.8184\n",
      "Epoch 98/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5962 - acc: 0.81410s - loss: 0.5980\n",
      "Epoch 99/200\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.5929 - acc: 0.8151\n",
      "Epoch 100/200\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.5840 - acc: 0.8182\n",
      "Epoch 101/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.5862 - acc: 0.8166\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5885 - acc: 0.8167\n",
      "Epoch 103/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5844 - acc: 0.81701s - loss\n",
      "Epoch 104/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5883 - acc: 0.81440s - loss: 0.5\n",
      "Epoch 105/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5837 - acc: 0.8183\n",
      "Epoch 106/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5797 - acc: 0.8183\n",
      "Epoch 107/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5869 - acc: 0.8151\n",
      "Epoch 108/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5737 - acc: 0.8216\n",
      "Epoch 109/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5805 - acc: 0.81880s - loss: 0.5821 -\n",
      "Epoch 110/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.5789 - acc: 0.8188\n",
      "Epoch 111/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5798 - acc: 0.8200\n",
      "Epoch 112/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.5840 - acc: 0.8176\n",
      "Epoch 113/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5784 - acc: 0.8199\n",
      "Epoch 114/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.5762 - acc: 0.8197\n",
      "Epoch 115/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5773 - acc: 0.8182\n",
      "Epoch 116/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.5700 - acc: 0.8208\n",
      "Epoch 117/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5793 - acc: 0.8183\n",
      "Epoch 118/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5773 - acc: 0.8195\n",
      "Epoch 119/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5679 - acc: 0.8253\n",
      "Epoch 120/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5727 - acc: 0.8197\n",
      "Epoch 121/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5711 - acc: 0.8221\n",
      "Epoch 122/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5672 - acc: 0.8215\n",
      "Epoch 123/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5678 - acc: 0.8207\n",
      "Epoch 124/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5735 - acc: 0.8229\n",
      "Epoch 125/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.5626 - acc: 0.8240\n",
      "Epoch 126/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.5671 - acc: 0.8211\n",
      "Epoch 127/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5685 - acc: 0.8227\n",
      "Epoch 128/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.5630 - acc: 0.8244\n",
      "Epoch 129/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.5635 - acc: 0.8241\n",
      "Epoch 130/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5593 - acc: 0.8229\n",
      "Epoch 131/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5635 - acc: 0.8230\n",
      "Epoch 132/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.5616 - acc: 0.8245\n",
      "Epoch 133/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5649 - acc: 0.8235\n",
      "Epoch 134/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.5631 - acc: 0.8239\n",
      "Epoch 135/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.5621 - acc: 0.8223\n",
      "Epoch 136/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5547 - acc: 0.8257\n",
      "Epoch 137/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.5542 - acc: 0.8249\n",
      "Epoch 138/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5553 - acc: 0.8268\n",
      "Epoch 139/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5613 - acc: 0.8247\n",
      "Epoch 140/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5588 - acc: 0.8233\n",
      "Epoch 141/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5474 - acc: 0.8275\n",
      "Epoch 142/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5556 - acc: 0.8245\n",
      "Epoch 143/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5467 - acc: 0.8277\n",
      "Epoch 144/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.5551 - acc: 0.8277\n",
      "Epoch 145/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5526 - acc: 0.8264\n",
      "Epoch 146/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5528 - acc: 0.8250\n",
      "Epoch 147/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5554 - acc: 0.8253\n",
      "Epoch 148/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5520 - acc: 0.8298\n",
      "Epoch 149/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5486 - acc: 0.8276\n",
      "Epoch 150/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5523 - acc: 0.8280\n",
      "Epoch 151/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5505 - acc: 0.8270\n",
      "Epoch 152/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5468 - acc: 0.8293\n",
      "Epoch 153/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.5533 - acc: 0.8260\n",
      "Epoch 154/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5383 - acc: 0.8317\n",
      "Epoch 155/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.5530 - acc: 0.8285ETA: 0s - loss: 0.5523 - acc: \n",
      "Epoch 156/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5447 - acc: 0.8279\n",
      "Epoch 157/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5453 - acc: 0.8304\n",
      "Epoch 158/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5499 - acc: 0.8260\n",
      "Epoch 159/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5435 - acc: 0.8291\n",
      "Epoch 160/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.5395 - acc: 0.8311\n",
      "Epoch 161/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5470 - acc: 0.8283\n",
      "Epoch 162/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5433 - acc: 0.8262\n",
      "Epoch 163/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5378 - acc: 0.8305\n",
      "Epoch 164/200\n",
      "42000/42000 [==============================] - 20s 482us/step - loss: 0.5463 - acc: 0.82823s - loss: 0.5459 -  - ETA: 3s - loss: 0.5452 - acc: 0.828 - ETA: 3s - lo\n",
      "Epoch 165/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.5383 - acc: 0.8306\n",
      "Epoch 166/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5445 - acc: 0.8301\n",
      "Epoch 167/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5384 - acc: 0.8301\n",
      "Epoch 168/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5337 - acc: 0.8323\n",
      "Epoch 169/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5461 - acc: 0.8274\n",
      "Epoch 170/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5364 - acc: 0.8304\n",
      "Epoch 171/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5354 - acc: 0.8307\n",
      "Epoch 172/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5404 - acc: 0.82972s - loss: 0.5380 - \n",
      "Epoch 173/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5390 - acc: 0.8288\n",
      "Epoch 174/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5420 - acc: 0.83061s - lo\n",
      "Epoch 175/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5327 - acc: 0.8335\n",
      "Epoch 176/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.5407 - acc: 0.8289\n",
      "Epoch 177/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5367 - acc: 0.8305\n",
      "Epoch 178/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.5370 - acc: 0.83369s  - ETA: 3s - loss: 0 - ETA: 2s - loss: 0.5354 -\n",
      "Epoch 179/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5404 - acc: 0.8305\n",
      "Epoch 180/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5364 - acc: 0.8311\n",
      "Epoch 181/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5349 - acc: 0.83050s - loss: 0.5351 - acc: 0.8\n",
      "Epoch 182/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5358 - acc: 0.8313\n",
      "Epoch 183/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5359 - acc: 0.8323\n",
      "Epoch 184/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.5308 - acc: 0.8324\n",
      "Epoch 185/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.5297 - acc: 0.8339\n",
      "Epoch 186/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5385 - acc: 0.8310\n",
      "Epoch 187/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.5262 - acc: 0.8336\n",
      "Epoch 188/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.5362 - acc: 0.8310\n",
      "Epoch 189/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5325 - acc: 0.8348\n",
      "Epoch 190/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5319 - acc: 0.8321\n",
      "Epoch 191/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5375 - acc: 0.8308\n",
      "Epoch 192/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5303 - acc: 0.8343\n",
      "Epoch 193/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.5311 - acc: 0.8338\n",
      "Epoch 194/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5262 - acc: 0.8326\n",
      "Epoch 195/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5262 - acc: 0.8336\n",
      "Epoch 196/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.5314 - acc: 0.8344\n",
      "Epoch 197/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.5310 - acc: 0.8343\n",
      "Epoch 198/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5308 - acc: 0.8348\n",
      "Epoch 199/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5231 - acc: 0.8377\n",
      "Epoch 200/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5285 - acc: 0.8340\n",
      "Epoch 1/200\n",
      "42000/42000 [==============================] - 22s 532us/step - loss: 1.8442 - acc: 0.3487\n",
      "Epoch 2/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 1.3894 - acc: 0.5470\n",
      "Epoch 3/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 1.2500 - acc: 0.6002\n",
      "Epoch 4/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 1.1766 - acc: 0.6274\n",
      "Epoch 5/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 1.1314 - acc: 0.6448\n",
      "Epoch 6/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 1.0819 - acc: 0.6609\n",
      "Epoch 7/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 1.0421 - acc: 0.6726\n",
      "Epoch 8/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 1.0104 - acc: 0.6853\n",
      "Epoch 9/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.9842 - acc: 0.6912\n",
      "Epoch 10/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.9601 - acc: 0.7015\n",
      "Epoch 11/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.9337 - acc: 0.7081\n",
      "Epoch 12/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.9213 - acc: 0.7113\n",
      "Epoch 13/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.8976 - acc: 0.7202\n",
      "Epoch 14/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.8865 - acc: 0.72390s - loss: 0.8877 - a\n",
      "Epoch 15/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.8688 - acc: 0.7305\n",
      "Epoch 16/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.8526 - acc: 0.7329\n",
      "Epoch 17/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.8489 - acc: 0.7362\n",
      "Epoch 18/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.8248 - acc: 0.7428\n",
      "Epoch 19/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.8225 - acc: 0.7440\n",
      "Epoch 20/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.8080 - acc: 0.7484\n",
      "Epoch 21/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.7977 - acc: 0.7510\n",
      "Epoch 22/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.7926 - acc: 0.7523\n",
      "Epoch 23/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.7909 - acc: 0.7536\n",
      "Epoch 24/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.7750 - acc: 0.7571\n",
      "Epoch 25/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.7766 - acc: 0.7592 \n",
      "Epoch 26/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.7572 - acc: 0.7645\n",
      "Epoch 27/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.7579 - acc: 0.76460s - loss: 0.75\n",
      "Epoch 28/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.7532 - acc: 0.7641\n",
      "Epoch 29/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.7418 - acc: 0.7673\n",
      "Epoch 30/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.7428 - acc: 0.7682\n",
      "Epoch 31/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.7288 - acc: 0.7741\n",
      "Epoch 32/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.7314 - acc: 0.77271s - \n",
      "Epoch 33/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.7257 - acc: 0.7740\n",
      "Epoch 34/200\n",
      "42000/42000 [==============================] - 18s 435us/step - loss: 0.7291 - acc: 0.7729\n",
      "Epoch 35/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.7155 - acc: 0.7757\n",
      "Epoch 36/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.7098 - acc: 0.7767\n",
      "Epoch 37/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.7105 - acc: 0.7757\n",
      "Epoch 38/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.7018 - acc: 0.7806\n",
      "Epoch 39/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.7002 - acc: 0.7818\n",
      "Epoch 40/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6953 - acc: 0.7831\n",
      "Epoch 41/200\n",
      "42000/42000 [==============================] - 18s 437us/step - loss: 0.6929 - acc: 0.7839\n",
      "Epoch 42/200\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.6935 - acc: 0.7845\n",
      "Epoch 43/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6801 - acc: 0.7868\n",
      "Epoch 44/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6886 - acc: 0.7847\n",
      "Epoch 45/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6831 - acc: 0.7873\n",
      "Epoch 46/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.6771 - acc: 0.7893\n",
      "Epoch 47/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6744 - acc: 0.7895\n",
      "Epoch 48/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6681 - acc: 0.7919\n",
      "Epoch 49/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.6631 - acc: 0.7950\n",
      "Epoch 50/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.6626 - acc: 0.7905\n",
      "Epoch 51/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.6616 - acc: 0.7930\n",
      "Epoch 52/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.6551 - acc: 0.7949\n",
      "Epoch 53/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.6535 - acc: 0.7950\n",
      "Epoch 54/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6566 - acc: 0.7967\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.6561 - acc: 0.7944\n",
      "Epoch 56/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6459 - acc: 0.7978\n",
      "Epoch 57/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6497 - acc: 0.7970\n",
      "Epoch 58/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.6499 - acc: 0.7954\n",
      "Epoch 59/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.6448 - acc: 0.79852s - loss: 0.6446 - acc: 0.7 - E\n",
      "Epoch 60/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6428 - acc: 0.80061s - l\n",
      "Epoch 61/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.6406 - acc: 0.8020\n",
      "Epoch 62/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.6421 - acc: 0.8014\n",
      "Epoch 63/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6410 - acc: 0.8009\n",
      "Epoch 64/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.6389 - acc: 0.8025\n",
      "Epoch 65/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6321 - acc: 0.8038\n",
      "Epoch 66/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.6293 - acc: 0.8036\n",
      "Epoch 67/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.6339 - acc: 0.8006\n",
      "Epoch 68/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6262 - acc: 0.8049\n",
      "Epoch 69/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.6233 - acc: 0.8030\n",
      "Epoch 70/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6239 - acc: 0.8051\n",
      "Epoch 71/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.6211 - acc: 0.8060\n",
      "Epoch 72/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.6182 - acc: 0.80690s - loss: 0.6167 - a\n",
      "Epoch 73/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6232 - acc: 0.8054\n",
      "Epoch 74/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6202 - acc: 0.8095\n",
      "Epoch 75/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.6098 - acc: 0.8101\n",
      "Epoch 76/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6109 - acc: 0.8092\n",
      "Epoch 77/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6181 - acc: 0.8077\n",
      "Epoch 78/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.6103 - acc: 0.8087\n",
      "Epoch 79/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.6091 - acc: 0.8099\n",
      "Epoch 80/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6127 - acc: 0.81080s - loss: 0.6116 \n",
      "Epoch 81/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6062 - acc: 0.8115\n",
      "Epoch 82/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6035 - acc: 0.8117\n",
      "Epoch 83/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.6074 - acc: 0.8135\n",
      "Epoch 84/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6090 - acc: 0.8121\n",
      "Epoch 85/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.6055 - acc: 0.8115\n",
      "Epoch 86/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.6060 - acc: 0.8110\n",
      "Epoch 87/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6012 - acc: 0.8116\n",
      "Epoch 88/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.6026 - acc: 0.8126\n",
      "Epoch 89/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.6015 - acc: 0.8130\n",
      "Epoch 90/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5955 - acc: 0.8135\n",
      "Epoch 91/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5889 - acc: 0.8159\n",
      "Epoch 92/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5936 - acc: 0.8147\n",
      "Epoch 93/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5916 - acc: 0.8161\n",
      "Epoch 94/200\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.5935 - acc: 0.8154\n",
      "Epoch 95/200\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 0.5916 - acc: 0.8145 - 19s 443us/step - loss: 0.5917 - acc: 0.8144\n",
      "Epoch 96/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5916 - acc: 0.8144\n",
      "Epoch 97/200\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.5939 - acc: 0.8139\n",
      "Epoch 98/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5903 - acc: 0.8152\n",
      "Epoch 99/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5919 - acc: 0.8159\n",
      "Epoch 100/200\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.5887 - acc: 0.8157\n",
      "Epoch 101/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5865 - acc: 0.8163\n",
      "Epoch 102/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5893 - acc: 0.8139\n",
      "Epoch 103/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5782 - acc: 0.8210\n",
      "Epoch 104/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5865 - acc: 0.8176\n",
      "Epoch 105/200\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 0.5826 - acc: 0.8175\n",
      "Epoch 106/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5762 - acc: 0.8200\n",
      "Epoch 107/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5792 - acc: 0.8196\n",
      "Epoch 108/200\n",
      "42000/42000 [==============================] - 18s 429us/step - loss: 0.5803 - acc: 0.8176\n",
      "Epoch 109/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5760 - acc: 0.8191\n",
      "Epoch 110/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5737 - acc: 0.8204\n",
      "Epoch 111/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5754 - acc: 0.8177\n",
      "Epoch 112/200\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5741 - acc: 0.8203\n",
      "Epoch 113/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5779 - acc: 0.8207\n",
      "Epoch 114/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5725 - acc: 0.8217\n",
      "Epoch 115/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5750 - acc: 0.8201\n",
      "Epoch 116/200\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.5681 - acc: 0.8224\n",
      "Epoch 117/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5673 - acc: 0.82140s - loss: 0.5662 \n",
      "Epoch 118/200\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5678 - acc: 0.82331s - loss:\n",
      "Epoch 119/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5639 - acc: 0.8227\n",
      "Epoch 120/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5684 - acc: 0.8226\n",
      "Epoch 121/200\n",
      "42000/42000 [==============================] - 19s 441us/step - loss: 0.5660 - acc: 0.8226\n",
      "Epoch 122/200\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.5612 - acc: 0.8253\n",
      "Epoch 123/200\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.5586 - acc: 0.8247\n",
      "Epoch 124/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5684 - acc: 0.8227\n",
      "Epoch 125/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5586 - acc: 0.8235\n",
      "Epoch 126/200\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.5668 - acc: 0.8214\n",
      "Epoch 127/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5647 - acc: 0.8242\n",
      "Epoch 128/200\n",
      "42000/42000 [==============================] - 18s 430us/step - loss: 0.5653 - acc: 0.8232\n",
      "Epoch 129/200\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 0.5609 - acc: 0.8238\n",
      "Epoch 130/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5579 - acc: 0.8262\n",
      "Epoch 131/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5592 - acc: 0.8258\n",
      "Epoch 132/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5496 - acc: 0.82700s - loss: 0.5494 - acc: 0.827\n",
      "Epoch 133/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5603 - acc: 0.8255\n",
      "Epoch 134/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.5536 - acc: 0.8287\n",
      "Epoch 135/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5548 - acc: 0.8259\n",
      "Epoch 136/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5568 - acc: 0.8253\n",
      "Epoch 137/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5609 - acc: 0.82460s - loss: 0.5611 - acc: 0.824\n",
      "Epoch 138/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.5581 - acc: 0.8274\n",
      "Epoch 139/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.5496 - acc: 0.8280\n",
      "Epoch 140/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5553 - acc: 0.8259\n",
      "Epoch 141/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.5493 - acc: 0.8297\n",
      "Epoch 142/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5597 - acc: 0.8256\n",
      "Epoch 143/200\n",
      "42000/42000 [==============================] - 20s 482us/step - loss: 0.5509 - acc: 0.8281\n",
      "Epoch 144/200\n",
      "42000/42000 [==============================] - 20s 481us/step - loss: 0.5464 - acc: 0.8270\n",
      "Epoch 145/200\n",
      "42000/42000 [==============================] - 20s 485us/step - loss: 0.5447 - acc: 0.8299\n",
      "Epoch 146/200\n",
      "42000/42000 [==============================] - 20s 484us/step - loss: 0.5549 - acc: 0.8245\n",
      "Epoch 147/200\n",
      "42000/42000 [==============================] - 20s 485us/step - loss: 0.5504 - acc: 0.8289\n",
      "Epoch 148/200\n",
      "42000/42000 [==============================] - 20s 486us/step - loss: 0.5468 - acc: 0.8276\n",
      "Epoch 149/200\n",
      "42000/42000 [==============================] - 20s 483us/step - loss: 0.5447 - acc: 0.8304\n",
      "Epoch 150/200\n",
      "42000/42000 [==============================] - 21s 488us/step - loss: 0.5473 - acc: 0.8276\n",
      "Epoch 151/200\n",
      "42000/42000 [==============================] - 20s 486us/step - loss: 0.5406 - acc: 0.83085s - loss: 0.5368\n",
      "Epoch 152/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.5413 - acc: 0.8324\n",
      "Epoch 153/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.5480 - acc: 0.8283\n",
      "Epoch 154/200\n",
      "42000/42000 [==============================] - 21s 489us/step - loss: 0.5491 - acc: 0.8282\n",
      "Epoch 155/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.5414 - acc: 0.8295\n",
      "Epoch 156/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.5428 - acc: 0.8304\n",
      "Epoch 157/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.5456 - acc: 0.8281\n",
      "Epoch 158/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.5476 - acc: 0.8277\n",
      "Epoch 159/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.5474 - acc: 0.8274\n",
      "Epoch 160/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.5421 - acc: 0.8287\n",
      "Epoch 161/200\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.5335 - acc: 0.8332\n",
      "Epoch 162/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.5399 - acc: 0.8299\n",
      "Epoch 163/200\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.5378 - acc: 0.8318\n",
      "Epoch 164/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.5383 - acc: 0.8296\n",
      "Epoch 165/200\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.5357 - acc: 0.8321\n",
      "Epoch 166/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5359 - acc: 0.8313\n",
      "Epoch 167/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5418 - acc: 0.8294\n",
      "Epoch 168/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5297 - acc: 0.83361s - loss: 0\n",
      "Epoch 169/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.5354 - acc: 0.8330\n",
      "Epoch 170/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.5342 - acc: 0.8328\n",
      "Epoch 171/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5399 - acc: 0.83071s - loss: 0\n",
      "Epoch 172/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5403 - acc: 0.8312\n",
      "Epoch 173/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5346 - acc: 0.8330\n",
      "Epoch 174/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.5328 - acc: 0.8332\n",
      "Epoch 175/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.5324 - acc: 0.8338\n",
      "Epoch 176/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.5363 - acc: 0.8328\n",
      "Epoch 177/200\n",
      "42000/42000 [==============================] - 20s 464us/step - loss: 0.5334 - acc: 0.8315\n",
      "Epoch 178/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5361 - acc: 0.83250s - loss: 0.5363 - acc: 0.\n",
      "Epoch 179/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5284 - acc: 0.8338\n",
      "Epoch 180/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.5269 - acc: 0.8361\n",
      "Epoch 181/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5249 - acc: 0.8340\n",
      "Epoch 182/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.5306 - acc: 0.8351\n",
      "Epoch 183/200\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.5237 - acc: 0.8357\n",
      "Epoch 184/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5332 - acc: 0.8325\n",
      "Epoch 185/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.5255 - acc: 0.8346\n",
      "Epoch 186/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5307 - acc: 0.8345\n",
      "Epoch 187/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5343 - acc: 0.8312\n",
      "Epoch 188/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5317 - acc: 0.8319\n",
      "Epoch 189/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5223 - acc: 0.8357\n",
      "Epoch 190/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5199 - acc: 0.8345\n",
      "Epoch 191/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.5185 - acc: 0.83790s - loss: 0.5183\n",
      "Epoch 192/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5256 - acc: 0.8349\n",
      "Epoch 193/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5250 - acc: 0.8350\n",
      "Epoch 194/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.5245 - acc: 0.8353\n",
      "Epoch 195/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5231 - acc: 0.83423s - - ETA: 2s - loss: 0.5 - ETA: 1s - loss:\n",
      "Epoch 196/200\n",
      "42000/42000 [==============================] - 20s 464us/step - loss: 0.5276 - acc: 0.8351\n",
      "Epoch 197/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.5238 - acc: 0.8353\n",
      "Epoch 198/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.5273 - acc: 0.8353\n",
      "Epoch 199/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5221 - acc: 0.8368\n",
      "Epoch 200/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.5137 - acc: 0.8369\n",
      "Epoch 1/200\n",
      "42000/42000 [==============================] - 23s 551us/step - loss: 1.8772 - acc: 0.33130s - loss: 1.8847 - acc:\n",
      "Epoch 2/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 1.4076 - acc: 0.5409\n",
      "Epoch 3/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 1.2494 - acc: 0.6033\n",
      "Epoch 4/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 1.1709 - acc: 0.6295\n",
      "Epoch 5/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 1.1195 - acc: 0.6488\n",
      "Epoch 6/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 20s 472us/step - loss: 1.0810 - acc: 0.6612\n",
      "Epoch 7/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 1.0417 - acc: 0.6739\n",
      "Epoch 8/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 1.0093 - acc: 0.6841\n",
      "Epoch 9/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.9791 - acc: 0.6949\n",
      "Epoch 10/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.9431 - acc: 0.7060\n",
      "Epoch 11/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.9245 - acc: 0.7107\n",
      "Epoch 12/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.9133 - acc: 0.7181\n",
      "Epoch 13/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.8965 - acc: 0.7212\n",
      "Epoch 14/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.8773 - acc: 0.7254\n",
      "Epoch 15/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.8652 - acc: 0.7304\n",
      "Epoch 16/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.8485 - acc: 0.7376\n",
      "Epoch 17/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.8404 - acc: 0.7388\n",
      "Epoch 18/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.8298 - acc: 0.7429\n",
      "Epoch 19/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.8127 - acc: 0.7476\n",
      "Epoch 20/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.8147 - acc: 0.7445\n",
      "Epoch 21/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.7967 - acc: 0.75110s - loss: 0.7964 - acc: 0.75\n",
      "Epoch 22/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.7924 - acc: 0.75260s - loss: 0.7910 - \n",
      "Epoch 23/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.7771 - acc: 0.7597\n",
      "Epoch 24/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.7819 - acc: 0.7570\n",
      "Epoch 25/200\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 0.7619 - acc: 0.7621\n",
      "Epoch 26/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.7609 - acc: 0.7640\n",
      "Epoch 27/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.7523 - acc: 0.7665\n",
      "Epoch 28/200\n",
      "42000/42000 [==============================] - 21s 498us/step - loss: 0.7539 - acc: 0.7649\n",
      "Epoch 29/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.7535 - acc: 0.76611\n",
      "Epoch 30/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.7362 - acc: 0.7700\n",
      "Epoch 31/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.7353 - acc: 0.7725\n",
      "Epoch 32/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.7245 - acc: 0.77320s - loss: 0.7246 - ac\n",
      "Epoch 33/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.7233 - acc: 0.7755\n",
      "Epoch 34/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.7196 - acc: 0.7750\n",
      "Epoch 35/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.7147 - acc: 0.7778\n",
      "Epoch 36/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.7155 - acc: 0.7780\n",
      "Epoch 37/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.7086 - acc: 0.7787\n",
      "Epoch 38/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.7010 - acc: 0.78101s - loss: 0\n",
      "Epoch 39/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.7040 - acc: 0.7822\n",
      "Epoch 40/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6898 - acc: 0.7852\n",
      "Epoch 41/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.6873 - acc: 0.7854\n",
      "Epoch 42/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6928 - acc: 0.7842\n",
      "Epoch 43/200\n",
      "42000/42000 [==============================] - 19s 444us/step - loss: 0.6874 - acc: 0.7855\n",
      "Epoch 44/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6821 - acc: 0.7882\n",
      "Epoch 45/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.6811 - acc: 0.7902\n",
      "Epoch 46/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.6713 - acc: 0.7911\n",
      "Epoch 47/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.6752 - acc: 0.7902\n",
      "Epoch 48/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6800 - acc: 0.7878\n",
      "Epoch 49/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.6606 - acc: 0.7932\n",
      "Epoch 50/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.6671 - acc: 0.7906\n",
      "Epoch 51/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6583 - acc: 0.7949\n",
      "Epoch 52/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.6586 - acc: 0.7960\n",
      "Epoch 53/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.6599 - acc: 0.7950\n",
      "Epoch 54/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.6554 - acc: 0.7946\n",
      "Epoch 55/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.6561 - acc: 0.7934\n",
      "Epoch 56/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6515 - acc: 0.79710s - loss: 0.6\n",
      "Epoch 57/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6487 - acc: 0.7976\n",
      "Epoch 58/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6529 - acc: 0.7989\n",
      "Epoch 59/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.6482 - acc: 0.7989\n",
      "Epoch 60/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6375 - acc: 0.8016\n",
      "Epoch 61/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6436 - acc: 0.8006\n",
      "Epoch 62/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6444 - acc: 0.7993\n",
      "Epoch 63/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.6350 - acc: 0.8027\n",
      "Epoch 64/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6401 - acc: 0.7988\n",
      "Epoch 65/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.6412 - acc: 0.8015\n",
      "Epoch 66/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6331 - acc: 0.8011\n",
      "Epoch 67/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.6342 - acc: 0.8032\n",
      "Epoch 68/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6277 - acc: 0.8032\n",
      "Epoch 69/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.6343 - acc: 0.80194s - loss: 0.6338 - acc: 0.80  - ETA:\n",
      "Epoch 70/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.6296 - acc: 0.8054\n",
      "Epoch 71/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6207 - acc: 0.8065\n",
      "Epoch 72/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6173 - acc: 0.8059\n",
      "Epoch 73/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.6249 - acc: 0.80410s - loss: 0.6249 -\n",
      "Epoch 74/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6249 - acc: 0.8043\n",
      "Epoch 75/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6162 - acc: 0.8081\n",
      "Epoch 76/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6234 - acc: 0.8061\n",
      "Epoch 77/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6124 - acc: 0.8094\n",
      "Epoch 78/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.6123 - acc: 0.8119\n",
      "Epoch 79/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.6067 - acc: 0.8115\n",
      "Epoch 80/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6147 - acc: 0.8093\n",
      "Epoch 81/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.6122 - acc: 0.8082\n",
      "Epoch 82/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.6074 - acc: 0.8118\n",
      "Epoch 83/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.6090 - acc: 0.8100\n",
      "Epoch 84/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.6044 - acc: 0.8110\n",
      "Epoch 85/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.6055 - acc: 0.8102\n",
      "Epoch 86/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.5978 - acc: 0.8135\n",
      "Epoch 87/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.6039 - acc: 0.8104\n",
      "Epoch 88/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.6009 - acc: 0.81211s - loss: 0.6022 - acc - ETA: 0s - loss: 0.6021 - \n",
      "Epoch 89/200\n",
      "42000/42000 [==============================] - 20s 464us/step - loss: 0.5963 - acc: 0.8138\n",
      "Epoch 90/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.6041 - acc: 0.8095\n",
      "Epoch 91/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.5972 - acc: 0.8119\n",
      "Epoch 92/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.5955 - acc: 0.8138\n",
      "Epoch 93/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5924 - acc: 0.8158\n",
      "Epoch 94/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5945 - acc: 0.8158\n",
      "Epoch 95/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5935 - acc: 0.8143\n",
      "Epoch 96/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.5914 - acc: 0.8127\n",
      "Epoch 97/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.5960 - acc: 0.8144\n",
      "Epoch 98/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.5890 - acc: 0.8157\n",
      "Epoch 99/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5897 - acc: 0.8148\n",
      "Epoch 100/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5935 - acc: 0.8154\n",
      "Epoch 101/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5864 - acc: 0.8144\n",
      "Epoch 102/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5841 - acc: 0.8176\n",
      "Epoch 103/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5865 - acc: 0.8167\n",
      "Epoch 104/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.5922 - acc: 0.8145\n",
      "Epoch 105/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.5893 - acc: 0.81600s - loss: 0.58\n",
      "Epoch 106/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5814 - acc: 0.8188\n",
      "Epoch 107/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5839 - acc: 0.81840s - loss: 0.5826 - a\n",
      "Epoch 108/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5796 - acc: 0.8174\n",
      "Epoch 109/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5890 - acc: 0.81521s - loss: 0.5895 - a - ETA: 1s - loss\n",
      "Epoch 110/200\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 0.5849 - acc: 0.8186\n",
      "Epoch 111/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5731 - acc: 0.8210\n",
      "Epoch 112/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5767 - acc: 0.8195\n",
      "Epoch 113/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5819 - acc: 0.81651s - loss:\n",
      "Epoch 114/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5708 - acc: 0.8227\n",
      "Epoch 115/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.5741 - acc: 0.8189\n",
      "Epoch 116/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5815 - acc: 0.8186\n",
      "Epoch 117/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.5742 - acc: 0.8201\n",
      "Epoch 118/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5688 - acc: 0.8194\n",
      "Epoch 119/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5784 - acc: 0.8193\n",
      "Epoch 120/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5694 - acc: 0.82240s - loss: 0.56\n",
      "Epoch 121/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5755 - acc: 0.8202\n",
      "Epoch 122/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5667 - acc: 0.8200\n",
      "Epoch 123/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5743 - acc: 0.8218\n",
      "Epoch 124/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5731 - acc: 0.8227\n",
      "Epoch 125/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5755 - acc: 0.8208\n",
      "Epoch 126/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5669 - acc: 0.8237\n",
      "Epoch 127/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5710 - acc: 0.8212\n",
      "Epoch 128/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5707 - acc: 0.8236\n",
      "Epoch 129/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5630 - acc: 0.8245\n",
      "Epoch 130/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5611 - acc: 0.82501s - \n",
      "Epoch 131/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5605 - acc: 0.82360s - loss: 0.5615 - acc:\n",
      "Epoch 132/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5616 - acc: 0.8235\n",
      "Epoch 133/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5587 - acc: 0.8247\n",
      "Epoch 134/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5653 - acc: 0.8228\n",
      "Epoch 135/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5563 - acc: 0.82620s - loss: 0.5\n",
      "Epoch 136/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5650 - acc: 0.8215\n",
      "Epoch 137/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5557 - acc: 0.82580s - loss: 0.5556 -\n",
      "Epoch 138/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5636 - acc: 0.8224\n",
      "Epoch 139/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5557 - acc: 0.8256\n",
      "Epoch 140/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5607 - acc: 0.82620s - loss: 0.5\n",
      "Epoch 141/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5585 - acc: 0.8256\n",
      "Epoch 142/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5626 - acc: 0.8235\n",
      "Epoch 143/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5572 - acc: 0.8249\n",
      "Epoch 144/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5586 - acc: 0.8238\n",
      "Epoch 145/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5588 - acc: 0.8260\n",
      "Epoch 146/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5558 - acc: 0.8275- ETA: 5s - loss: 0.5 \n",
      "Epoch 147/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5588 - acc: 0.8238\n",
      "Epoch 148/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5547 - acc: 0.8257\n",
      "Epoch 149/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5556 - acc: 0.8267\n",
      "Epoch 150/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5516 - acc: 0.8275\n",
      "Epoch 151/200\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.5534 - acc: 0.8277A: 3s - loss: 0.55\n",
      "Epoch 152/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5499 - acc: 0.8260\n",
      "Epoch 153/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5428 - acc: 0.83011s - los\n",
      "Epoch 154/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5508 - acc: 0.8260\n",
      "Epoch 155/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5455 - acc: 0.8294\n",
      "Epoch 156/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5577 - acc: 0.8251\n",
      "Epoch 157/200\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.5457 - acc: 0.8285\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.5461 - acc: 0.8289\n",
      "Epoch 159/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.5475 - acc: 0.8284\n",
      "Epoch 160/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5462 - acc: 0.8288\n",
      "Epoch 161/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5380 - acc: 0.8316\n",
      "Epoch 162/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5402 - acc: 0.8299\n",
      "Epoch 163/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.5399 - acc: 0.8288\n",
      "Epoch 164/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.5389 - acc: 0.83101s - loss: 0.5384 - acc:  - ETA: 1s - loss: \n",
      "Epoch 165/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5402 - acc: 0.8296\n",
      "Epoch 166/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5377 - acc: 0.8305TA: 0s - loss: 0.5373 - acc: \n",
      "Epoch 167/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.5472 - acc: 0.8285\n",
      "Epoch 168/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5424 - acc: 0.8309\n",
      "Epoch 169/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5406 - acc: 0.8301\n",
      "Epoch 170/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5428 - acc: 0.8299\n",
      "Epoch 171/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5423 - acc: 0.8299\n",
      "Epoch 172/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5429 - acc: 0.8298\n",
      "Epoch 173/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5331 - acc: 0.8317\n",
      "Epoch 174/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5403 - acc: 0.8308\n",
      "Epoch 175/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5436 - acc: 0.8306\n",
      "Epoch 176/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5353 - acc: 0.8318\n",
      "Epoch 177/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.5408 - acc: 0.82932s - loss: 0.5403 \n",
      "Epoch 178/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5411 - acc: 0.83191s - los\n",
      "Epoch 179/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5350 - acc: 0.8329\n",
      "Epoch 180/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5362 - acc: 0.83251s - loss\n",
      "Epoch 181/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5345 - acc: 0.83160s - loss: 0.5342 - acc: 0.8\n",
      "Epoch 182/200\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 0.5404 - acc: 0.82970s - loss: 0.540\n",
      "Epoch 183/200\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 0.5371 - acc: 0.8301\n",
      "Epoch 184/200\n",
      "42000/42000 [==============================] - 19s 458us/step - loss: 0.5296 - acc: 0.8321\n",
      "Epoch 185/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.5346 - acc: 0.83440s - loss: 0.5331 \n",
      "Epoch 186/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5372 - acc: 0.8314\n",
      "Epoch 187/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5339 - acc: 0.83231s - lo\n",
      "Epoch 188/200\n",
      "42000/42000 [==============================] - 19s 457us/step - loss: 0.5376 - acc: 0.8317\n",
      "Epoch 189/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5354 - acc: 0.8331\n",
      "Epoch 190/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5364 - acc: 0.8307\n",
      "Epoch 191/200\n",
      "42000/42000 [==============================] - 19s 450us/step - loss: 0.5337 - acc: 0.8327\n",
      "Epoch 192/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5312 - acc: 0.8337\n",
      "Epoch 193/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5353 - acc: 0.8328\n",
      "Epoch 194/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5336 - acc: 0.8328\n",
      "Epoch 195/200\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.5372 - acc: 0.82921s \n",
      "Epoch 196/200\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 0.5339 - acc: 0.8327\n",
      "Epoch 197/200\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 0.5263 - acc: 0.8342\n",
      "Epoch 198/200\n",
      "42000/42000 [==============================] - 19s 452us/step - loss: 0.5300 - acc: 0.8345\n",
      "Epoch 199/200\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.5224 - acc: 0.83461s - los\n",
      "Epoch 200/200\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5351 - acc: 0.8317\n",
      "Epoch 1/200\n",
      "42000/42000 [==============================] - 25s 602us/step - loss: 1.7836 - acc: 0.3757\n",
      "Epoch 2/200\n",
      "42000/42000 [==============================] - 21s 495us/step - loss: 1.3780 - acc: 0.5553\n",
      "Epoch 3/200\n",
      "42000/42000 [==============================] - 21s 498us/step - loss: 1.2488 - acc: 0.6054\n",
      "Epoch 4/200\n",
      "42000/42000 [==============================] - 21s 497us/step - loss: 1.1767 - acc: 0.6295\n",
      "Epoch 5/200\n",
      "42000/42000 [==============================] - 21s 493us/step - loss: 1.1209 - acc: 0.6482\n",
      "Epoch 6/200\n",
      "42000/42000 [==============================] - 21s 498us/step - loss: 1.0894 - acc: 0.6607\n",
      "Epoch 7/200\n",
      "42000/42000 [==============================] - 21s 504us/step - loss: 1.0508 - acc: 0.6720\n",
      "Epoch 8/200\n",
      "42000/42000 [==============================] - 21s 496us/step - loss: 1.0122 - acc: 0.6839\n",
      "Epoch 9/200\n",
      "42000/42000 [==============================] - 21s 506us/step - loss: 0.9923 - acc: 0.6905\n",
      "Epoch 10/200\n",
      "42000/42000 [==============================] - 21s 503us/step - loss: 0.9578 - acc: 0.7010\n",
      "Epoch 11/200\n",
      "42000/42000 [==============================] - 22s 516us/step - loss: 0.9343 - acc: 0.70952s - loss: 0. - ETA: 1 - ETA: 0s - loss: 0.9344 - acc: 0.709\n",
      "Epoch 12/200\n",
      "42000/42000 [==============================] - 21s 506us/step - loss: 0.9214 - acc: 0.7141\n",
      "Epoch 13/200\n",
      "42000/42000 [==============================] - 21s 506us/step - loss: 0.8979 - acc: 0.72191\n",
      "Epoch 14/200\n",
      "42000/42000 [==============================] - 21s 496us/step - loss: 0.8775 - acc: 0.7278\n",
      "Epoch 15/200\n",
      "42000/42000 [==============================] - 21s 496us/step - loss: 0.8681 - acc: 0.7298\n",
      "Epoch 16/200\n",
      "42000/42000 [==============================] - 20s 486us/step - loss: 0.8514 - acc: 0.7350\n",
      "Epoch 17/200\n",
      "42000/42000 [==============================] - 20s 488us/step - loss: 0.8420 - acc: 0.7363\n",
      "Epoch 18/200\n",
      "42000/42000 [==============================] - 21s 490us/step - loss: 0.8290 - acc: 0.7408\n",
      "Epoch 19/200\n",
      "42000/42000 [==============================] - 20s 480us/step - loss: 0.8118 - acc: 0.7445\n",
      "Epoch 20/200\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.8147 - acc: 0.7464\n",
      "Epoch 21/200\n",
      "42000/42000 [==============================] - 21s 492us/step - loss: 0.8066 - acc: 0.74843s - loss: 0.8032\n",
      "Epoch 22/200\n",
      "42000/42000 [==============================] - 21s 489us/step - loss: 0.7877 - acc: 0.7559\n",
      "Epoch 23/200\n",
      "42000/42000 [==============================] - 21s 499us/step - loss: 0.7868 - acc: 0.7548\n",
      "Epoch 24/200\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.7753 - acc: 0.75770s - loss: 0.77\n",
      "Epoch 25/200\n",
      "42000/42000 [==============================] - 20s 480us/step - loss: 0.7675 - acc: 0.7629\n",
      "Epoch 26/200\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.7573 - acc: 0.7646\n",
      "Epoch 27/200\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.7544 - acc: 0.7659\n",
      "Epoch 28/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.7487 - acc: 0.7665\n",
      "Epoch 29/200\n",
      "42000/42000 [==============================] - 20s 475us/step - loss: 0.7458 - acc: 0.7676\n",
      "Epoch 30/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.7399 - acc: 0.7692\n",
      "Epoch 31/200\n",
      "42000/42000 [==============================] - 20s 475us/step - loss: 0.7358 - acc: 0.7684\n",
      "Epoch 32/200\n",
      "42000/42000 [==============================] - 20s 478us/step - loss: 0.7248 - acc: 0.7737\n",
      "Epoch 33/200\n",
      "42000/42000 [==============================] - 21s 506us/step - loss: 0.7187 - acc: 0.7744\n",
      "Epoch 34/200\n",
      "42000/42000 [==============================] - 21s 503us/step - loss: 0.7206 - acc: 0.77521s - l\n",
      "Epoch 35/200\n",
      "42000/42000 [==============================] - 22s 513us/step - loss: 0.7137 - acc: 0.7799\n",
      "Epoch 36/200\n",
      "42000/42000 [==============================] - 21s 511us/step - loss: 0.7107 - acc: 0.7782\n",
      "Epoch 37/200\n",
      "42000/42000 [==============================] - 21s 508us/step - loss: 0.7064 - acc: 0.77960s - loss: 0.7065 - acc: \n",
      "Epoch 38/200\n",
      "42000/42000 [==============================] - 21s 505us/step - loss: 0.7048 - acc: 0.7806\n",
      "Epoch 39/200\n",
      "42000/42000 [==============================] - 21s 500us/step - loss: 0.6976 - acc: 0.78160s - loss: 0.697\n",
      "Epoch 40/200\n",
      "42000/42000 [==============================] - 22s 519us/step - loss: 0.6874 - acc: 0.7849\n",
      "Epoch 41/200\n",
      "42000/42000 [==============================] - 21s 510us/step - loss: 0.6916 - acc: 0.7840\n",
      "Epoch 42/200\n",
      "42000/42000 [==============================] - 22s 520us/step - loss: 0.6881 - acc: 0.7868\n",
      "Epoch 43/200\n",
      "42000/42000 [==============================] - 22s 523us/step - loss: 0.6887 - acc: 0.7860\n",
      "Epoch 44/200\n",
      "42000/42000 [==============================] - 21s 490us/step - loss: 0.6824 - acc: 0.7854\n",
      "Epoch 45/200\n",
      "42000/42000 [==============================] - 21s 512us/step - loss: 0.6810 - acc: 0.7885\n",
      "Epoch 46/200\n",
      "42000/42000 [==============================] - 21s 505us/step - loss: 0.6686 - acc: 0.7903\n",
      "Epoch 47/200\n",
      "42000/42000 [==============================] - 21s 505us/step - loss: 0.6696 - acc: 0.7902\n",
      "Epoch 48/200\n",
      "42000/42000 [==============================] - 21s 506us/step - loss: 0.6676 - acc: 0.79160s - loss: 0.6659 - a\n",
      "Epoch 49/200\n",
      "42000/42000 [==============================] - 22s 514us/step - loss: 0.6666 - acc: 0.79201s - loss:\n",
      "Epoch 50/200\n",
      "42000/42000 [==============================] - 21s 511us/step - loss: 0.6651 - acc: 0.7920\n",
      "Epoch 51/200\n",
      "42000/42000 [==============================] - 21s 493us/step - loss: 0.6577 - acc: 0.7934\n",
      "Epoch 52/200\n",
      "42000/42000 [==============================] - 21s 494us/step - loss: 0.6568 - acc: 0.7966\n",
      "Epoch 53/200\n",
      "42000/42000 [==============================] - 21s 503us/step - loss: 0.6463 - acc: 0.79730s - loss: 0.6460 - acc: 0.7\n",
      "Epoch 54/200\n",
      "42000/42000 [==============================] - 22s 514us/step - loss: 0.6527 - acc: 0.7974\n",
      "Epoch 55/200\n",
      "42000/42000 [==============================] - 20s 482us/step - loss: 0.6464 - acc: 0.7978\n",
      "Epoch 56/200\n",
      "42000/42000 [==============================] - 20s 483us/step - loss: 0.6428 - acc: 0.8010\n",
      "Epoch 57/200\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.6528 - acc: 0.7974\n",
      "Epoch 58/200\n",
      "42000/42000 [==============================] - 20s 481us/step - loss: 0.6402 - acc: 0.7979\n",
      "Epoch 59/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.6364 - acc: 0.80180s - loss: 0.6358 - acc: 0\n",
      "Epoch 60/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.6347 - acc: 0.8005\n",
      "Epoch 61/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.6413 - acc: 0.7990\n",
      "Epoch 62/200\n",
      "42000/42000 [==============================] - 21s 492us/step - loss: 0.6352 - acc: 0.8024\n",
      "Epoch 63/200\n",
      "42000/42000 [==============================] - 20s 481us/step - loss: 0.6414 - acc: 0.7981\n",
      "Epoch 64/200\n",
      "42000/42000 [==============================] - 20s 483us/step - loss: 0.6331 - acc: 0.8018\n",
      "Epoch 65/200\n",
      "42000/42000 [==============================] - 21s 501us/step - loss: 0.6304 - acc: 0.8030\n",
      "Epoch 66/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.6268 - acc: 0.8044\n",
      "Epoch 67/200\n",
      "42000/42000 [==============================] - 20s 485us/step - loss: 0.6234 - acc: 0.8045\n",
      "Epoch 68/200\n",
      "42000/42000 [==============================] - 21s 489us/step - loss: 0.6272 - acc: 0.8052- ETA: 0s - loss: 0.6278 \n",
      "Epoch 69/200\n",
      "42000/42000 [==============================] - 22s 512us/step - loss: 0.6223 - acc: 0.8052\n",
      "Epoch 70/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.6114 - acc: 0.8064\n",
      "Epoch 71/200\n",
      "42000/42000 [==============================] - 20s 478us/step - loss: 0.6141 - acc: 0.8080\n",
      "Epoch 72/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.6137 - acc: 0.8083\n",
      "Epoch 73/200\n",
      "42000/42000 [==============================] - 20s 478us/step - loss: 0.6194 - acc: 0.8057\n",
      "Epoch 74/200\n",
      "42000/42000 [==============================] - 20s 488us/step - loss: 0.6044 - acc: 0.8108\n",
      "Epoch 75/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.6131 - acc: 0.80660s - loss: 0\n",
      "Epoch 76/200\n",
      "42000/42000 [==============================] - 21s 498us/step - loss: 0.6050 - acc: 0.8108\n",
      "Epoch 77/200\n",
      "42000/42000 [==============================] - 21s 498us/step - loss: 0.6083 - acc: 0.8086\n",
      "Epoch 78/200\n",
      "42000/42000 [==============================] - 20s 484us/step - loss: 0.6076 - acc: 0.8109\n",
      "Epoch 79/200\n",
      "42000/42000 [==============================] - 20s 480us/step - loss: 0.6098 - acc: 0.8081\n",
      "Epoch 80/200\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.6053 - acc: 0.8086\n",
      "Epoch 81/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.6029 - acc: 0.81450s - loss: 0.6027 - acc: 0.8\n",
      "Epoch 82/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5972 - acc: 0.8138\n",
      "Epoch 83/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.5981 - acc: 0.8130\n",
      "Epoch 84/200\n",
      "42000/42000 [==============================] - 20s 488us/step - loss: 0.5991 - acc: 0.8140\n",
      "Epoch 85/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.6012 - acc: 0.8120\n",
      "Epoch 86/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.6019 - acc: 0.8125\n",
      "Epoch 87/200\n",
      "42000/42000 [==============================] - 20s 482us/step - loss: 0.5962 - acc: 0.81480s - loss: 0.5965 - acc: 0.\n",
      "Epoch 88/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5933 - acc: 0.8135\n",
      "Epoch 89/200\n",
      "42000/42000 [==============================] - 20s 478us/step - loss: 0.5958 - acc: 0.8153\n",
      "Epoch 90/200\n",
      "42000/42000 [==============================] - 20s 485us/step - loss: 0.5969 - acc: 0.8143\n",
      "Epoch 91/200\n",
      "42000/42000 [==============================] - 21s 496us/step - loss: 0.5905 - acc: 0.8155\n",
      "Epoch 92/200\n",
      "42000/42000 [==============================] - 21s 501us/step - loss: 0.5964 - acc: 0.8127\n",
      "Epoch 93/200\n",
      "42000/42000 [==============================] - 20s 486us/step - loss: 0.5902 - acc: 0.8155\n",
      "Epoch 94/200\n",
      "42000/42000 [==============================] - 20s 483us/step - loss: 0.5896 - acc: 0.8173\n",
      "Epoch 95/200\n",
      "42000/42000 [==============================] - 20s 484us/step - loss: 0.5867 - acc: 0.81710s - loss: 0.5857 - acc: 0\n",
      "Epoch 96/200\n",
      "42000/42000 [==============================] - 20s 482us/step - loss: 0.5875 - acc: 0.8161\n",
      "Epoch 97/200\n",
      "42000/42000 [==============================] - 21s 490us/step - loss: 0.5838 - acc: 0.8165\n",
      "Epoch 98/200\n",
      "42000/42000 [==============================] - 21s 491us/step - loss: 0.5808 - acc: 0.8181\n",
      "Epoch 99/200\n",
      "42000/42000 [==============================] - 21s 510us/step - loss: 0.5820 - acc: 0.8170\n",
      "Epoch 100/200\n",
      "42000/42000 [==============================] - 21s 490us/step - loss: 0.5825 - acc: 0.8162\n",
      "Epoch 101/200\n",
      "42000/42000 [==============================] - 20s 485us/step - loss: 0.5839 - acc: 0.8153\n",
      "Epoch 102/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.5765 - acc: 0.8176\n",
      "Epoch 103/200\n",
      "42000/42000 [==============================] - 21s 489us/step - loss: 0.5793 - acc: 0.8187\n",
      "Epoch 104/200\n",
      "42000/42000 [==============================] - 21s 497us/step - loss: 0.5752 - acc: 0.81997s - ETA: 3s\n",
      "Epoch 105/200\n",
      "42000/42000 [==============================] - 22s 514us/step - loss: 0.5764 - acc: 0.8190\n",
      "Epoch 106/200\n",
      "42000/42000 [==============================] - 21s 493us/step - loss: 0.5760 - acc: 0.8208\n",
      "Epoch 107/200\n",
      "42000/42000 [==============================] - 20s 481us/step - loss: 0.5713 - acc: 0.8216\n",
      "Epoch 108/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 21s 502us/step - loss: 0.5749 - acc: 0.8210\n",
      "Epoch 109/200\n",
      "42000/42000 [==============================] - 21s 511us/step - loss: 0.5704 - acc: 0.82250s - loss: 0.5703 - ac\n",
      "Epoch 110/200\n",
      "42000/42000 [==============================] - 21s 503us/step - loss: 0.5709 - acc: 0.8220\n",
      "Epoch 111/200\n",
      "42000/42000 [==============================] - 21s 498us/step - loss: 0.5680 - acc: 0.8217\n",
      "Epoch 112/200\n",
      "42000/42000 [==============================] - 20s 486us/step - loss: 0.5828 - acc: 0.8190\n",
      "Epoch 113/200\n",
      "42000/42000 [==============================] - 20s 484us/step - loss: 0.5653 - acc: 0.8219\n",
      "Epoch 114/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.5713 - acc: 0.8210\n",
      "Epoch 115/200\n",
      "42000/42000 [==============================] - 20s 481us/step - loss: 0.5645 - acc: 0.8218\n",
      "Epoch 116/200\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.5619 - acc: 0.8215\n",
      "Epoch 117/200\n",
      "42000/42000 [==============================] - 21s 491us/step - loss: 0.5627 - acc: 0.8235\n",
      "Epoch 118/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5589 - acc: 0.82573s - loss: 0\n",
      "Epoch 119/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5653 - acc: 0.8226\n",
      "Epoch 120/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5634 - acc: 0.8244\n",
      "Epoch 121/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5630 - acc: 0.82416\n",
      "Epoch 122/200\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.5574 - acc: 0.8263\n",
      "Epoch 123/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.5599 - acc: 0.8248\n",
      "Epoch 124/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.5598 - acc: 0.8243\n",
      "Epoch 125/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.5550 - acc: 0.8257\n",
      "Epoch 126/200\n",
      "42000/42000 [==============================] - 21s 491us/step - loss: 0.5627 - acc: 0.8226\n",
      "Epoch 127/200\n",
      "42000/42000 [==============================] - 21s 498us/step - loss: 0.5614 - acc: 0.8256ETA\n",
      "Epoch 128/200\n",
      "42000/42000 [==============================] - 21s 506us/step - loss: 0.5520 - acc: 0.8271\n",
      "Epoch 129/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5574 - acc: 0.8244\n",
      "Epoch 130/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5561 - acc: 0.8258\n",
      "Epoch 131/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5539 - acc: 0.8273\n",
      "Epoch 132/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5559 - acc: 0.8246\n",
      "Epoch 133/200\n",
      "42000/42000 [==============================] - 20s 475us/step - loss: 0.5520 - acc: 0.82650s - loss: 0.5509 - acc: \n",
      "Epoch 134/200\n",
      "42000/42000 [==============================] - 20s 478us/step - loss: 0.5649 - acc: 0.8225\n",
      "Epoch 135/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.5516 - acc: 0.82810s - loss: 0.55\n",
      "Epoch 136/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5531 - acc: 0.8257\n",
      "Epoch 137/200\n",
      "42000/42000 [==============================] - 20s 480us/step - loss: 0.5515 - acc: 0.82681s - \n",
      "Epoch 138/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.5449 - acc: 0.8280\n",
      "Epoch 139/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5523 - acc: 0.8267\n",
      "Epoch 140/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5513 - acc: 0.8279\n",
      "Epoch 141/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5499 - acc: 0.8256\n",
      "Epoch 142/200\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 0.5449 - acc: 0.8312\n",
      "Epoch 143/200\n",
      "42000/42000 [==============================] - 20s 475us/step - loss: 0.5467 - acc: 0.8291\n",
      "Epoch 144/200\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 0.5383 - acc: 0.8311\n",
      "Epoch 145/200\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 0.5460 - acc: 0.8280\n",
      "Epoch 146/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5406 - acc: 0.8298\n",
      "Epoch 147/200\n",
      "42000/42000 [==============================] - 19s 464us/step - loss: 0.5465 - acc: 0.8281\n",
      "Epoch 148/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.5418 - acc: 0.83073s - los\n",
      "Epoch 149/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.5399 - acc: 0.8311\n",
      "Epoch 150/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.5436 - acc: 0.82910s - loss: 0.5436 - acc: 0\n",
      "Epoch 151/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.5429 - acc: 0.8289\n",
      "Epoch 152/200\n",
      "42000/42000 [==============================] - 20s 476us/step - loss: 0.5392 - acc: 0.8295\n",
      "Epoch 153/200\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.5425 - acc: 0.83022s - loss: 0.540\n",
      "Epoch 154/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5429 - acc: 0.8294\n",
      "Epoch 155/200\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.5408 - acc: 0.8287\n",
      "Epoch 156/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5408 - acc: 0.82670s - loss: 0.5405 - acc: 0.826 - ETA: 0s - loss: 0.5405 - acc: 0.82\n",
      "Epoch 157/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5440 - acc: 0.8301\n",
      "Epoch 158/200\n",
      "42000/42000 [==============================] - 20s 483us/step - loss: 0.5408 - acc: 0.8288\n",
      "Epoch 159/200\n",
      "42000/42000 [==============================] - 20s 477us/step - loss: 0.5425 - acc: 0.8298\n",
      "Epoch 160/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.5324 - acc: 0.8320\n",
      "Epoch 161/200\n",
      "42000/42000 [==============================] - 20s 469us/step - loss: 0.5362 - acc: 0.8308\n",
      "Epoch 162/200\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.5393 - acc: 0.8306\n",
      "Epoch 163/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5443 - acc: 0.8274\n",
      "Epoch 164/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.5258 - acc: 0.8359\n",
      "Epoch 165/200\n",
      "42000/42000 [==============================] - 20s 488us/step - loss: 0.5308 - acc: 0.8339\n",
      "Epoch 166/200\n",
      "42000/42000 [==============================] - 21s 492us/step - loss: 0.5327 - acc: 0.8320\n",
      "Epoch 167/200\n",
      "42000/42000 [==============================] - 21s 494us/step - loss: 0.5323 - acc: 0.8325\n",
      "Epoch 168/200\n",
      "42000/42000 [==============================] - 20s 474us/step - loss: 0.5284 - acc: 0.8339\n",
      "Epoch 169/200\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.5305 - acc: 0.83216s - loss: 0.5334 - acc: 0.831 - ETA: 6s - loss: 0.5 - ETA: 5s - loss - ETA:\n",
      "Epoch 170/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5356 - acc: 0.8306\n",
      "Epoch 171/200\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 0.5312 - acc: 0.8347\n",
      "Epoch 172/200\n",
      "42000/42000 [==============================] - 19s 462us/step - loss: 0.5258 - acc: 0.8362\n",
      "Epoch 173/200\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.5304 - acc: 0.8328\n",
      "Epoch 174/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5289 - acc: 0.8340\n",
      "Epoch 175/200\n",
      "42000/42000 [==============================] - 19s 463us/step - loss: 0.5265 - acc: 0.8350\n",
      "Epoch 176/200\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.5232 - acc: 0.83491s - loss: 0.\n",
      "Epoch 177/200\n",
      "42000/42000 [==============================] - 20s 468us/step - loss: 0.5291 - acc: 0.83490s - loss: 0.5\n",
      "Epoch 178/200\n",
      "42000/42000 [==============================] - 19s 461us/step - loss: 0.5289 - acc: 0.8335\n",
      "Epoch 179/200\n",
      "42000/42000 [==============================] - 20s 481us/step - loss: 0.5212 - acc: 0.83481s - los - ETA: 0s - loss: 0.5209 - acc:\n",
      "Epoch 180/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.5252 - acc: 0.8360\n",
      "Epoch 181/200\n",
      "42000/42000 [==============================] - 20s 481us/step - loss: 0.5284 - acc: 0.83425s - loss: 0\n",
      "Epoch 182/200\n",
      "42000/42000 [==============================] - 20s 484us/step - loss: 0.5303 - acc: 0.83151s - loss: 0.5297 - acc:  - ETA: 1s -\n",
      "Epoch 183/200\n",
      "42000/42000 [==============================] - 21s 494us/step - loss: 0.5216 - acc: 0.8366\n",
      "Epoch 184/200\n",
      "42000/42000 [==============================] - 21s 489us/step - loss: 0.5258 - acc: 0.8331\n",
      "Epoch 185/200\n",
      "42000/42000 [==============================] - 20s 483us/step - loss: 0.5234 - acc: 0.8345\n",
      "Epoch 186/200\n",
      "42000/42000 [==============================] - 20s 472us/step - loss: 0.5216 - acc: 0.8363\n",
      "Epoch 187/200\n",
      "42000/42000 [==============================] - 20s 467us/step - loss: 0.5221 - acc: 0.8349\n",
      "Epoch 188/200\n",
      "42000/42000 [==============================] - 20s 482us/step - loss: 0.5285 - acc: 0.8353\n",
      "Epoch 189/200\n",
      "42000/42000 [==============================] - 21s 493us/step - loss: 0.5162 - acc: 0.8370\n",
      "Epoch 190/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.5151 - acc: 0.8366\n",
      "Epoch 191/200\n",
      "42000/42000 [==============================] - 21s 498us/step - loss: 0.5239 - acc: 0.83561s - loss: 0\n",
      "Epoch 192/200\n",
      "42000/42000 [==============================] - 21s 494us/step - loss: 0.5175 - acc: 0.83691\n",
      "Epoch 193/200\n",
      "42000/42000 [==============================] - 21s 499us/step - loss: 0.5204 - acc: 0.8379\n",
      "Epoch 194/200\n",
      "42000/42000 [==============================] - 21s 496us/step - loss: 0.5229 - acc: 0.8355\n",
      "Epoch 195/200\n",
      "42000/42000 [==============================] - 21s 494us/step - loss: 0.5167 - acc: 0.8368\n",
      "Epoch 196/200\n",
      "42000/42000 [==============================] - 21s 501us/step - loss: 0.5248 - acc: 0.8361\n",
      "Epoch 197/200\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.5206 - acc: 0.8370\n",
      "Epoch 198/200\n",
      "42000/42000 [==============================] - 21s 495us/step - loss: 0.5251 - acc: 0.8346\n",
      "Epoch 199/200\n",
      "42000/42000 [==============================] - 20s 486us/step - loss: 0.5206 - acc: 0.8347\n",
      "Epoch 200/200\n",
      "42000/42000 [==============================] - 22s 514us/step - loss: 0.5168 - acc: 0.8371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('model31',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000006E3273C8>),\n",
       "                             ('model32',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000006E327E10>),\n",
       "                             ('model33',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000006E327160>),\n",
       "                             ('model34',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000006E3276A0>),\n",
       "                             ('model35',\n",
       "                              <keras.wrappers.scikit_learn.KerasClassifier object at 0x0000000060277B38>)],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_clf3.fit(Xtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Validation:  0.9249166666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred_val3 = ensemble_clf3.predict(Xval)\n",
    "print('Accuracy on Validation: ', accuracy_score(y_pred_val3, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing:  0.8846111111111111\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = ensemble_clf3.predict(Xtest)\n",
    "print('Accuracy on Testing: ', accuracy_score(y_pred3, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Results - traditional method (KNN) and Neural Networks (Python and Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy = 83.78% \n",
    "\n",
    "Test accuracy = 45.92%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python based Neural Network with 1 Hidden Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy = 89.30% \n",
    "\n",
    "Test accuracy = 83.01%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras Neural Network Ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy = 92.49% \n",
    "\n",
    "Test accuracy = 88.46%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
